"uuid","repository link","title","author","contributor","publication year","abstract","subject topic","language","publication type","publisher","isbn","issn","patent","patent status","bibliographic note","access restriction","embargo date","faculty","department","research group","programme","project","coordinates"
"uuid:3e99a5d4-dabe-4dfc-aa64-24756c6f5eb4","http://resolver.tudelft.nl/uuid:3e99a5d4-dabe-4dfc-aa64-24756c6f5eb4","A concept design for the removal of concrete gravity based structures","Vogel, J.A.","","2015","During the early 1970s the offshore market in the North Sea was growing rapidly. Due to discoveries of natural reserves and development of the oil price, the oil and gas industry developed and deployed many bottom founded offshore structures in the North Sea. Apart from steel jacket structures, many concrete gravity based structures (CGBSs) were constructed. In the coming decades many of the fields with these CGBSs approach the end of their lifetime and must be decommissioned. However, currently no solid solution for the removal of these concrete substructures is available. The introduction of Allseas’ Pioneering Spirit and her capabilities could provide new possibilities for the decommissioning of CGBSs. The Pioneering Spirit is designed for the decommissioning of platforms and its substructure using a unique “single lift” technique. A complete removal of the concrete substructure using this single lift technique is, due to its constructive condition, a perilous operation. However, the decommissioning regulations allow a partial removal of the concrete legs above a water depth of 55 meters. At this water depth a cut is made in the legs and after cutting the Pioneering Spirit can perform a lift of the leg using the Jacket Lift System (JLS). During this removal operation various problems may arise. In this study the main technical difficulties of the partial removal operation are identified. A focus is laid on the difficulties during the lifting phase. The hoisting speed of the JLS is relatively slow compared to the ship motions. This can result in a rebound of the leg during the initial lifting phase. Damage to the leg and a probable damage to the part of the GBS where the oil storage was located may then occur. The objective in this study is to design a solution for the initial lifting phase and analyze the feasibility of this concept. An adequate fast lifting solution after cut-off could overcome the problems associated with the rebound of the concrete legs. The designed solution for fast lifting is based on the concept of a passive heave compensator (PHC). To gain insight into the feasibility of this concept a model has been made in MATLAB/Simulink. Input data for the simulation model are mechanical dimension parameters and physical pressure parameters. By altering the parameters an estimation of the motions of the leg and hence the feasibility of the concept is determined.","Allseas; decommisioning; CGBSs","en","master thesis","","","","","","","","2017-09-30","Mechanical, Maritime and Materials Engineering","Offshore & Dredging Engineering","","BFS","",""
"uuid:9350afdd-8fbf-4c45-8e74-65251cc9a140","http://resolver.tudelft.nl/uuid:9350afdd-8fbf-4c45-8e74-65251cc9a140","LNG as fuel for inland waterway vessels","Van den Berg, M.","Tavasszy, L.A. (mentor); Hekkenberg, R.G. (mentor); Van Ham, J.C. (mentor)","2015","Natural Gas (NG) can be used as fuel for several forms of transport. NG converted into liquid is called Liquefied Natural Gas (LNG). LNG has a favourable energy density for ease of transport and storage. On the other hand, LNG is cryogenic at -162 °C. To use LNG as fuel, it is required to use an LNG fuel system. An LNG fuel system consists mainly of a cryogenic tank, for storage, and a regasification unit to convert LNG to NG. Cryonorm Systems B.V. is a designer and supplier of such LNG fuel systems for several forms of transport. In addition to this LNG fuel system, another type of engine is required. Cryonorm already designed and supplied four inland waterway (IWW) vessels with a marine LNG fuel system. LNG as fuel was introduced by seagoing vessels, though there is also a propensity to move to LNG as fuel for IWW vessels. LNG as fuel for IWW vessels is the subject of investigation in this thesis, with the aim to define the business opportunities of LNG fuel systems for IWW vessels in different scenarios for Cryonorm. In this research, the financial viability of LNG as fuel for inland waterway vessels is investigated. The financial viability depends mainly on fuel consumption, fuel prices and the required financial resources to move to LNG as fuel. With this investigation, the necessary preconditions in which shipping companies possibly will move to LNG as fuel are determined. These preconditions consist of the financial eligibility of inland waterway vessels and the extent to which obstacles, such as LNG bunkering logistics, have to be overcome. With this determination, scenarios are developed in which the business opportunities of LNG fuel systems for inland waterway vessels for Cryonorm are rendered for the next 10 years.","LNG; inland shipping","en","master thesis","","","","","","","","2020-09-24","Technology, Policy and Management","Transport, Infrastructure & Logistics","","TIL - Engineering","",""
"uuid:9b9e2651-d876-4f5b-b1af-1ab37093de76","http://resolver.tudelft.nl/uuid:9b9e2651-d876-4f5b-b1af-1ab37093de76","Conceptual Design of a Flameless Combustor for Aircraft Engines","El Abbassi, M.","Gangoli Rao, A. (mentor)","2015","The thesis presents a feasibility study of a proposed combustion methodology by means of two-dimensional Computational Fluid Dynamics on the application of Flameless Combustion to an aircraft engine with the objective to achieve ultra-low pollutant emissions. The geometrical design started from scratch while taking the philosophy of the proposed methodology into account. The combustor’s entry conditions and global size were estimated based on a conventional turbofan engine and corrected at atmospheric pressure. The most important parameter to achieve flameless combustion was considered to be the recirculation ratio in order for it to be sustained. Therefore a cold flow simulation was carried out first with the objective to fulfil this requirement by using RANS-based turbulence models. The models were first validated by attempting to reproduce the measured flow velocities of three wind tunnel cases with similar geometry. All models under predicted maximum recirculation. The k-? Realizable model was closest to determine maximum recirculation and its axial position. This model also showed a similar linear trend in maximum recirculation ratio with respect to different channel-to-inlet area ratios, and also the reattachment point was well predicted, which aided in the prediction of the flow paths. Attempts were also made to reproduce the measured velocity profiles with the Large Eddy Simulation derived models. The Embedded Large Eddy Simulation model did not give better results, probably due to the mesh resolution still being too coarse and the Detached Eddy Simulation model even performed worse which made it not suitable for such flow cases. Within the engine’s size limits the annular type combustor could achieve a maximum recirculation ratio up to a value of 1.3 in the cold flow while a can type combustor could achieve a value of 3.2. In the literature a value of at least 3 was required. The simulations of the reacting flow were first applied to the can combustor and were carried out with the Eddy Dissipation Concept model, using a detailed reaction mechanism. Combustion reduced the cold flow’s recirculation ratio by a factor 2. By studying the results and understanding the reacting flow, the combustion could be controlled with qualitative refinement of the combustors geometry and repositioning of the inlets of the fuel and secondary air. With the help of using vitiated air as oxidant, these rough alterations led to CO emissions to reduce to less than 50 ppm and NOX to less than 2 ppm, both for cruise and take-off conditions. The more realistic annular combustor did not perform as good when using the final geometry of the can combustor, which led to an increase of CO emissions by an order of magnitude. Due to the area ratio of the annular combustor being roughly six times smaller than that of the can combustor, the velocity of the recirculated flow increased from 24 m/s to nearly 140 m/s, which led to high heat dissipation and a residence time five times as short. Further geometry improvement was more difficult making the emissions harder to control. However CO emissions were eventually reduced to below 200 ppm and NOX below 65 ppm. The annular combustor was also tested to simulate actual flight conditions by also pressurising the inlet air. This led to an improved reaction rate and the CO emissions were further reduced to below 100 ppm. The NOX emissions reduced to below 20 ppm and the pressure loss was less than 3% which was below the imposed requirement of an aero engine combustor.","flameless; combustion; gas turbine; aircraft; aerospace; engine; cfd","en","master thesis","","","","","","","","","Aerospace Engineering","Propulsion and Power","","Flight Performance and Propulsion","",""
"uuid:a1599dd3-8fa0-42fa-acc8-be9deaabaadb","http://resolver.tudelft.nl/uuid:a1599dd3-8fa0-42fa-acc8-be9deaabaadb","Maintenance of a Long Baseline Along-Track Formation for the DelFFI Mission","Deep, A.","Guo, J. (mentor)","2015","","","en","master thesis","","","","","","","","","Aerospace Engineering","Space Systems Engineering","","","",""
"uuid:e3957fb7-e6fa-4e35-91a0-bbf26e30e50e","http://resolver.tudelft.nl/uuid:e3957fb7-e6fa-4e35-91a0-bbf26e30e50e","Design for positive engagement on future personal financial management","Lin, H.","Desmet, P.M.A. (mentor); Pasman, G.J. (mentor)","2015","This project proposed a design solution of positive engagement on personal financial management (PFM) with the help of mobile technology. The end goal of creating engaging PFM is to improve (young) people’s financial well-being. The project identified young bank users as the target user group, who was found to have low financial capability. In the end, a design associating time elements with financial management came into being. A user evaluation was done as an assessment of this idea.","financial management; emotion; motivation; young people; banks; mobile; time","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Master of Science Design for Interaction","",""
"uuid:f55f9c73-c71c-4448-b3e6-ac96dad4b269","http://resolver.tudelft.nl/uuid:f55f9c73-c71c-4448-b3e6-ac96dad4b269","Identification of response amplitude operators for ships based on full scale measurements","Skandali, D.","Metrikine, A.V. (mentor); Ogink, R. (mentor); Lourens, E. (mentor)","2015","Heerema Marine Contractors (HMC) operates several crane vessels, barges and tug boats for the transportation, the installation and the removal of offshore facilities. To achieve safe and successful projects, the accurate prediction of vessel motions is of great importance. Vessel motions are calculated on the basis of wave energy spectra and motion Response Amplitude Operators (RAOs). The wave energy spectra are received from meteorological services or from wave rider buoys and the RAOs are typically calculated using diffraction software, based on potential theory. In order to validate the numerical models, HMC regularly monitors the motions of its vessels. In a number of cases, HMC has noticed that the calculated vessel motions do not fully match with the full scale measurements. This inaccuracy can occur due to several reasons such as forward speed of the vessel, awkward draught, viscous damping forces or the variations of mass distribution during the projects. The main objective of this graduation study is to develop a mathematical method which determines the motion RAOs using the full scale motion measurements and the available wave data. Basically, it is examined whether it is possible to calibrate the following model properties of the vessels: The potential added mass, the potential damping and the potential wave forces. The radii of gyration, the viscous damping matrix and the coordinates of the CoG. The procedure for the identification of the RAOs is applied on a specific vessel with a certain draft: the semi-submersible crane vessel Thialf at 22m draft in deep water. The first challenge of this graduation study is to find a method to express each element of the hydrodynamic database with a limited number of parameters for the entire frequency range. This is accomplished by applying the vector fitting method. According to this method, the hydrodynamic elements are fitted to approximation functions with a certain number of coefficients. In order to calibrate the motion RAOs, a sensitivity analysis is first be performed. The sensitivity analysis leads to the minimization of the number of parameters that is to be examined. The identification process is repeated several times until the best possible modifications are found. In order to choose the correct modification, the normalized root mean square error between the calculated and the measured vessel motions is determined. Apart from the normalised root mean square error, the selection of the possible modifications should be based on logical criteria as well. For this graduation study, the identification method is tested by several cases. These test cases are based on simulated data. The results of this study shows that we can identify several parameters that lead to more accurate response spectra. However, in most cases the solution is not unique and we have to choose between the calibration of two parameters or more. For instance, the same results can be achieved by either changing the radius of gyration or a diagonal element of the hydrodynamic added mass and damping. This can be solved by examining the causes of inaccuracies. If the draft of the vessel is such that the awkward draft does not occur, then the hydrodynamic data base should not be modified. Small changes of the viscous damping could not be identified for the SSCV ‘Thialf’. The viscous damping is much lower than the hydrodynamic damping and as a consequence, it doesn’t have noticeable impact on the vessel responses. However, this is not valid for rolling ships such as the deep-water construction vessel Aegir. Finally, it should be mentioned that the errors of the data processing influence the accuracy of the identification procedure.","RAO; identification; response amplitude operators; measurements; full scale measurements; vessel motions; vector fitting; curve fitting; offshore engineering; marine; ships; vessels; hydrodynamics; validation; hydromechanics; ship motions; ship responses","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Maritime and Transportation Technology","","Offshore and Dredging Engineering","",""
"uuid:b531b356-5a5f-47b0-ab00-9069f9385d0e","http://resolver.tudelft.nl/uuid:b531b356-5a5f-47b0-ab00-9069f9385d0e","Determination of optimal separation well distance for single borehole ATES systems, in the Netherlands, implementing an an axi-symmetric numerical model","Xynogalou, M.","Olsthoorn, T.N. (mentor)","2015","Aquifer thermal energy storage (ATES) is a growing technology in the Netherlands. There are two kinds of ATES configurations the doublet and the single borehole ATES (SB-ATES) layout. The limited subsurface space in combination with the lower construction cost and the lower performance of the SB-ATES lead to the need to optimize their design. This master thesis focuses on gaining a better insight in the processes that occur around this configuration. Specifically, how anisotropy influences the efficiency of the design and what the optimal distance between filter screens is, in order to limit the interference between the screens, as it has a negative impact on system performance. To meet these two objectives, an axisymmetric numerical model was developed in a MATLAB environment, using MODFLOW and MT3DMS or SEAWAT groundwater flow and transport simulators. The simulation of heat advection was conducted applying the finite different method (FD method), it was the only method compatible with axial symmetric models that produced consistent results. As the FD method is subjected to numerical dispersion, three different grid resolutions were tested that were the 0.25, 0.50 and 1.00 m, respectively. The finest grid was decided to be used in the elaboration, as it gave the most accurate results compared to larger thickness width grid cells. Capacity test and borehole profile data were used to calibrate the overall vertical anisotropy of the case-studies. The capacity test allowed the calibration of one hydraulic parameter, for which the overall vertical anisotropy was chosen. The Kozeny-Carmen equation was used to calculate the hydraulic conductivity of each soil layer. This overall vertical anisotropy even though was estimated roughly, can be used to determine the presence of overlooked clay layers during the drilling of the borehole (anisotropy <<2) or whether high permeable layers are between the screens (anisotropy >>10). A sensitivity analysis was applied to estimate the optimum distance between the filter screens. Three different types of SB-ATES, called GT15xx, GT20xx and GT25xx, were examined separately. The numbers indicate the installed pump capacity in m3/h while their mean and representative screen lengths are 5, 7 and 10m respectively. For the sensitivity analysis, three discharge fractions, Qfrac, were tested with values 0,25, 0.50 and 1.00 and three anisotropy values of 2, 5 and 10, these values are representative for sandy soils. The simulation time was 5 years, which was sufficient to the recommended efficiencies. The results showed that the maximum efficiency is practical independent of system type. The optimum separation distance for an anisotropy of 2 is respectively, 25, 30 and 35 m for Qfrac 0.25, 0.50 and 1.00. The evaluation of the sensitivity analysis was conducted, using real-scale case-studies and taking into account the distribution of conductivities along the layers. The available 18 case-studies were examined in terms of efficiency; it was found that a thin resistance layer between the screens, like a clay or peat layer, influences positively the performance of SB-ATES systems. On the other hands, when there is a high conductive zone between the screens, the efficiency drops. Finally, it was found that temperature induced differences in density and viscosity have a negligible effect on SB-ATES systems, at least with injection temperature differences between warm and cold wells up to 120C.","groundwater; Hydrogeology; ATES; SB-ATES; single borehole; aquifer; aquifer energy storage","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","Hydrogeology","",""
"uuid:6bb16a01-a8a5-47b7-9dd4-90ce01c6560e","http://resolver.tudelft.nl/uuid:6bb16a01-a8a5-47b7-9dd4-90ce01c6560e","The relationship between employee satisfaction and customer satisfaction","Gijzel, J.C.R.","Verburg, R.M. (mentor); Rook, L. (mentor); Kortmann, L.J. (mentor)","2015","","employee satisfaction; customer satisfaction; organizational behaviour; customer relationship management; organisational resources; work engagement; service climate; service quality; employee performance; customer loyalty; KLM; airline","en","master thesis","","","","","","","","2016-09-01","Technology, Policy and Management","Values, Technology and Innovation","","Economics of Technology and Innovation","",""
"uuid:0c7c2208-c1d8-4bcf-b679-b0567da18e26","http://resolver.tudelft.nl/uuid:0c7c2208-c1d8-4bcf-b679-b0567da18e26","The effects of trust on the effectiveness of project risk management for engineering and construction projects","Robert Assis, D.","Vrancken, J.L.M. (mentor)","2015","The main objective of this research project is to explore the potential correlation between the level of trust between project managers and clients and the effectiveness of their Project Risk Management. The research project is executed in collaboration with Tebodin, a Dutch company that offers engineering and consultancy services worldwide; therefore, one of the project’s main targets is to professionalize Tebodin’s PM practices through the implementation of managerial recommendations for the PM department. Trust is a complex psychological state (Rosseau, Sitkin Burt & Camerer1998) that has plethora of different definitions that are often derived from discipline-driven studies subsequently causing ambiguity and controversy (Brewer & Strahorn 2012). Nevertheless, the definitions presented in this documented highlight the importance of vulnerability as a key component of trust between two co-existing individuals; subsequently, vulnerability brings up the concept of uncertainty which is an important concept when dealing with Project Risk Management. The idea behind the conceptual model that is tested in this research project is that trust serves as a driver for projects managers and clients to engage in productive communication when using Project Risk Management techniques that include human interaction; “If trust is present, people can spontaneously engage in constructive interaction without pondering what hidden motives exchange partners might have, who is formally responsible for problems, or the risks of disclosing information” (Kadefors 2004, pg. 176). The scope of the research follows a 6-phase process starting by performing a literature review on types and dynamics of trust as well as models and techniques of project risk management. The objective of the literature review is to be able to find a method to measure the level of trust and the performance of the Project Risk Management executed in the ten selected projects for analysis. For this study, the dimensions described by Hartman (1999) serve as building blocks to define the basis of trust between project managers and clients’ project managers. Competence trust, Integrity trust and Intuitive trust are the notions that better match the scope of the project as being focused on a project manager-client relationship. Project Risk Management is operationalized according to present practices at Tebodin which are based on existing literature (PMBOK, Gray & Larson 2008, Hillson & Simon 2012) Among such practices certain tools are used in which trust is likely to influence performance (Raz & Michael 2001). Then, two surveys are formulated to measure the variables of the conceptual model: the level of trust (LOT) and the Effectiveness of Project Risk Management (EPRM). The analysis consists of comparing these two in order to verify the existence of a correlation. Moreover, the approach of the research project is to analyze how trust develops in two different scenarios: on the one hand there are projects that were conducted on a partnering basis and on the other hand there are projects that were performed in an operational environment. Therefore, projects were selected with certain criteria including the type of working environment. After analyzing quantitatively and qualitatively the data obtained from ten different construction and engineering projects at Tebodin, there is evidence to claim that there is a significant correlation between the level of trust from clients towards project managers and the effectiveness of the risk management techniques that involve human interaction. Hartman’s three trust dimensions were tested separately against the variable EPRM to verify their individual correlation and the only dimension that showed a significant level of correlation was integrity trust. The data showed that projects belonging to the type-partnering environment ranked in average higher on both variables, LOT and EPRM, than the projects under the type-operational environment conditions. Furthermore, the qualitative analysis performed supports the correlations explained before. Interviews with project managers and Customer satisfaction reports filled by the clients describe specific incidents that support the correlations between studied variables. Important aspects such as cultural differences, client proximity, technical affinity, technological complexity and project manager-client professional history were identified and supported with existing literature on Project Management. These aspects gave insight into the data provided by project managers and their clients which created the correlation lines between the studied variables. The last section of the document includes as part of the conclusions of the research a list of recommendations for future research that include: 1. analyze the importance of Meyerson’s concept “swift trust” in projects executed in operational environments or as she defines it: temporary organizational structures (Meyerson, Weick & Kramer 1996); 2. Characterization of the trust between members of the project organization in terms of its resiliency and fragility and how could project managers deal with the two different types; 3. Further develop the effects of the level of trust in the control mechanisms used in Project Risk Management procedures. Under the same section a list of managerial recommendations for Tebodin project managers is given including: 1. Start a program on trust management among all project managers to emphasize the importance of trust on the EPRM techniques used at Tebodin; 2. Emphasize the importance of partnering environments for the execution of projects; 3. Include the concept of integrity trust in future Customer Satisfaction reports; 4. Continue the research on trust dimensions and their effects on EPRM by enlarging the data sample. Finally, the most important limitation of this research project was the reduced data sample of projects; this had undesirable repercussions on the quantitative analysis because the quantitative methods to analyze that type of data were limited to nonparametric techniques. The statistical analysis that was performed was non-parametric because the assumption of normality due to the small sample size could not be made. The consequences this had on the results is that the conclusions about the hypotheses are rather signaling a certain behavior but do not entirely confirm the conceptual model.","trust; trust dimensions; project organization; project risk management","en","master thesis","","","","","","","","","Technology, Policy and Management","Management","","management of technology","",""
"uuid:4eb016fa-9e4a-4fc5-a128-99f762cfbff3","http://resolver.tudelft.nl/uuid:4eb016fa-9e4a-4fc5-a128-99f762cfbff3","Designing a multi-sports helmet based on 3D ergonomic data","Van Seggelen, S.W.H.","Molenbroek, J.F.M. (mentor); De Smit, A. (mentor); Dekker, L. (mentor); Heusschen, C. (mentor)","2015","For a Dutch helmet manufacturer, a product proposal has been made. The key feature of the product, is the development done based on 3D scanning techniques. The actual project was twofold; The scientific part of the project was about developing an algorithm for incorporating 3D scanning data in product design. The main challenge was to automatically align the scans based in the Grasshopper environment. After this, an envelope based on a certain percentile in 3D space could be retrieved which is direct geometry input for the actual design, without the need of traditional measurements. Subdivision modeling was used to have full control over the thickness of the different layers of the product. The other part of the project contains the analysis of the company and industry, in an internal and external analysis. Via this way, the strengths, weaknesses, opportunities and threats are being discovered. This analysis output has been the input for the synthesis part; A classical product development which consisted of sketching, modeling and prototyping of the actual helmet.","helmet; 3D scanning; landmarks; morphometrics; aligning meshes; genetic solver; Grasshopper; algorithm; subdivision modeling; workflow","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design Engineering","","Master of Science Integrated Product Design","",""
"uuid:3cf91cf8-2288-48fb-a445-cd0860196a17","http://resolver.tudelft.nl/uuid:3cf91cf8-2288-48fb-a445-cd0860196a17","Sustainable LNG regasification terminal","Quirijns, S.","Velllinga, T. (mentor); Quist, P. (mentor)","2015","In Ukraine a project is initiated to increase their energy independence, which is realized with diversifying the natural gas supply. By expanding the port of Yuzhny with an import LNG regasification terminal, it is possible to import LNG from various production plants. The scope separates the value of the terminal in short term and long term value. Three types of unloading concepts are compared for technical feasibility, which are the onshore Conventional Terminal, Floating Storage and Regasification Unit, and Gravity Based Structure. The FSRU resulted in the most optimal solution in these conditions. A preliminary design is made for the FSRU where various mooring structures are compared for constructability. The end-result is an single point mooring structure, where the FSRU serves as a mooring point for the berthing LNG carriers.","LNG; Regasification Terminal; Sustainability; Technical Feasibility","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Ports, Rivers and Dredging","","46.605265, 31.019107"
"uuid:c7e2d0a4-f0e9-4317-9fc2-5d0bc63b48b3","http://resolver.tudelft.nl/uuid:c7e2d0a4-f0e9-4317-9fc2-5d0bc63b48b3","Design of a Performance Benchmarker for Fully Distributed IaaS Clouds","Cilissen, M.H.J.; Van Elsas, M.","Iosup, A. (mentor); Feliksik, E. (mentor); De Meijer, M. (mentor); Larson, M.A. (mentor)","2015","Nerdalize B.V. is an infrastructure-as-a-service (IaaS) cloud provider aiming to offer substantially lower prices than its competitors. In order to visualize its cost savings to customers and measure its own systems against competitors in a cloud market reigned by opaque pricing models, it would like to utilize an application benchmarker to give customers insight into the resource utilization and operation costs of their applications among various cloud providers. We have done research into the fields of cloud computing, benchmarking and the intersection thereof, determined requirements for such a benchmarker, and assessed any existing solutions in the field. We then chose Nerdalize’s internal prototype implementation as a suitable base to develop a fully featured, production-ready application benchmarker. We identified five main design goals of correctness, robustness, security, extensibility and maintainability. We then analyzed and prioritized potential improvements and extensions to this prototype, and implemented them in an agile-driven Extreme Programming (XP) development process. The main contributions lie in designing and implementing a fully automated test suite and system, vastly improving the accuracy and stability of the benchmarker, re-designing the deployment model from monolithic to modularized and extensible, implementing support for provisioning to arbitrary Linux-based hosts, and deployment of complex workload architectures. We then experimentally verify the accuracy of the benchmarker, and assess that its deployment overhead is very small to negligible, and run several tests against real-world cloud providers. The end result of this project is a stable, well-tested, featured benchmarker application that is used in production environments at Nerdalize.","cloud; benchmark; iaas; distributed; performance; infrastructure-as-a-service","en","bachelor thesis","","","","","","","","2016-12-31","Electrical Engineering, Mathematics and Computer Science","Parallel and Distributed Systems","","Bachelor Computer Science and Engineering","",""
"uuid:5ae39fb9-f7a9-4010-8dfe-b88ccc89ec61","http://resolver.tudelft.nl/uuid:5ae39fb9-f7a9-4010-8dfe-b88ccc89ec61","Power Balance Control of DC Microgrids","Chaudhuri, S.","Ramirez Elizondo, L.M. (mentor); Mackay, L.J. (mentor)","2015","Control techniques for DC microgrid.","DC microgrid Control","en","master thesis","","","","","","","","2016-09-30","Electrical Engineering, Mathematics and Computer Science","Electical Sustainable Energy","","","",""
"uuid:98a4ddb9-0cd9-4a7c-bab1-b0c215977425","http://resolver.tudelft.nl/uuid:98a4ddb9-0cd9-4a7c-bab1-b0c215977425","The Passive Display Case","Chang Lara, C.","Luscuere, P.G. (mentor); Van der Spoel, W. (mentor); Schipper, H.R. (mentor); Pleysier, J.A. (mentor)","2015","Display cases around the world contain some of the most precious objects of our cultural heritage, from Leonardo Da Vinci’s La Gioconda to the Gutenberg Bible. We trust that these display cases will protect our objects of cultural heritage from vandalism, light, pollution and most importantly, moisture. Nevertheless, museums sometimes find that although their objects of art are in display cases, these are not providing the ideal protective microclimate. Hygroscopic materials can help restore this protective microclimate, but there are situations where the use of moisture-buffering materials is ineffective. This project aims to find when these situations occur. In order to do this, a computational and experimental model were developed, and the results are evaluated in this report.","hygroscopic materials; display case; conservation of art","en","master thesis","","","","","","","","2015-10-02","Civil Engineering and Geosciences","Structural Engineering","","Building Engineering/Building Technology & Physics","",""
"uuid:7840e15c-74aa-4e5f-8552-867cf17b4143","http://resolver.tudelft.nl/uuid:7840e15c-74aa-4e5f-8552-867cf17b4143","Designing the green energy highway with on-road contactless power transfer.","Scheele, N.","Silvester, S. (mentor); Zijlstra, J. (mentor)","2015","This thesis presents the implementation of Inductive Power Transfer on the A12 in the Netherlands. On-road inductive power transfer allows electric vehicles to charge while driving. This technology can realize higher driving ranges of vehicles, but also allows decreasing the battery size what would result in a higher performance of the vehicle. Power is transfered from the inverter to the primary coil. Via an electromagnetic flux field, energy is transfered via an air gap to the pickup of the vehicle. The pickup transfers the energy to the motor or the battery. Municipalities want to reduce the amount of fossil fueled vehicles in their cities and stimulate the use of electric vehicles. Though the biggest threshold for consumers is the low driving range of the electric vehicle. The current electric vehicles, apart from Tesla, have an average driving range of 100-130km. This is perfect for daily transportation within the city, but does not allow the vehicle to go far in one drive or unplanned trips. Driving from The Hague to Nijmegen, a very common daily journey for many Dutch people, the electric vehicle would need a 30-minute recharge just after Utrecht. The low driving range and long ‘refueling’ of the vehicles cause discomfort, so called ‘range anxiety’, among its users and is the biggest reason for other drivers to choose an other type of vehicle. In 2030 the share of full autonomous vehicles will be more than 10% and will first be introduced on the highway before they will be allowed to drive full autonomous in cities. The autonomous vehicle will decrease the number of road accidents and the traffic jams on the highway. The autonomous vehicle will also be used for shared vehicle services. Although the vehicles will be shared more often, the amount of traffic will not decrease since the target group for full autonomous vehicles will increase. More people will be able/allowed to use this type of vehicle. The vehicles will also cover 10% more of the current average distances, due to shared services. On-road charging will not only decrease range anxiety, it will also support the growth of autonomous electric vehicles and their shared services. n this thesis the design is worked out for the A12 in the Netherlands. The A12 is one of the highways in the Netherlands with a daily high density of traffic that covers long distances (more than 100km in one journey). This highway runs horizontally across the middle of the Netherlands and connects citizens of multiple big Dutch cities together. Travelling along the A12 the user will find himself in many different landscapes with different impressions. The design proposal in this thesis focuses on the use of autonomous electric vehicles in 2030. The combination of both technological developments allows the user to feel completely free on the highway; the user does not have to keep an eye on the road and does not have to worry about the range of the vehicle. Therefore the design proposal is based on the following vision: The design of the inductive power transfer highway should enable the user to feel ‘sustainable’ and comfortable. The design proposal is based on three design qualities: 1. Making the user feel more sustainable by visually linking the renewable energy sources to the charging sections of the inductive power transfer highway. 2. Stimulation of comfort use of autonomous vehicles by reserving a lane for autonomous vehicles only. The autonomous vehicles will be separated from the normal highway traffic and can therefore perform better. 3. The renewable energy sources are installed in a way that does not interfere with the view on the landscapes. The emergency lanes are covered with solar panels; the so-called solaroad, and Vortex are placed on several places along the banks and the median. In one journey the vehicle encounters two charge sections on the A12, forming together a coverage of 30% of the total driving distance, which in total charge the vehicle enough to travel from The Hague to Nijmegen in one journey. The vehicle can immediately return from Nijmegen to The Hague with only on-road charging. The energy of this round trip is 50% charged on the road. The energy used for on-road charging is 100% generated by renewable energy sources installed along the highway. The location of the IPT system is for both directions installed on locations where traffic with other destinations will be able to charge when their journey is overlapping with the A12. Upon installing the primary coils for inductive power transfer, the toplayer will be renewed with self-healing asphalt. The lifetime of a regular top layer is around 5 years, while self-healing asphalt can last at least 20 years. Every few years the toplayer receives special treatment by induction heating of small iron fibers that are mixed in the bitumen in the top layer. By heating up the fibers, the bitumen becomes soft and cracks in the layer will heal. All the emergency lanes on the highway will be replaced by solaroad. Since this lane creates a very large surface and is often not in use it is a perfect location for harvesting solar energy. A Vortex is a 13m high pole made of reinforced fiberglass, which generates energy by the speed of the wind. Since it has no blades and it takes per Vortex a small surface it is an ideal product to install along the highway. In relation to regular wind turbines, Vortexes are cheaper to use","contactles power transfer; green energy highway; inductive power transfer; the highway in 2030; contactless charging; dynamic charging","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Master of Science Integrated Product Design","",""
"uuid:eff78299-c32a-418f-a910-e6bbe82e0c67","http://resolver.tudelft.nl/uuid:eff78299-c32a-418f-a910-e6bbe82e0c67","Revived Beauty: Researching aesthetic pleasure in material experience to valorise waste in design","Sauerwein, M.","Karana, E. (mentor); Rognoli, V. (mentor)","2015","Eco-friendly materials have great potential to foster sustainability, but there is limited progress in relation to aesthetics. Therefore the focus of this study is to explore the aesthetic pleasure in eco-friendly material design. The main objective is to address the following question: How should materials derived from discarded raw materials (i.e. revived materials) be mobilized or be manipulated in order to be appraised as aesthetically pleasant? Part 1 Exploration Eco-friendly materials can be divided into two main categories, i.e. raw materials derived from nature and raw materials derived from waste. This research project will focus a sub category that I defined as materials derived from discarded raw materials and named ‘revived materials’. Revived materials were chosen, because they are an emerging category and create a double benefit within the life cycle analysis, but waste as raw material does not seem to benefit the image. A checklist was developed in order to select these materials. Eighteen revived materials were selected and fourteen samples were obtained. Most of these materials are eco-friendly alternatives for existing materials and commercially available. Improved aesthetics are needed for design for sustainability. A new aesthetic approach is required to perceive aesthetic pleasure for revived materials. Only little information is available on the aesthetic values of eco-friendly materials and restricted to only general principles of aesthetic pleasure. In this study, grounding on the aesthetic universals (Hekkert, 2006), I developed the aesthetic universals in an attempt to make the general principles applicable for revived materials. Part 2 Investigation These aesthetic universals were applied on revived material samples and evaluated in two expert meetings. This required in a new approach: To defined the aesthetically pleasant criteria, the universals should be explored in relation to tactile and visual material qualities and their interrelationships. Accordingly each aesthetic universal obtained a new definition: Contrast: there is a sensory incongruity between vision and touch Originality: the visual expectation differs from the tactile experience Solving puzzles: the visual domain is ambiguous, but the tactile experience is unambiguous. Peak shift: the visual expectation is enlarged by the tactile experience Similarity: there is a sensory congruity between vision and touch Simplicity: there is a consistency between the visual and tactile experience and a minimum of parts is experienced in the visual and tactile domain. Symmetry: there is a an ambiguity in the visual domain, which is mirrored by the tactile experience. Subsequently, these descriptions were linked to the visual and tactile properties of revived materials in an user study (N=30). This study is the first attempt to find general principles for aesthetic pleasure in material design. Multi-sensorial aspects were evaluated in both the visual and tactile domain and linked to the aesthetic pleasure in response to the material. However, the results turned out to be too detailed for practical use. Therefore only a distinction between similarity, variance and contrast was made, which was linked to beauty and willingness to design. Significant relations were found for regularity and fibre when associated with willingness to design. It showed that contrast is the desired relationship between the visual and tactile perception. This is supported by the comments of the subjects, as well as a clear pattern between the mean plots of all other multi-sensorial aspects in relation to beauty and willingness to design. The combined data lead to a new hypothesis: Contrast between the visual and tactile qualities of revived materials favours aesthetically pleasant experiences To test this hypothesis Ecovative, Goodhout and Palmleather were manipulated. The intention was to create one sample with a similar and one with a contrasting sensorial experience for each of those materials. Several material tests were performed and the six samples were created for the final user study. Part 3 Validation In the final user study the manipulated materials were tested for inter-sensorial relation as well as aesthetic pleasure (N=38). Only a significant sensorial difference was found between the samples for Goodhout, but neither samples are explicitly similar nor contrasting. In a second test, the indicated contrasting sample scored higher on beauty. However, within the contrasting sample a negative correlation was found between beauty and similarity. In other words, the outcome of the final user test is not consistent and suggests that other factors may influence the aesthetic pleasure. The combined data make further research necessary and are indicative for a certain differentiation between visual and tactile experiences. In addition, this thesis introduced the method to study the inter-sensorial aspects in order to find aesthetic pleasure for materials. This method may not be limited to the revived material category, but has the potential to be applied on other materials. The graduation project completes with the design of an exhibition in which the journey of the research project is shown.","eco-friendly materials; aesthetic pleasure; inter-sensorial relationship; sustainability; revived materials (materials from waste)","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design Engineering","","Master of Science Integrated Product Design","",""
"uuid:682a56ed-8e21-4b70-af11-0e8e9e298fa2","http://resolver.tudelft.nl/uuid:682a56ed-8e21-4b70-af11-0e8e9e298fa2","Consolidated Deep Actor Critic Networks","Van der Laan, T.A.","Loog, M. (mentor); Kober, J. (mentor)","2015","The works [Volodymyr et al. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.] and [Volodymyr et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.] have demonstrated the power of combining deep neural networks with Watkins Q learning. They introduce deep Q networks (DQN) that learn to associate High dimensional inputs with Q values in order to produce discrete actions, allowing the system to learn complex strategies and play Atari games such as Breakout and Space invaders. Although powerful the system is limited to discrete actions. If we wish to control more complex systems like robots we need the ability to output multidimensional continuous actions. In this paper we investigate how to combine deep neural networks with actor critic models which have the ability to output multidimensional continuous actions. We name this class of systems deep actor critic networks (DACN) following the DQN naming convention. We derive and experiment with four methods to update the actor. We then consolidate the actor and critic networks into one unified network which we name consolidated deep actor critic networks (C-DACN). We hypothesize that consolidating the actor and critic networks might lead to faster convergence. We test the system in two environments named Acrobot (under actuated double pendulum) and Bounce (continuous action Atari Breakout look alike).","reinforcement learning; deep learning; actor critic model; experience replay","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Pattern Recognition and Bioinformatics","","Computer Science","",""
"uuid:09244c43-76db-4575-816f-73619b22b50d","http://resolver.tudelft.nl/uuid:09244c43-76db-4575-816f-73619b22b50d","Computational Modeling of a Biomass Micro-gasifier Cookstove","Asranna, A.","Benediks, B.J. (mentor); Van der Sluis, P. (mentor); Pourquie, M.J.B.M. (mentor); De Jong, W. (mentor)","2015","The World Health Organization (WHO) states that indoor air pollution resulting from inefficient burning of biomass in traditional cookstoves is a major health hazard affecting around 2.7 billion people globally. With the use of biomass for cooking energy expected to grow in the coming decades, efficient energy conversion technologies are an urgent need. The Philips woodstove based on the principle of micro-gasification of biomass is one such technology having the lowest reported emissions among wood burning cookstoves. The stove is based on experimental design and gaps exist in understanding the physics involved. Integration of simulations into the design process is expected fill these gaps and enable the characterization of key design parameters. A computational fluid dynamics based simulation model is developed for this purpose to investigate the fluid flows and heat transfer in the secondary combustion zone of the micro-gasifier stove. A three dimensional model of the stove is built using COMSOL Multiphysics. Combustion is modelled as a volumetric heat source with uniform heat generation. With appropriate boundary conditions a close approximation of the stove operating conditions is obtained and convergence achieved. The results demonstrate that the model is partially successful in simulating the flaming mode operation of the stove. The flow pattern is in agreement with visual observations. The temperature field inside the combustion chamber suffers because of the uniform heat generation assumption. The highest temperatures occur around regions of flow stagnation which is not representative of the actual operating conditions. The temperature values outside the combustion chamber are more reasonable, however still inaccurate. Including all the design details is expected to return more agreeable results. It has been realized that the discrepancy in the temperature field is because the secondary air flow rate specified in the model is only 43% of the experimental value. By specifying a secondary air flow rate closer to the experimental value, a closer approximation of the operating conditions is expected. A mixing analysis is carried out for two different heights of the fuel bed using passive scalars. The recirculation zone is found to have a significant impact on the mixing of the two streams. The simulations predict that at higher fuel bed heights the recirculation zone is restricted in space leading to poor mixing of the air and fuel stream. A validation study of the mesh used is conducted to ascertain its accuracy. A high Reynolds number jet is simulated and the results are compared with experimental values. The centerline velocity decay obtained using the mesh in regions close to nozzle exit is within 10% of the experimental values. The lateral velocity predictions are 20% lower than the experimental values. Since the distances in the computational domain do not extend beyond x=15D, the mesh used is expected to return accurate results. In its current state, the model is a good tool for flow visualization and understanding the broad qualitative trends. For comprehensively characterizing the design parameters of the stove, a better definition of the heat source term and reactive flow modelling is necessary.","Cookstoves; Biomass; Gasifier stove; Philips woodstove","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Process and Energy","","","",""
"uuid:d7605981-7506-477a-aa97-f265230ff655","http://resolver.tudelft.nl/uuid:d7605981-7506-477a-aa97-f265230ff655","Transverse Strength of a Twin Hull FPSO","Poen, G.J.","Romeijn, A. (mentor); Bosman, T.N. (mentor); Den Besten, J.H. (mentor); Miedema, S.A. (mentor)","2015","The size and production capacity of Floating Production, Storage and Offloading units, or FPSOs, is ever growing. SBM Offshore specialized in converting existing oil tankers into FPSOs. One of the limitations they found was the available deck space for the topsides modules. A new concept was proposed to solve this problem: a twin hull FPSO, made of two tanker hulls connected side to side by a slim, rigid structure. An initial study done by SBM suggested the transverse bulkheads were the most critical components, failing on shear stress. The main question addressed in this thesis is: How does the twin hull structure behave and is it strong enough? To answer this, a finite element twin hull and mono hull model was created containing only the main structural components. To avoid extensive modelling, all stiffened panels were modelled as one continuous, orthotropic laminate. A second, very simple, 2-D model made of beam elements was created to be used as a crude but simple tool to predict shear forces in the structure. A number of still water load cases were applied to both mono hull and twin hull models. The beam model shows an error margin of 10-30% compared the more detailed model. In the twin hull vessel the main deck is stressed less by applied loads, but the transverse components are stressed more, mainly by shear. Increasing the transverse strength by closing the swash bulkheads will bring down stresses in the transverse bulkheads to acceptable levels. The twin hull connection structure puts too much shear force and bending moment on the vessel as it is. It needs to be ballasted to decrease bending stresses in the main deck and shear stresses in the transverse bulkheads.","FPSO; still water; twin hull; transverse; bulkheads; beam grid; layup","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Maritime Technology","","Science Master - Ship & Offshore Structures","",""
"uuid:890f32a1-d09f-4c78-a44e-733f594e9717","http://resolver.tudelft.nl/uuid:890f32a1-d09f-4c78-a44e-733f594e9717","Composite Floors: A Theoretical Research into the Design of Steel-Concrete Composite Floors with a Bigger Unpropped Span of 7.2 m","Van Blokland, J.","Bijlaard, F.S.K. (mentor); Abspoel, R. (mentor); Pasterkamp, S. (mentor); Stark, R. (mentor); Prins, H. (mentor)","2015","In the Netherlands buildings are designed using a grid with multiples of 3.6 meter. Within these designs floor spans of 7.2 meter are popular (double grid size). The deep decks of composite floors at this moment in time are designed to reach an unpropped span of 5.5 meter. This master research is focused on finding a possible deck design for a steel-concrete composite floor slab that can span 7.2 m and that is constructed without the need of temporary supports. This resulted in the JorFlor, a light-weight, big span steel-concrete composite floor that can compete with current floor systems.","steel-concrete composite floor; unpropped; big span; design","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Design & Construction","",""
"uuid:a062ee46-2731-45fb-9774-8c6ea480e309","http://resolver.tudelft.nl/uuid:a062ee46-2731-45fb-9774-8c6ea480e309","Investigation into the erosion velocity of grains subjected to a constant water flow","Smit, A.A.J.","Van Rhee, C. (mentor)","2015","To remove rocks from objects at the seabed, a mass flow jet could be the solution. The knowledge on the jet production is very scarse for larger particles (larger than 1.5mm). This knowledge starts with a good model for the erosion velocity of grains. Nowadays, only models are validated up to a mean particle diameter of 1.5mm. To give a better funded base for extrapolating the existing models to larger rock sizes, scaled tests are executed. The results of these scale tests are then compared to existing models. It is concluded that extrapolating existing theories can be done.","","en","master thesis","","","","","","","","2020-09-23","Mechanical, Maritime and Materials Engineering","Offshore and Dredging Engineering","","Dredging Engineering","",""
"uuid:0453e599-de18-4884-8039-25dda81512da","http://resolver.tudelft.nl/uuid:0453e599-de18-4884-8039-25dda81512da","Design of an E-Cargo Bike","Krishna, K.","Kuipers, H. (mentor); Zijlstra, J. (mentor)","2015","A cargo bike is essentially a vehicle that is human powered and has provisions for the transportation of goods. Cargo bikes have been in use in different forms since the 1920s. Cargo bikes can also be referred to by different names depending on geographic location, context of use or even by the number of wheels it has, namely freight bicycle, carrier cycles, cargo trike, Long John, Long tail, Porteur bicycle, Bakfiets and rickshaw. These bikes have been used by various professionals like bakers, milkmen and delivery men in the past and have now been adopted by various professions like street vendors, advertising distributors, municipal garbage collectors as well as the urban parent. The post-modern urban transportation scene is taking a step back in history by embracing the cargo bike as a solution that is economical, sustainable and most importantly, fast. These bikes are quite popular in the cities of Copenhagen and Amsterdam; 25% of families in Copenhagen with 2 or 3 children own a cargo bike. Germany is seeing a rapid rise in interest for cargo bikes owing to the initiatives of the government. Many old and new cargo bike manufactures have a very fairly large market in the country. Several local cargo bike dealers and initiatives have helped in spreading awareness about them. Germany is also the biggest market presently for Van Raam bicycles. Van Raam presently designs and manufactures only special needs bikes. They have been approached by a German client, E-Motion Technologies to design and develop an electric cargo trike that can be sold in the German market. E-Motion Technologies have a very large network of dealers in Germany. Evaluating the market for cargo bikes in Germany, the marketing strength of E-Motion and the design & engineering capabilities at Van Raam this project is a strategic move forward for Van Raam in tapping a new market. Van Raam finds this opportunity as a starting point to expand their portfolio by introducing a new line of cargo bikes. This project illustrates the process of developing an electric cargo bike and finally presents the new cargo bike design.","cargo bike; electric; design aesthetics; Germany","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Master of Science Integrated Product Design","",""
"uuid:10ee1352-2f45-46b5-babc-6ea7d3579995","http://resolver.tudelft.nl/uuid:10ee1352-2f45-46b5-babc-6ea7d3579995","The Development of an Inertia Estimation Method to Support Handling Quality Assessment","Mutluay, T.","Vos, R. (mentor)","2015","Mass of an aircraft as well as distribution of it along the body are important concerns during the design process due to their significant influence on performance and inertia. Inertia of an aircraft has to be known in order to examine the handling qualities, which have a large effect on aircraft’s design. However, due to the smeared properties of the structure and many of the system architectures, the calculation of inertia is nontrivial. Moreover, non-traditional aircraft designs might not be able to rely on existing empirical calculations. Hence, the main objective of this project is developing a rapid geometry based inertia estimation applicable for any aircraft configuration. In order to have a design sensitive method, the aircraft is divided into three categories namely bodies of revolution, lifting surfaces and point masses where components are grouped according to their shapes. Fuselage and nacelle components are categorized as bodies of revolution and their inertias are estimated in a similar fashion. Lifting surfaces include main wing, horizontal and vertical stabilizers. The rest of the aircraft components are assumed to be point masses. The implemented method is used in conventional, canard and three surface configurations. In order to examine their handling qualities, control and stability derivatives are also needed which is why a calculation method is implemented. Calculated inertia and derivatives of each configuration are used as input in a flight mechanics toolbox. Hence, the influence of configuration on aircraft’s inertia and consequently on aircraft behavior has been able to examined in a rapid process.","moment of inertia; stability derivatives; handling quality","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Engineering","","Flight Performance and Propulsion","",""
"uuid:a93514c2-721a-490f-bcda-626dec1d61c8","http://resolver.tudelft.nl/uuid:a93514c2-721a-490f-bcda-626dec1d61c8","The Accuracy and Error Sources of Radar-derived Bathymetry by the XMFit Algorithm","Tenthof van Noorden, C.","Reniers, A.J.H.M. (mentor); Friedman, J.A. (mentor); Hoekstra, R. (mentor); Radermacher, M. (mentor); Swinkels, C.M. (mentor)","2015","The coastal bathymetry is one of the most essential inputs for understanding, monitoring and modelling the coastal environment. A remote sensing technique called marine radar may overcome disadvantages of present monitoring techniques and has the advantage to obtain measurements with a high spatial and temporal resolution. A Fast Fourier Transformation on the radar images produces pairs of wavenumbers and wave frequencies that can be linked to water depths and velocities by means of the dispersion relation. The drawback of present radar processing software is it closed and fixed nature and unclear accuracy. Therefore an X-band Matlab Fitting algorithm (XMFit) was developed by Friedman (2014). This development was induced by a radar stationed near the Sand Motor, a large nourishment, near Kijkduin in the Netherlands. However, XMFit requires further validation and a solution for the present bias in the depth estimate. Therefore, the goal of this thesis is to determine the accuracy of XMFit radar-derived bathymetry at the Sand Motor for a storm in October 2014 and determine the cause for the present overestimation of the water depth by XMFit. The accuracy is obtained by validation studies for the Sand Motor and Ameland radar and the error sources follow from analyses of the 3D wavenumber frequency space (kw-space). The Sand Motor validation study shows a mean depth bias of XMFit of 4.21 m. The XMFit accuracy depends on waveheight, wave period, precipitation and water depth. Additionally, fields of XMFit flow velocities sometimes show a growing eddy on the lee side of the Sand Motor. A mean depth bias of 1.04 m is found for a comparable validation study at Ameland. In contradiction with the Sand Motor, the Ameland depth bias depends differently on the metocean conditions, the water depth and the radar range. Nine different error sources were identified in this thesis. The in situ data, the fitting procedure of the XMFit and the bottom slope were found to have a negligible contribution to the depth bias. The metocean conditions, the precipitation, the Rhine's fresh water plume, amplitude dispersion and the water depth itself do have a considerable effect on the depth bias. Precipitation explains many of the large outliers of the XMFit depth estimate. Some other extreme outliers coincide with the passing of the fresh water plume. The increased depth bias for large ranges at the Sand Motor is due to the increased bias at deeper water depths and not due to the incidence angle. A comparison of the in situ depth with the depth that can actually be felt by the existing wave climate showed that the applicability of the dispersion relation is stretched too far at the Sand Motor. A filter has been developed separating valid from invalid results based on the L<L0-criterion. The invalid depth estimates show a 2 m higher depth bias than the valid depth estimates. XMFit's reduced accuracy at the shallower water depths can be attributed to the fact that XMFit does not include amplitude dispersion. A simple computation of amplitude dispersion for shallow water depths showed a reduction of the depth bias by 0.2 m. Some of these error sources can be filtered from the XMFit results. Filtering on preferential metocean conditions and radar range reduces the depth bias of 4.2 m at the Sand Motor by 1.9 m to 2.3m. A depth bias of 0.8 m remains in the Ameland results after applying a comparable filter. This remaining depth bias is induced by a shift of the measured energy in the kw-space. The energy is observed at smaller wavenumbers than theoretically expected. The XMFit fit aligns exactly with the provided energy. The accuracy of XMFit cannot be improved by giving more importance to the longer waves, since the energy is also shifted for these longer waves. During storm conditions energy is also observed on a the location of the first harmonic of the dispersion relation. Although this first harmonic is not observed to influence the depth bias, it does cause a severe onshore directed velocity error. Additionally, this kw-space shows stripes of energy which can be attributed to the bottom slope. None of the researched error sources explains the shift of the energy in the kw-space. Therefore it becomes likely that the error is related to a cause outside of the XMFit Algorithm. To find this cause, the local radar set-up and the preprocessing before the XMFit algorithm should be investigated.","XMFit; accuracy; radar; bathymetry; Sand Motor; Ameland; error sources","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Environmental Fluid Mechanics","",""
"uuid:f85976bf-c67e-4243-9184-98c81f2c9d4a","http://resolver.tudelft.nl/uuid:f85976bf-c67e-4243-9184-98c81f2c9d4a","Improving the purchase, nesting and order release process of a batch shop production process","Sturm, L.C.A.","Beelaerts van Blokland, W.W.A. (mentor)","2015","At the sheet metal department at Fokker Aerostructures a high variety of low demand sheet metal products are produced for use in different aerospace assembly programs. The production process operates in a batch shop style of operations in order to cope with the high-variety and low-volume nature of the product mix. In order to ensure that the products are delivered on time, the release of production orders is planned by an ERP system and production order receive a planned start date. It is the task of the production planner to ensure that the different orders are released as close to this planned start date as possible. This task is however made more difficult because of batched release of production orders. Production orders are released in batches in order to be able to cut multiple order from the same sheet at the milling process, which is the first step in the production process. Orders are released in batches in order to reduce the material losses. This batched release does however reduce the effectiveness of the production process by increasing the variation of start of the production process and thereby increasing the level of work in progress. The objective of this master thesis is determine the effects of using different sheet metal dimensions in the milling process on both the incurred material costs and the effectiveness of the downstream production process. Furthermore this research investigate if different functional process designs are able to improve the effectiveness the purchase and order release process. The scope of the research is limited to the process that takes place before the milling process step. The production process is analyzed using a combination of Value Stream Mapping and the ‘PROPER’ model from the Delft Systems Approach. These analyses are performed, in combination with a quantitative and statistical analysis, in order to uncover the operational constrains and limitations of the current order release process. From these analyses it has become apparent that in the current functional design, sheets are purchased based on an expected material usage as calculated by the ERP system. The expected material usage is an overestimation of the actual material requirements. After receiving the different materials a custom milling pattern is created in the nesting process. In the nesting process the actual material requirements are uncovered. Due to the overestimation of material usage additional production orders are released in nesting process to use up the excess ordered sheet material. Therefore two alternative future states are designed, based on lean manufacturing principles, in order to mitigate the negative effects of this material use overestimation. The different future state process designs are tested using a simulated model in order to analyze their effects the performance of the purchase and order release process. As part of the simulation model the nesting process is simulated using a greedy 2D bin packing algorithm. First the current functional design is modeled in which the influence of alternative sets of sheet dimensions are tested. Moreover in this first model the influence of different purchase and order release process parameters is put forward. In the second future state model, named feedback control, the nesting process is performed just before purchase orders are created. Thereby in this future state it is possible to determine the size of the purchase order based on the actual material usage. In the last future state, Kanban control for purchase orders is introduced in combination with using a two bin inventory of raw sheet material. Again in this model the purchase orders are based on the actual material usage instead of the overestimation of material usage as calculated by the ERP system in the current operation. The material efficiency of the different functional designs is judged based on the total material costs as calculated by the simulation model. The effectiveness of the different future states is determined based on the variation in the OTP of the order release process. Additionally the effectiveness is judge based on the average number of production orders that are released in a batch. From the simulation model it can be concluded that by changing the sheet dimensions of 20 out of a total of 86 material groups, a cost reduction of 12.500 euro can be achieved. Furthermore it can be concluded that the purchase and order release process is improved the most when implementing a Kanban control method for the replenishing of sheet material. Both the material costs as well as the effectiveness of the production process is improved the most when implementing this future state design. Variation in the OTP of the order release process can be reduced by an estimated 16%. Furthermore Kanban control reduces the average number of production order released per batch by 10%. Additionally from the simulation model it has become apparent that in the current production process design at least 6.3% of the production volume is comprised out of ineffective work release caused by the overestimation of material usage. This ineffective work release comprises out of the increase of production order batch sizes and the additional excessive early release of production orders. This ineffective work is released in order to use up the sheets that are ordered in excess of the actual material requirements. Both in feedback and Kanban control this overestimation of material usage is remove which in turn reduces the level of overproduction and increases the effectiveness of the downstream production process. The implementation of this control method is however only partially feasible because of the limitation in available storage space. Feedback control offers a good alternative for those material groups that cannot be controlled via Kanban. This functional process designs offers the same material efficiency with a slightly lesser increase of process effectiveness. This future state model offers similar results for the reduction of overproduction and an equal decrease of average number of orders that are released in a batch. The variation in OTP of this future state model is however reduced by 3% in comparison to the current process design. ?","lean manufacturing; operational research; case study; process effectiveness; modelling; bin packing algorithm","en","master thesis","","","","","","","","2020-08-29","Mechanical, Maritime and Materials Engineering","Marine & Transport Technology","","Transport Engineering and Logistics","",""
"uuid:7d84920d-e0d7-425e-885b-10af7208e9d6","http://resolver.tudelft.nl/uuid:7d84920d-e0d7-425e-885b-10af7208e9d6","Synthesis of a Long Vertically Aligned Carbon Nanotube Array by Chemical Vapour Deposition","Buter, J.","Ghatkesar, M. (mentor)","2015","Although a lot of research has been devoted to the growth of a long Vertically Aligned Carbon Nanotube Array (VANTA), it remains difficult to find the right parameters on new systems. It has turned out that the recipe that allows the growth of Carbon Nanotube (CNT) in a particular Chemical Vapour Deposition (CVD) oven is very specific and not appropriate for other CVD systems. Besides, the condition in and around the oven should be suitable for CNT growth. Therefore a systematic approach is proposed in which this condition can be verified and actual CNT growth can be ensured. With this approach CNT arrays have been grown reproducibly with a length of 500 microns and more. The quality of the CNTs have been investigated by Transmission Electron Microscopy (TEM) imaging. It has been found that the CNTs contain only few defects and no undesired coverage of amorphous carbon. Besides this framework for CNT growth initial steps have been taken to synthesize a spinnable CNT array. Wires up to 5 mm have been pulled from the array and multiple directions have been investigated to improve spinnability towards a continuous spinning process.","CNT; array; forest; carbon; nanotube","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering","","Micro- and Nano Engineering","",""
"uuid:2afbab9a-d30f-4c06-96fc-ab17e96b244c","http://resolver.tudelft.nl/uuid:2afbab9a-d30f-4c06-96fc-ab17e96b244c","Numerical model for the induced failure of the Leendert de Boerspolder embankment","Rodriguez Barragan, O.P.","Vardon, P.J. (mentor); Jommi, C. (mentor); Bakker, K.J. (mentor); De Gast, T. (mentor)","2015","The investigation performed for this MSc graduation thesis has focused on the creation of a numerical model for the induced failure of the trial embankment in location B within the Leendert de Boerspolder. What has been modeled in this MSc graduation thesis project is a 2D cross section of the mentioned embankment. Later on its safety has been analyzed by means of the limit equilibrium ‘LEM’ and finite element ‘FEM’ methods. The main objective of this research has been to build a numerical model capable of capturing the induced failure process as close to reality as possible given the inherent limitations of the data, methods and models used. It has also aimed to estimate at which stage of the process failure may occur and the type of mechanism most likely to develop. Moreover it has focused on comparing the safety factor ‘SF’ values obtained from the created LEM and FEM models. Likewise, the work presented in this document has tested the sensitivity of the FEM model, in specific to permeability. All the necessary input to build the numerical model has been derived from the available site and laboratory data. This document then describes the proposed induced failure process in all its 9 stages. Beginning with the saturation of the dyke which decreases its initial safety i.e. SF= 2.3 by about 14% when using the FEM. Whereas after the last submerged excavation and its respective dewatering stage, the SF values found are equal to or slightly below 1 e.g. SF= 0.96 for the 1m depth 1:1 slope FEM model. Indicating that the dyke will be at the verge of failure but not actually guaranteeing it, since it is possible that failure occurs at the excavation and not the dyke body. The sensitivity analysis for permeability is made at specific locations of the dyke, i.e. the crest and toe, both at surface level. In regard to vertical displacements at the crest and toe surfaces, combination 4 with a low permeability for the dyke material i.e. k= 4,0E-07 m/s, and a higher permeability for the peat i.e. k= 1,2E-05 m/s, gives the largest values. In terms of the obtained displacements at the toe and crest, these can indicate how the induced failure process develops in time. As well as correlate the respective displacements at any given stage to its corresponding SF value to give an idea of pre-failure. The results that have been obtained are useful as aid in the design of optimal slopes and to determine how the safety of the embankments within a polder can be guaranteed. The results may also serve as comparison to site measurements from monitoring during the actual process, to later on evaluate the effectiveness of the FEM model in capturing the soil behavior and the induced failure process.","numerical model; finite element method; trial embankment; induced failure; sensitivity to permeability","en","master thesis","","","","","","","","2016-08-29","Civil Engineering and Geosciences","Geoscience & Engineering","","Geo-engineering","","51.998647, 4.375551"
"uuid:23ce777c-44ac-4707-9bd1-2986152afebd","http://resolver.tudelft.nl/uuid:23ce777c-44ac-4707-9bd1-2986152afebd","Partial Discharge Analysis and Recognition in Cables with Medium Voltage DC","Palmans, J.A.","Rodrigo Mor, A. (mentor)","2015","With the current state of power electronic conversion operating at Direct Current (DC) can in some cases be more efficient than ordinary Alternating Current (AC). DC voltage energy transmission and distribution is, thus, playing an increasingly important role. The continuity of the electrical energy supply is essential for a well-functioning society. At AC voltage problems with cables can in most cases be prematurely detected with Partial Discharge (PD) measurements. Partial discharges represent defects in the insulation system of electrical cables and devices. They can be categorized in three groups: corona, surface and internal discharges. By knowing the source of the partial discharges and quantifying the discharges, the dielectric condition of a cable can be estimated. At AC voltage the various patterns of partial discharges, also known as fingerprints, can already be recognized and the source of the defect can be determined. The trend for electrical power distribution is, thus, to use DC voltage instead of AC voltage. However, one problem that should be tackled is that partial discharges under DC voltage form a challenge in recognizing the source of the discharge. Therefore, it is still difficult to properly estimate the dielectric condition of the cable based on DC voltage partial discharges. To help overcome this challenge this master thesis research will try to find a relation between measured medium voltage DC partial discharge patterns and the actual source of the discharges. The influence of noise and data segregation of the measurements will be investigated. A method to separate multiple types of partial discharge from a single measurement will also be discussed. Furthermore, limits will be determined in how long a measurement needs to take in order to reliably detect the source of a certain defect. Four artificial PD defects were used to acquire specific data on the individual types of discharges at DC voltage. PD analysis has been done with a self-written Matlab program. By creating classification figures trends have been found that help recognizing the type of defect in a DC PD measurement. The influence of noise and data segregation on DC PD measurements has also been evaluated. Noise can significantly influence the classification figure. Therefore, noise should be prevented or filtered out prior to the classification process. A single noise waveform can already cause a large standard deviation in the classification figures. However, the calculated average values keep displaying the same trend. Thus, a single noise waveform does not hinder the recognition of the PD source. If due to high background noise lower amplitude PDs are lost, data segregation occurs. The physical relationships of time and charge between succeeding discharges will be distorted. To achieve a successful classification a minimum amount of PD waveforms is necessary, which has been investigated. For positive corona at least 250 PDs should be acquired to get the same trends in the classification figure as with 1000 PD waveforms. To be certain of a negative corona discharge, at least 300 PD-pulses should be recorded, instead of the reference measurement, which contained 903. The reference measurement with the internal discharge sample comprises of 578 waveforms. For proper classification a minimum of 300 discharges should be obtained. In order to be certain of surface discharges, 450 out of 999 waveforms are sufficient to create the same trends in the classification figure. To separate and classify individual discharge types from a multiple PD source measurement, cluster techniques are used. The cluster techniques used in this thesis are based on the (Energy versus Charge) and (Equivalent time versus Equivalent bandwidth) of a waveform. It has proven to be possible to classify and identify individual discharge sources. A classification of a certain cluster showed the same trends as in a reference surface discharge measurement. For negative and positive corona clusters 3 out of 4 classification figures comply with the reference measurements.","Partial Discharge; High Voltage; Medium Voltage; Direct Current","en","master thesis","","","","","","","","2018-09-29","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","Electrical Sustainable Energy track","",""
"uuid:0dd55171-6768-4e46-b6cd-970eb912e2ac","http://resolver.tudelft.nl/uuid:0dd55171-6768-4e46-b6cd-970eb912e2ac","Aerodynamic Design Optimization of the MTT Radial Micro Turbine","Govindarajan, S.","Colonna, P. (mentor); Pini, M. (mentor); Visser, W.P.J. (mentor)","2015","Micro turbines are touted to become the prime system for the combined heat and power(CHP) applications in light of their significant advantages in terms of performance, size, costs and reduced CO2 emissions [1]. Micro Turbine Technology B.V. (MTT) is currently developing a 3KW recuperated micro turbine for such applications. Commercially available off the shelf turbocharger components are used since they provide high performance with relatively low costs since they are mass produced.The drawback in using these components is that they are manufactured for the automotive sector and inherently operate at different conditions than the MTT operating point. Here in lies an interesting scope for performance improvement by optimizing the turbine and within the current work the focus is on aerodynamic optimization of the radial inflow turbine used in the MTT system. This study is a follow-up from the recommendations provided in [2] and [3]. A goal driven optimization is performed on the rotor geometry using ANSYS DesignXplorer and a total of four design solutions were obtained. The most important findings from the response surfaces, sensitivity analysis from the optimization were:  From the parametric sensitivity it was clear that all of the six design variables have a significant impact on efficiency. The exducer angles have the most predominant effect on efficiency with that of shroud larger than hub.  All of the optimal candidates exhibited an increase in the total-to-total efficiency ranging from a minimum of 6.38 percentage points to a maximum of 7.90 percentage points as compared to the baseline geometry. This efficiency improvement was accompanied by an increase in mass flow rate with a minimum value of 69.15 g/s and a maximum of 73.51 g/s. These design solutions are then coupled with the diffuser domain to study the performance characteristics and the interaction between the components. The most important outcomes from these simulations were: The efficiency of the rotor drops by 3 percentage points on an average due to the additional pressure losses introduced when coupled with the diffuser.  The diffuser performance has improved and the Cp experiences a maximum increase of 17.63 percentage points (Candidate D) and a minimum of 12.70 percentage points (Candidate B). The swirl coefficient for optimum diffuser performance is found at values close to 0.22. If the swirl coefficient is increased or decreased from this optimum, diffuser performance drops. The best design solution in terms of rotor efficiency and overall total-to-static efficiency is Candidate C. However it exhibits a poorer diffuser performance than the other optimal candidates. From this study, it is apparent that there is compromise between rotor and diffuser performance.  The improvement in rotor efficiency(<tt,5ax) ranges from a minimum of 6.16 (Candidate A) percentage points to a maximum of 8.54 percentage points (Candidate C) as compared to the baseline which is more or less similar to the case with the individual rotor domain simulations. The improvement in total-to-static efficiency (<ts,9) achieves only a maximum of 3.23 percentage points(Candidate C) as compared to the baseline. In other words ,whatever is gained in rotor total-to-total efficiency from the design optimization, less than half of it is utilized when coupled with the diffuser(Cp<50%). The best candidate solution selected from the simulations mentioned above is then analyzed from a structural point of view by comparing its equivalent von-Mises stresses with that of the baseline design. The most important conclusions from this study were:  The optimized rotor experiences higher at stresses at the tip station than the baseline. At the root section both geometries exhibit higher stresses than most of the other stations. At the trailing edge the optimized rotor exhibits lower stresses than the baseline geometry. A maximum stress of 1599.7MPa for the baseline occur at the trailing edge root section and the optimized rotor experiences a lower maximum of 1439.3MPa and it occurs closer to the shaft.  In order to reduce the tip stresses for the optimized rotor and bring them closer to the baseline values, the speed of rotation must be reduced to the neighborhood of 180 000 rpm. One of the important outcomes of this study is that there is a compromise between diffuser and rotor performance, consequently a coupled optimization is recommended with the diffuser domain in order to get a better understanding of the interaction effects between the components. The optimization procedure in this work is performed with only the rotor domain due to computational restrictions and the design solution does not take into account the diffuser performance during the optimization process. A scaling study with the volute is recommended for the optimized rotor geometry in order to restrict the mass flow rate. The design solution exhibits an increase in passage area and consequently a rise in mass flow rate. However, in the real application a change in mass flow rate might cause a mismatch with other components of the MTT power unit. The structural analysis is done post optimization and is carried out without any constraints for maintaining the stress within acceptable levels. Therefore a multi-disciplinary optimization combining the solid and fluid analysis is recommended to prevent the stress increase and maintain the creep life of the component.","microturbine; optimization; CFD analysis","en","master thesis","","","","","","","","2016-09-29","Aerospace Engineering","Flight Performance and Propulsion","","","",""
"uuid:f8b6c47c-b08e-46d7-9358-488602c50973","http://resolver.tudelft.nl/uuid:f8b6c47c-b08e-46d7-9358-488602c50973","Numerical Study of the Flow and Heat Transfer in Supercritical Water Based Fluidized Bed Reactor","Zeng, C.","De Jong, W. (mentor); Harinck, J. (mentor)","2015","Public version. This version is only limited to the summary. Supercritical water gasification (SCWG) is a novel process for the thermochemical conversion of wet organic waste to gas and minerals. It is an alternative to the anaerobic digestion process, dewatering followed by drying and incineration and conventional dry gasification. Its advantages include no requirement for drying, higher syngas yield and much shorter residence time. From societal aspect, SCWG offers a solution to environmental problems caused by wet waste and fossil fuels, through the production of renewable gas and minerals. TU Delft and Gensos B.V. collaborate in fundamental research on supercritical gasification using a fluidized bed reactor concept. This study focuses on the hydrodynamics and heat transfer of the fluidized bed reactor of the supercritical gasification process. It aims to develop CFD models that can correctly predict the main heat transfer and fluidization phenomena in the supercritical fluidized bed reactor. CFD models are developed to conduct numerical studies of single-phase heat transfer without fluidization, multi-phase fluidization without heat transfer and fluidization with heat transfer. The CFD models are validated by using experimental data from Yamagata, Mokry and Lu et al.","supercritical water gasification; CFD; heat transfer; fluidization","en","master thesis","","","","","","","","2020-09-01","Mechanical, Maritime and Materials Engineering","Sustainable Process and Energy Technology","","Energy Technology Section","",""
"uuid:480c27e9-784b-40db-aea1-a0cba5e1ee71","http://resolver.tudelft.nl/uuid:480c27e9-784b-40db-aea1-a0cba5e1ee71","Seismic Assessment of a Typical Dutch Rijtjeshuis","Michalaki, M.","Rots, J.G. (mentor); Hendriks, M.A.N. (mentor); Ravenshorst, G.J.P. (mentor); Steenbergen, R. (mentor)","2015","A critical issue that is raising concern among the scientists and engineers during the last decade in the Netherlands is the occurrence of relatively small earthquakes in the North part of the Netherlands (Groningen, Drenthe and North Holland). These earthquakes that have a non-tectonic origin and are related to the gas-field depletion, have caused feelings of anxiety among the residents of the area. Thus, investigation of the seismic vulnerability of the structures in the affected area is crucial and the necessity of it is increasing when taking into account the dominating type of buildings: unreinforced masonry structures. The present Master Thesis Project is focusing on the seismic assessment of the Dutch “Rijtjeshuis”. A seismic analysis of a series of unreinforced masonry houses in Groningen is executed, which indicates the response of the structure to seismic actions and identifies the near collapse state for a range of seismic scenarios. The assessment of the performance of existing buildings is usually done by either fully dynamical procedures (nonlinear time history analyses) or static pushover methods. Structural response is defined in the present study by non-linear static pushover analyses, using finite element software DIANA (Version 9.6). The capacity spectrum method, using the inelastic demand spectra, is followed after considering variability both in the material properties (thus capacity curves) and the demand spectra, with ultimate purpose the derivation of the fragility curves for this typology of structures. The fragility curves are necessary to allow for a reliability based judgment of the structure. The distribution of the seismic resistance is built up from several parameters the most important of which are the ground motion variability, within building variability, and building to building variability. Masonry as a material is characterized by high rigidity, low tensile and shear strength, low ductility and low capacity of bearing reverse loading. These are the main reasons for the frequent collapse of masonry buildings during earthquakes often responsible for a considerable number of casualties. In the majority of real cases building properties are not well known due to the variability in building materials and building techniques. The masonry structure examined in the present study is supposed to be representative of a class of buildings with similar structural characteristics, mechanical parameters have been considered as random variables. The material variability can be regarded as the most important source of uncertainty in the determination of structural response, with all other sources either deriving directly from its effects or being insignificant in size compared to it. After the variability in the material properties is taken into account and the pushover analyses have been executed, the behavior of this typology of structures is known with the pushover and capacity curves. The reliability of the model used for the pushover analyses, is based on the verification of the finite element model with an existing one made by TNO in the framework of the project “NPR 9998 - Rekenvoorbeeld betonconstructie (TNO pushover analyses in DIANA)”. The first model that is developed in the Finite Element Program DIANA FX+ (version 9.6) is made of reinforced concrete as the one made by TNO. After the model is certified, the material properties of the concrete wall are going to be replaced with the mean material properties of masonry (Calcium-silicate brickwork, typical approximately 1960-1985). All the material properties of the structure (referring to masonry only) are assumed to be random variables, to which normal probability distributions are associated based on realistic ranges of variation. The sensitivity study is done based on the mean values and the coefficient of variation of a dominant random variable which is chosen to be the tensile strength of masonry. The pushover curve of each analysis will be extracted with ultimate goal the derivation of the family of pushover curves for the different material properties and further the capacity displacement for each case. Following, the demand from different earthquake scenarios are examined: after a probabilistic seismic hazard assessment, the necessary knowledge of the likely earthquake actions at a subject site have been determined. After using Jayaram’s method for the derivation of the acceleration response spectra and accelerations- displacement spectra, the demand of each earthquake can be computed. The inelastic displacement demand, that needs to be calculated because of the energy dissipation observed during the performance of the structures under seismic loads, is derived using Lin and Miranda’s method, which is an accurate and non-iterative procedure. Finally, since both the capacity of the structure and the inelastic demand of each earthquake is known, the comparison between these values will reveal information about the performance of the structure for the examined limit state. Since the variability in both the material properties and in the ground motions have been considered, the probability of collapse for each earthquake can be calculated. The repetition of this procedure for all possible earthquakes and structures results in the fragility curve. As a conclusion, the fragility curve that came out has the expected behavior and is comparable to ones found in literature for unreinforced masonry structures. The assumptions made in the present study are discussed further and recommendations for future studies are done.","masonry; fragility curves; capacity spectrum method; ground motion variability; material properties variability; Groningen","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","","",""
"uuid:71046301-1f96-408f-961f-e6c2aa881e8d","http://resolver.tudelft.nl/uuid:71046301-1f96-408f-961f-e6c2aa881e8d","An Explorative Study on Factors outside the Influence of the Entrepreneur that can explain the Commercialization Gap for Cleantech Innovation in Israel","Schaap, T.A.","Scholten, V.E. (mentor); Cuppen, E.H.W.J. (mentor); Van Beers, C.P. (mentor)","2015","Background The research is executed as a master thesis for the MSc program Management of Technology at the TU Delft and is conducted in collaboration with the Embassy of the Kingdom of the Netherlands to the state of Israel in Tel Aviv. The researcher has spent six months in Israel to perform this research and was subsidized by Climate-KIC to execute this research. Problem statement and research question This research is an empirical exploration of the influence of external factors on the commercialization process for cleantech Technology Based New Ventures (TBNVs) in Israel after these ventures have received seed funding. External factors are defined as factors outside the influence of the entrepreneur. Literature has described the progression of TBNVs in different stage-based models, although these mainly describe the organizational development. This thesis uses models of Kazanjian et al (1989) and Vohora et al (2004) to describe the growth process of cleantech TBNVs and zooms in on the processes which cleantech TBNVs have to execute after they passed the credibility threshold (Vohora et al, 2004). This milestone reflects in the research by only considering cleantech TBNVs which have received seed funding and where thus deemed credible enough by their investors. Previous research has named Israel the most innovative country in cleantech, but showed that there is a lack of commercialization of this innovation. The purpose of this research is to explore explanations for this phenomenon and test whether the factors distilled from the literature study can be found in practice and explain the phenomenon. Ten factors were determined based upon a literature study and these were tested by conducting field interviews and studying research reports. The overall research question for this study is: Which factors, outside the influence of cleantech TBNVs, have consequences for the progression of cleantech TBNVs to the sustainable returns phase after seed funding has been received? Research Process Three angles were chosen in the literature study to determine external factors – markets, resources and policy. These factors served in general as a good framework for the practical exploration of the influence of external factors on the commercialization process of cleantech technology based ventures in Israel. The studied factors are accessibility of international markets, the need for high-paced growth, the need for an international network, availability of financial and human resources, risk tolerance of available financial resources, competition for financial resources with other fields of technology, the formal institutional regime for new innovations, the formal institutional regime for new sustainable innovations and perceived stability of the governmental policy by investors. Empirical research was done in the form of two rounds of data collection. The first data collection contained semi-structured interviews with ten respondents who were (in)directly involved with cleantech in Israel. These respondents were from four different areas – business development, government (policy), late stage finance and venture capitalists and were interviewed about the aforementioned factors. The results from these interviews prompted a second data collection in two specific topics that were thought to hold more explaining value about the observed commercialization gap. These two topics included the availability of financial resources and related factors, the policy for innovation in Israel in general and the policy surrounding cleantech innovation. The second data collection contained another four semi-structured interviews on these specific topics and the study of reports on the topics. Findings and conclusions The results of the empirical research showed that all the proposed factors were relevant and influenced cleantech TBNVs in Israel, although the influence of some factors is more explicit than that of others. Especially the availability of financial resources which can be used to invest in technology development of cleantech TBNVs were found to be lacking. This can be explained by the high financial costs of technology development for cleantech TBNVs. The investment in such a project bears a lot of risk, which only a few types of investors can cope with – namely specialized, early-stage Venture Capitalists, business angels and the government. Moreover, many cleantech TBNVs develop technologies related to the field of infrastructure which is a tough market for a start-up. Finally, the shift in policy relevant for cleantech TBNVs can be expected to offset investors, which also contributes to the lower amount of available financial resources. Implications Scientifically, this study contributes evidence to the validity of the applied theories in a specific setting – namely development of cleantech TBNVs in Israel. The conceptual model used in this study would be useful to explore similar research problems in other countries although a zoom into specific topics remains necessary. In this research the specific topics included policy relevant for cleantech TBNVs and the needs for funding for cleantech TBNVs. Practically, this research has implications for entrepreneurs and investors in this field and for governments both in Israel and Europe. Entrepreneurs and investors in this field should realize themselves that they are in a precarious position due to factors like the high costs of technology development and instable policy that heighten the already high amounts of uncertainty that is currently surrounding the process of cleantech TBNV development. Risk reduction strategies should be high on the priority list of these actors. Governments should realize that investors make investments with a five to ten year horizon and regulatory stability is therefore an important factor to take into account if one aims to increase in the sector. Especially the case which described the instability of the solar sector in Israel is an example of an increase in investment insecurity by governmental decisions. Moreover, the financial resources necessary for most of the cleantech start-ups are momentarily simply not available. The Venture Capital investment model is only suitable for those start-ups that can achieve high growth rates, which can be difficult for cleantech start-ups. Making different financial resources available tailored to the needs of cleantech TBNVs, for instance via debt financing instead of equity financing should be a priority for the governments both in Israel and Europe. Previous research of EIM showed challenges in Europe to be similar to the challenges that have been found in this research.","cleantech; start-ups; TBNVs; Israel; External factors; policy; markets; resources; financing; growth","en","master thesis","","","","","","","","","Technology, Policy and Management","Economics of Technology and Innovation","","Management of Technology","",""
"uuid:becb108b-1aec-4c08-852a-79ac83a0de4f","http://resolver.tudelft.nl/uuid:becb108b-1aec-4c08-852a-79ac83a0de4f","Does a V-shape improve performance of a multi-purpose construction vessel? A comparison study of a V-shape versus a conventional hullform","Oosterlaak, W.A.","Hopman, J.J. (mentor)","2015","The report presents a comparison study of a multi-purpose construction with a V-shape hullform versus one with a conventional (U-shape) hullform. This research considers multi-purpose construction vessels which have the capability to perform heavy lift and pipe lay operations. These different functions typically result in contradicting design requirements. In general heavy lift crane operations require a large breadth to provide sufficient stability. However, during other operations (such as pipe lay or light lifts) the high stability results in relatively short natural heave and roll periods with high accelerations. In fact, this kind of operations would benefit from a reduced breadth. In an attempt to improve the operational performance of these type of vessels, heavy lifting equipment specialist Huisman Equipment BV has proposed a new hullform. In this research the properties and operational performance of the multi-purpose construction vessel with a V-shape hull and one with a U-shape hull are compared. In order to perform this comparison, two vessel concepts have been designed and studied for the same operational requirements.","","nl","master thesis","","","","","","","","2020-08-28","Mechanical, Maritime and Materials Engineering","Marine & Transport Technology","","Floating structuurs","",""
"uuid:1b4ead72-22ea-4f2e-9eac-303a122d77c5","http://resolver.tudelft.nl/uuid:1b4ead72-22ea-4f2e-9eac-303a122d77c5","Customization of elevetad printing for interior designers","Kitajima Hitomi, H.","Geraedts, J. (mentor); Creusen, M. (mentor); Van den Hende, E. (mentor); Harkema, C. (mentor)","2015","This report is the result of my graduation project for Master of Design For Interaction at the faculty of Industrial Design Engineering at Delft University of Technology(TU Delft). The goal of this project is to design a Product Service System for Océ, to connect their newly invented elevated printing technology to the interior decoration field. This is achieved using the user-centered design approach. The process consists of three major phases; setting the scene (literature research and interview with people), conceptualization and validation. ‘Setting the scene’ phase starts with an overview of the current business portfolio of Océ and technical aspects of elevated printing technology. Then it describes how customization is the trendy topic for the interior decoration field by showing market growth. Thereafter, this phase also shows the analysis of stakeholder interviews, mainly focused on interior designers’ workflow and their idea generation process.These insights are helpful to identify what is currently lacking in order to promote the use of elevated printing more. Next, the conceptualization phase illustrates idea sketching and embodiment which is based on all the information I collected in the first phase. Through some brainstorming sessions with colleagues and friends, I made one concept for a product service system which enables interior designers to use as a starter kit for interior decoration customization. It is called, ‘Eiger on the desk’, and it includes four interactions with the following flow; 1)‘ask for print samples through online-library’, 2)‘receive a box with samples and basic information’, 3)‘install customization app with tablet’ 4) order new design via app. At the end of this phase, prototype design was made to test the concept with real interior designers. At final ‘Validation’ phase, evaluation tests with professional interior designers were conducted to get feedback for my prototype. Based on their feedback, I concluded this project with recommendations for further development direction to Océ and possible improvements for product idea as a graduation project.","customization; interior design; product service system","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design Engineering","","Master of Science Design for Interaction","",""
"uuid:f8358a5e-de63-4852-ae96-bdd90f43789d","http://resolver.tudelft.nl/uuid:f8358a5e-de63-4852-ae96-bdd90f43789d","Web shop packaging strategy for a FairChain coffee company","Peters, M.N.","Creusen, M.E.H. (mentor); Dehli, S.R. (mentor)","2015","New packaging is designed for a FairChain coffee start-up company. The project result contains insights in the company and the context, an overview of the web shop customer wants (after qualitative and quantitative research) and the final deliverable is a packaging design for a delivery box and the coffee bags inside.","coffee; packaging; Fairtrade; FairChain; branding","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","","",""
"uuid:103b47e1-4d6a-4eff-ba79-087af6668dfa","http://resolver.tudelft.nl/uuid:103b47e1-4d6a-4eff-ba79-087af6668dfa","Decision support framework for a sustainable waste management system in Ahmedabad, India","Bharambe, S.","Stikkelman, R. (mentor)","2015","Rapid Urbanization has been the recent trends observed across all the countries of the globe. With the rising levels of global population, the global consumption and hence the rate of waste generation has also simultaneously increased. Hence, one of the key roles of the local municipalities is to decide, develop and maintain a waste management system that can deal with these rising levels of waste. In addition, these local bodies also have to tackle the problems of insufficient budgetary funds, lack of human resources, lack of technical expertise and presence of a large unaccounted informal sector. Hence, this research focuses on the decision making aspects of these local bodies. One of the primary motives of such decision making processes is to arrive at a collaborated or negotiated solution for problem at hand. The pursuit of such a motive, on one hand requires development of structured and compartmentalized approach to arrive at the final aim, but on the other also pin-point the systemic nuisances in the system that prove to be barriers against successful implementation. Hence, decision making in waste management presents a case of a Multi-criteria decision making. Although, multi-criteria decision making has been previously researched in detail, there is a lack of detailed research of a holistic decision support frameworks that can be applied to municipal corporations in developing countries. City of Ahmedabad, in state of Gujarat, India, has been chosen as a case study location. Ahmedabad is a thriving city of 5.5 million residents facing rising levels of solid waste. This study intends to improve the existing conditiosn of the waste management system in the city, by highlighting the systemic nuisances and provide practical solutions while improving on the existing base knowledge in the field of decision making in waste management. The outcome of the research is a set of strategic and tactical suggestions and guidelines for implementations and system changes needed for these strategies. Also, on the scientific side, a holistic decision support framework is provided with adequate guidelines and directions on further usage and implementaion across other cities. In an attempt to address this gap in the academic literature, this masters thesis project aims to investigate the formulation of a decision support framework based on three major aspects of the waste management system: 1) analysis of the system flows (material, cash and information), 2) structured decision making approach and 3) consensus analysis. The thesis project has been divided into three main aspects: The analysis of the systemic flows associated with the waste management system, a structured decision making approach and finally analyzing the levels of consensus among the stakeholders for success during the implementation phase. The proposed decision support framework intends to facilitate the transition of ad hoc practices of decision making processes towards a new holistic accountable decision making processes. The perspective adopted during the complete analysis is that of the Municipal corporation for a time period of the next 3-5 years i.e. “if I was the chieft decision maker in the municipal corporation and I wanted to find system changes that could be implemented within coming 3-5 years, what are the strategies” This research has been conducted with support and aid from Witteveen en Bos (a leader in Engineering and Consulting) and Rijkswaterstaat (The Dutch Ministry of Environment and Infrastructure). The collection of the data for this research has been done by means of semi-structured interview with 12 stakeholders of the city that are directly associated with the waste management system. A review of the reports and organizational documents has also been done that were made available by stakeholders during field visit. The interviews conducted by my colleague Kedar Jani (the second Masters student working on this project) as a part of the data collection effort for his thesis “Sustainable Solid Waste Management, Ahmedabad” also have been used as an additional source of data. The structured decision making part of the decision support framework was facilitated by a stakeholder workshop with 5 attendees from various sections of the waste management system. No scientific methods have been employed for analyzing the data. But there were some important findings from the data gathered that needed immediate addressal and formed a set of primary issues regarding waste management in the city. These issues were as shown in the table below. The selection of these problem definitions were based on the collaborative efforts with Mr. Prashant Pandya, who helped in understanding which particluar problems needed immediate addressel. Also, based on the system analyis, strategies were proposed to target these primary issues. These strategies were result of the focus group discussion done with the stakeholders. Problem Definition Proposed Solution Actors needed Problem-1: Opportunistic Behavior by transport contractors Altering the system cash flows (Section 3.4.1) AMC, Transport Contractors, Treatment Plants Problem-2: Contamination of waste at secondary collection points Bin-less waste collection system (Section 3.4.2) AMC, Transport Contractors, General Public Problem-3: Poor coverage (44%) of H&K waste collection system Polluters pay principle (Section 3.4.3) AMC, Private H&K Contractors, Hotels and Kitchens, Treatment plants Problem-4: Need for source segregation and no littering in public places IEC (Information, education and communication)/BCC (Behavior change campaign) (Section 3.4.4) AMC, General Public, NGOs and CBOs, Media Problem-5: Unreliable data on daily waste collection GPS system installation in the collection fleet and inspection at weigh bridge (Section 3.4.5) AMC, GPS suppliers, IT Support Problem-6: No data about recyclables recovered from the Pirana Dump Yard Re-mining of dump yard using informal sector (Section 3.4.6) AMC, informal sector, NGOs, Recyclers Problem-7: No data about recyclables recovered by itinerant waste buyers Formalizing itinerant waste buyers for source segregation (Section 3.4.7) AMC, Itinerant waste buyers, General public, Recyclers Additionally, there were three main target areas that were of main concerns for the targeted improvement in the waste management system. These were:  Collection of waste in unsegregated form The waste collected on a daily basis was in mixed form i.e. the wet and dry fractions of waste were stored, collected and transported together. Presently, only 13% of the waste in the city was source segregated. Also, segregating a mix waste stream at a later stage is a costly affair and infeasible due to the higher presence of moisture content (40-50%) of the wet waste. In addition, the mixed streams cannot be directly treated without segregating at the present installed treatment facilities. Hence, almost all of the waste is directly dumped in the open dumping ground.  Insufficient treatment capacity It was found that presently the city was capable to treat only 550 metric tons (13.75%) of waste out of the 4000 metric tons of daily waste generation. Rest of the waste was diverted to the pirana open dumping yard. Also it was found that these plants were operating at 30% efficiency due to the mixed nature of incoming waste.  Absence of markets for end-products It was found during the study that the market sale of the end-products (compost and pellets) was reported to be lower. The major reason found was lack of customer awareness about product knowledge for e.g. the compost is repeatedly being compared by farmers with chemical based fertilizers. Compost is a soil conditioner while fertilizers are sources of nutrients (NPK-Nitrogen, Phosphorus and Potassium) and both are equally important. Also some of the recyclables like thin film plastics were not picked up by the informal sector because the efforts made in collecting were not sufficiently justified by the monetary returns through its sale. The main goal for the research was identified as ‘increasing the landfill diversion rates in the city’. The above three main concerns were showed to be directly linked with the landfill diversion rates. As shown in the image above, a problem hierarchy was developed to address these three main concerns to achieve the goal. It was concluced based on the stakeholder workshop that creating a market for compost in the city was a viable strategy to move ahead for the intended goal of increasing the landfill diversion rate in the city. Based on this strategy, consensus analysis was developed to identify systemic drivers and barriers and critical success factors necessary for successful strategy implementation. Recommendations are made for future research along the similar lines of proposed decision support framework.","waste management; ISWM; MCDM; AHP; decision making","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering Systems and Services","","Energy and Industry","","52.005805, 4.370427"
"uuid:66638b58-22ee-4e1e-a4c2-757a8700cda6","http://resolver.tudelft.nl/uuid:66638b58-22ee-4e1e-a4c2-757a8700cda6","Waste Glass as Partial Binder Precursor and Fine Aggregate Replacement in Alkali Activated Slag/Fly ash System","Zhang, S.","Ye, G. (mentor); Keulen, A. (mentor); Arbi, K. (mentor); van Breugel, K. (mentor)","2015","Fast increasing of generation and release of undesirable pollutants by the industrialization raise concerns about the environmental consequences of waste disposal. Among these waste materials, non-recycled waste glass constitutes one of the majority wastes produced in many European countries and sets a major environmental issue. Although glass could be recycled into the packaging stream without significantly changing its chemical and physical properties, there is still a significant proportion, which does not meet the strict criteria for packaging glass, to be sent to landfill. Waste glass is not a biodegradable material, which made landfilling waste glass become a highly unsustainable option. To find a better solution to reduce the environmental issues caused by disposal of waste glass, alkaline activation technology is introduced to use waste glass into alkali activated materials (AAMs) to produce construction materials. Waste glass is believed to have the potential to serve as precursor material in geopolymer production because of abundant amount of amorphous silica in a structure that does not reflect long range order. However, limited research has been conducted on waste glass as precursor material in AAMs, neither is the utilization of waste glass as replacement for binder precursor like fly ash. Currently, the feasibility of using waste glass in geopolymer systems is still largely unknown. This research project aims at a better understanding of the feasibility of using waste glass powder as partial binder precursor to create AAMs along with fly ash and slag and also the feasibility of using fine waste glass aggregate as partial aggregate replacement in geopolymer mortar. The main focus of this research is on the reactivity of waste glass powder and its influence on the microstructure development and gel formation, with correlation with mechanical strength. It has been found that: Waste glass powder, which is dominantly in irregular polygonal shape, has much finer particles size distribution and larger specific surface area compared with fly ash and slag. It has abundant amorphous Si and Ca, and has higher reactivity than fly ash. Waste glass powder serves as effective precursor replacement for fly ash in alkali activated slag/fly ash system. Fine waste glass aggregate is possible to be utilized to replace sand in alkali activated mortars when suitable workability is taken care off. This study provides a better insight on properties of waste glass powder as well as promotes understanding of the influence of waste glass powder on microstructure and gel formation, which is essential for future research and engineering application of this kind of waste material.","Waste glass; Alkaline Activation; Reactivity; Microstructure; Gel formation","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Materials Science & Engineering","","Materials Science & Engineering: Advanced Construction Materials","",""
"uuid:e977cc50-39d7-4ab7-a8c5-c7501dfdc03c","http://resolver.tudelft.nl/uuid:e977cc50-39d7-4ab7-a8c5-c7501dfdc03c","Increasing the Reliability of Settlement Predictions: A Bayesian Approach","Van der Meijs, R.C.J.","Kok, M. (mentor); Van der Wiel, W. (mentor); Schweckendiek, T. (mentor); Zwanenburg, C. (mentor)","2015","","Reliability Updating; Bayes; Settlement prediction; Equality information; Probabilistic Design","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Hydraulic Engineering","",""
"uuid:c2831f45-8cc5-4e14-8065-c98d54665d7e","http://resolver.tudelft.nl/uuid:c2831f45-8cc5-4e14-8065-c98d54665d7e","Modeling and performance analysis of a high bandwidth, low power ring interconnect","Kukreja, R.S.","Bertels, K. (mentor)","2015","As technology is improving and the performance of a single core has reached its peak performance, Multicore Systems on Chip have emerged as the trend of System on Chip designs to meet the performance requirements of high throughput embedded applications. The communication infrastructure (interconnect) of such systems are as vital as its various other computational and storage units. A good design of the interconnect plays a significant role in improving the performance of the system. Bandwidth, area and power requirements of the system make the interconnect design a challenging task. At Intel, heterogeneous Multicore System on Chips are designed for imaging applications. The current system bus based interconnect used in these systems do not meet the performance, area and power requirements of future generation chips. Furthermore, it suffers from being fully connected. For this reason, the interconnect design is migrating to a ring based Network on Chip interconnect. This thesis implements a flexible framework to test and validate the ring interconnect (RI). Using this framework, one can analyze the response of the ring infrastructure for different topologies, reservation mechanisms and traffic scenarios and then configure the RI for a real world traffic scenario. We propose distinct RI configurations to meet the requirement of such a scenario. Furthermore, this framework will allow Intel to verify if the infrastructure fulfills the required performance of imaging applications in the pre-silicon stage.","Interconnect; Network on Chip; Ring","en","master thesis","","","","","","","","2020-09-23","Electrical Engineering, Mathematics and Computer Science","Microelectronics & Computer Engineering","","Embedded Systems","",""
"uuid:3f319e5c-437f-4b17-88fc-bdf8bc21e838","http://resolver.tudelft.nl/uuid:3f319e5c-437f-4b17-88fc-bdf8bc21e838","Design of a modular fuselage for commercial aircraft: To cope with seasonal variation in passenger demand","Van Keymeulen, Q.P.D.","Voskuijl, M. (mentor); De Breuker, R. (mentor)","2015","The subject of this research is a new concept of modular aircraft designed to cope with the seasonal variation in passenger demand by opening the fuselage and increasing its length with extra bits of fuselages. The goal is to find out if this new aircraft concept is more profitable than the current alternatives. Previous work have looked at increasing the size of existing aircraft only once in their lifetime. Or offered opening mechanisms for jet fighters or studied modularity for products or Unmanned Aerial Vehicles (UAV). The idea is to design a new aircraft from scratch able to change its size twice a year in order to improve the offer to the passenger demand. The research was performed in four phases by making two tools. The first phase is making the first tool which allows the design of a family of aircraft according to an input network and passenger demand. This is going to serve as basis of comparison for the modular aircraft. The second phase is making the second tool, based on The Initiator, able to design a modular aircraft. The third phase is performing a structural analysis to compute the mass penalty caused by the connection mechanism. The final phase is studying the profitability of the modular aircraft compared to the optimal family form the first phase. This economical study is performed at two levels: the aircraft level and the airline level. The best concept is starting with the long version. Then the short version uses the same wing and tail but smaller engines and landing gear. When using a safety factor of 8 for the connection mechanism, the mass penalty is relatively small ranging from 1 to 6% of the fuselage mass depending on the aircraft configuration. The principal factor driving the performance of the modular aircraft is not the mass penalty but the non-optimal wing used for the short version. To generate the same profitability as the optimal design, the modular aircraft should reach a load factor of 85.6% instead of 80%. Even in a network, the increased fit between the offer and demand cannot outweigh the design penalty. As a result, the potential for a modular aircraft seems low when compared to the alternatives able to increase the aircraft utilization such as real-time-health monitoring of aircraft to improve the maintenance and price-setting algorithms able to improve both the load factor and the profitability.","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy, Flight Performance and Propulsion (AWEP)","","Flight Performance & Propulsion","",""
"uuid:80aad407-9ea4-4173-8611-fe7c4c851bee","http://resolver.tudelft.nl/uuid:80aad407-9ea4-4173-8611-fe7c4c851bee","Transport of Ultra-Thin Chips Using a Micro-Conveyor","Stam, I.D.","Tichem, M. (mentor)","2015","In the thesis research discussed in this document, a concept that enables the conveying of ultra-thin chips without the presence of mechanical contact is discussed. The design is based on passive air cushion levitation and magnetic self-assembly. By means of angled nozzles, the ultra-thin chips are levitated and translated. Thereby, electromagnets are used to stop and hold the chips at certain desired locations. A nozzle pitch gradient is used to guide the ultra-thin chips. In addition to the design of the concept, the research focuses on the main physics corresponding to the concept to determine the theoretical system behaviour. By means of modelling steps the main physics were determined and by means of experiments the model results were evaluated. The measurement results show that the bending of the nozzle plate is relatively small in comparison with the chip levitation heights from the vertical airflow force test. In addition, the results show that the measurement set-up is able to lift the ultra-thin chips not only in the case when the electromagnet of the system is turned off, but also when it is turned on. The measurement results corresponding to the vertical airflow force test do, however, differ about 40% from the model results. Nevertheless, the shape of the trend-line corresponding to the measurement results from the vertical airflow force test with the pole piece tip located near the nozzle plate seems to correspond well with the shape of the trend-line corresponding to the matching model results. In addition, that is also the case for the trend-lines corresponding to the vertical magnetic force test results and three of the four model estimations sets. Finally, the chip rotation test results show that the rotation of the chip that is caused by the magnetic forces from the electromagnet is significant and shouldn't be neglected.","transport; ultra-thin; chips; micro; conveyor; airflow; self-assembly; flexible electronics","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering","","Micro and Nano Engineering","",""
"uuid:539a2316-0c7e-4e44-b9cb-794f3a606c0e","http://resolver.tudelft.nl/uuid:539a2316-0c7e-4e44-b9cb-794f3a606c0e","Shock attenuation of running shoes","Moltmaker, M.","Jansen, A.J. (mentor); Jellema, A.H. (mentor)","2015","During the running activity impact with the ground causes the muscles of the runner to vibrate. Especially vibrations in the lower frequencies of around 10 Hz can be uncomfortable. Footwear can help to decrease muscle vibrations. Softer and more viscous shoes allow the muscles to come to rest at a later moment in time. However, the softness of the midsole material may not be a good indicator of comfort. The Cushioning Parameter (CP) gives an indication of how much of the lower frequencies are attenuated compared to an uncushioned situation. This parameter is in better accordance with the perceived level of comfort. It was found that the idea of using a metamaterial sole structure, consisting of buckling rods, can increase this Cushioning Parameter. A metamaterial shows varying behaviour due to its structure. In this case the rod’s dimensions are adjusted over the sole of the shoe. In the cushioning heel region the rods need to buckle earlier, while offering more stability in the toe region. By varying the density of the rods the structure may also counter running related effects like pronation. Next to that the required 3D printing of the structure also opens up possibilities for an improved supply chain, shopping experience and brand loyalty. However, the printing technology is still in development and needs improvements concerning the production costs, and most of all its material’s mechanical properties like tear strength. Tests and simulations were performed to check the validity of the claims made about the metamaterial structure. It was claimed, for example, that the Metamaterial concept shows an increased CP-value, compared to the current situation, due to spreading of the impact pressure and lowering of the loading rate. The tests and simulations showed that the metamaterial structure can indeed improve the pressure distribution and lower the loading rate. Therefore the structure is also likely to have an increased CP value. However, the CP value could not be determined within the timeframe of this project and the used material’s behaviour proved to be very complex. Its viscoelastic behaviour was very difficult to reliably simulate and its 3D printed nature meant the material’s behaviour changed with every compression. It is therefore recommended to further explore a TPU printing material, which is likelier to offer more consistent results and overall better material properties.","footwear; running; additive manufacturing; shock attenuation","en","master thesis","","","","","","","Campus only","2016-09-28","Industrial Design Engineering","Industrial Design","","","",""
"uuid:9a4b55c1-7787-432e-abd3-0c400b8a3c4b","http://resolver.tudelft.nl/uuid:9a4b55c1-7787-432e-abd3-0c400b8a3c4b","Low Drift, Wireless Temperature Sensor for Harsh Industrial Applications","Kulkarni, A.","Nihtianov, S. (mentor)","2015","In this thesis, a stand-alone, battery powered, wireless temperature sensor that can be used for diagnostic purposes in harsh industrial environments, including magnetic interferences and very low pressure (vacuum), is presented. Being easily mountable and removable during machine stoppage, this autonomous temperature sensor can be used at any location in a machine to gather temperature data over a period of a few months. The proposed sensor is realized with easily available, off-the-shelf components. It outperforms wireless temperature sensors available on the market in terms of the resolution achieved: 0.2 mK (3 sigma/ 1 reading per second), the power consumption: <1 mW, and long-term stability: <1 mK/year.","wireless temperature sensor; high resolution; longterm stability; NTC temperature sensors","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Microelectronics","","","",""
"uuid:61573fc1-e03d-4c09-a659-e5343dd45523","http://resolver.tudelft.nl/uuid:61573fc1-e03d-4c09-a659-e5343dd45523","Condition monitoring for track circuits: A multiple-model approach","Verbeek, W.P.","Verbert, K.A.J. (mentor); De Schutter, B. (mentor)","2015","","","en","master thesis","","","","","","","","2016-09-28","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control","","","",""
"uuid:dfa0ef97-a72a-47f5-88ff-c2d10872faec","http://resolver.tudelft.nl/uuid:dfa0ef97-a72a-47f5-88ff-c2d10872faec","Operational Scheduling in a Multi-Actor Environment using Multiagent Systems","Brouns, M.","Chorus, C.G. (mentor); Aldewereld, H.M. (mentor); Van Cranenburgh, S. (mentor); De Weerdt, M.M. (mentor); Jansen, M. (mentor)","2015","Liquid bulk is one of the largest industries of the modern world, and efficient scheduling is needed for liquid bulk terminals to remain competitive. A common solution for the planning efficiency problem is to apply planning algorithms, rather than human planners for creating schedules. However, replacing human planners by algorithms is often a difficult problem and its implementation is often obstructed both by management and the planning departments. In order to still achieve efficient schedules, attention has shifted from planning algorithms intended to replace the human planner, to scheduling support systems which aim to assist the planner in making efficient schedules. A core aspect of both automated scheduling systems as well as scheduling support systems is an optimization engine, to allow the human planner to quickly generate alternative schedule options. In recent years, literature has emerged introducing the concept of Multiagent systems for planning and scheduling. Multiagent scheduling seems to be a suitable approach for scheduling in liquid bulk terminals since the agents provide a natural metaphor for the scheduling problem in a liquid bulk terminal. This is mainly because a terminal serves vessels which are owned by a set of customers which are largely uninterested in the vessels of other customers. Multiagent systems can, in such a scenario, offer flexibility with regards to optimality criteria which traditional scheduling systems cannot. The main goal of this study was to determine whether Multiagent Systems (MAS) could be effectively applied for proposing schedule allocation options to assist schedulers in liquid bulk terminals. The research started with an extensive background study, in which it was found that there are a number of stakeholders involved in the scheduling process, which have partially conflicting goals. The main relevant stakeholders are the terminal and its customers, both consisting of several independent departments or entities. The main goals of the terminal are achieving a high terminal utilization, having a low impact on surroundings, having low pressure on operations, and not paying demurrage costs. The main goals from the customer side are achieving short turnaround times, and maintaining product quality. By combining the stakeholder requirements with information regarding the processes, infrastructure, and products at liquid bulk terminals, it was found that the scheduling problem resembles a specific version of the job-shop problem: $JMPT | pmtn, prec, r_i, s_{ij} | multi$. Since this problem is strongly $\NP$-hard it is generally infeasible to find an optimal solution as the size of the problem instance grows, and efficient algorithms are needed to find solutions. After formally defining the scheduling problem as present in liquid bulk terminals, this thesis moved on to determine the applicability of several common Multiagent scheduling solutions based on four dimensions: whether the solution provides a good analogy to the scheduling problem, whether the solution could deal with the arrival and departure of vessels over time, whether the solution could incorporate shared infrastructure between routes and whether terminal preferences could be modelled. It was found that Combinatorial Auctions provided the best fit on the scheduling problem. Due to existing contract structures between terminals and their customers it proved impossible to incorporate an additional payment structure in the scheduling problem, making it infeasible to design the auction to be truthful and strategy-proof. Nonetheless, this is not considered a problem as strategic behaviour can easily be verified. For incorporating multi-attribute optimizations several design directions were explored including Pareto-optima, hard constraints and discrete choice models. It was found that only discrete choice models offered the flexibility needed in this case. Two types of discrete choice models were proposed and implemented: a Utility Maximization model, as well as a Regret Minimization model. Empirical studies have shown that in certain cases, a regret minimizing model provides a significantly better fit on the preference data, meaning that they are better able to capture the trade-offs made by humans when comparing alternatives. As the goal of this scheduling system is to assist the planner in making allocation choices, it can benefit greatly from these features. A major drawback of the regret model is that finding the regret for all alternatives is an $O(n^2)$ operation, rather than an $O(n)$ operation. For solution methods where this posed a bottleneck, a hybrid regret / utility model was proposed and implemented. The auction structure was implemented using a minor modification on the Contract Net Protocol. The final Multiagent Systems consists of three agent types: a Route Finder agent, a Vessel agent, and a Planner agent. The Route Finder agent is responsible for finding routes for specific vessels and products on the terminal, as well as for making accurate estimations on the processing times on that route. This is implemented using a combination of Yen's algorithm and a modification of Dijkstra's shortest path algorithm. The Vessel agent uses te routes from the Route Finder agent to determine a valuation on the routes and submits these valuations as bids to the Planner agent. The Planner agent retrieves all bids and uses these to create final schedule proposals. This is done by solving the Winner Determination Problem (WDP) of the auction. Several algorithms were implemented and tested for solving the WDP, including a Branch and Bound algorithm, informed search methods and two Simulated Annealing approaches. It was found that Simulated Annealing, implemented as a search among alternative complete solutions, was the most feasible solution strategy. Typically, it provided solutions which, on average, achieve a 50\% higher utility model value than a First-Come First-Served Heuristic in three minutes. Furthermore, the Simulated Annealing process can be stopped at any moment and still provide a better alternative solution. All algorithms were implemented using a utility-maximising as well as a regret-minimising strategy. It was found that the top ten results from these different models typically differed from each other with an edit distance of five. Further empirical studies would be needed to determine whether regret-based models provide better accepted solutions in practice than their utility-based counterparts.","multiagent systems; multiprocessor tasks; multi-actor scheduling; scheduling","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi Actor Systems","","SEPAM - Information Architecture","",""
"uuid:e884477a-cef0-4b83-ab44-755321e1f1c4","http://resolver.tudelft.nl/uuid:e884477a-cef0-4b83-ab44-755321e1f1c4","Effectively processing the system performances of infrastructural systems in the tender phase, from the perspective of the Dutch contractor","Koppes, J.A.H.","Hertogh, M.J.C.M. (mentor); Schoenmaker, R. (mentor); Annema, J.A. (mentor)","2015","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Construction Management and Engineering","",""
"uuid:11704f38-a955-4569-9dc7-b7a353b2cc8b","http://resolver.tudelft.nl/uuid:11704f38-a955-4569-9dc7-b7a353b2cc8b","Motion analysis of a Semi-Submersible Crane Vessel at Inconvenient Draft: A flooding Tank approach","Vos, T.G.","Huijsmans, R.H.M. (mentor)","2015","Conventional computer programs based on radiation-diffraction theory are not able to predict the motions of a semi-submersible crane vessel in case the pontoons are only slightly submerged. Cause of the erroneous results are non-linearities like breaking waves on top of the pontoons and emergence of the pontoons itself. Objective of this thesis was to create a numerical model, capable of calculating these motions. A free flooding tank approach is used in order to capture the non-linear behaviour of a body. A significant improvement is found.","Heerema; flooding; tank; SSCV","en","master thesis","","","","","","","","2020-09-25","Mechanical, Maritime and Materials Engineering","Marine & Transport Technology","","Offshore and Dredging Engineering","",""
"uuid:37d97e0d-5b3d-4591-8f78-196c4d7f06f7","http://resolver.tudelft.nl/uuid:37d97e0d-5b3d-4591-8f78-196c4d7f06f7","Macro-economic imbalances in the Eurozone: An input-output analysis of the core-periphery divergence","Leysen, S.L.J.","Storm, S.T.H. (mentor); Cunningham, S. (mentor); Van Beers, C.P. (mentor)","2015","With stagnating growth rates among a majority of its members, soaring unemployment figures and an increased risk of deflation, the economic conditions of the European economy do not strongly reflect a stronger and more united Europe as was intended by the monetary unification in 2001. On the contrary, the recent painstaking negotiations between Greece and its bailout creditors pushed Greece on the brink of leaving the Eurozone and seriously questioned the sustainability of the European monetary union. This research is an attempt to shed new light on the structural evolution of the main Eurozone economies in the years leading up to and following the monetary unification by means of a detailed data analysis using the recently released World Input-Output Database and adopting an inter-country input-output model. This database offers many insights in the productive structures of economies as well as the international interrelatedness in productive and demand structures. By empirically getting an appraisal on both the historal evolution and current structure of the Eurozone’s economy, this thesis furthermore aims at providing new lines of thought to the European policy makers in their search for industrial policy solutions to reduce the macro-economic imbalances and achieve a sustained recovery of the Eurozone economy. The first part of our findings basically all support the overall message that a substantial economic restructuring among the main Eurozone countries has been taking place in the last two decades. We collected various structural insights which all to a greater or lesser extent point out that the monetary unification in 2001 reinforced a process of structural divergence between mainly Germany and the peripheral, or Southern European, economies both in terms of their productive and trade structures and reflected on the surface by a growing divergence between their overall current account balances. Germany has gradually been intensifying its technological capabilities by building up a strong comparative advantage in the production and export of high and medium-high technology manufacturing goods, while the periphery has been technologically stagnant and got locked-up in specializing in low and medium-low tech manufacturing industries as well as several non-traded industries including construction and tourism to name just a few. The second part addresses the important question on how to deal with this structural problem, by presenting several broad policy suggests and lines of thought substantiated by empirical and theoretical arguments. These scenario’s include: laissez-faire (let the process happen without strongly interfering), introduce a Eurozone central fiscal stabilization mech- anism, and try to exploit intra-Eurozone trade spillovers. Especially this last scenario is care- fully empirically examined for the year 2011, the most recent year in the World Input-Output Database. A two-folded conclusion can be drawn from this analysis. Firstly, a ‘coordinated Eurozone-level domestic demand-led strategy’ (by targeted industrial and income policies) could potentially be beneficial in supporting a general sustained recovery of the recession in the Eurozone, because the empirical results reveal that the intra-Eurozone spillovers can significantly be exploited to increase the overall gains from coordinated action. Moreover, our analysis points out that stimulation of the manufacturing industries yields significant more benefits for each Eurozone country, in comparison to scenarios in which all industries would be stimulated. Secondly, with regards to reducing the macro-economic imbalances between the core and peripheral countries, our analysis suggests that it is not feasible to help re-balancing the Eurozone industrial structure by such spillover generating coordinated pro- grams of domestic public investments. The results of our stimulus scenarios clearly suggest that the intra-Eurozone spillover effects generate much smaller impacts on the peripheral countries while contributing much more to the core countries.","input-output analysis","en","master thesis","","","","","","","","","Technology, Policy and Management","Economics of Innovations","","Management of Technology","",""
"uuid:ea680cda-124a-4e50-8235-422b1fce25ee","http://resolver.tudelft.nl/uuid:ea680cda-124a-4e50-8235-422b1fce25ee","Improved Lens Aberration Estimation Using Model-Based Filtering, Applied to Lithographic DUV Imaging Systems","Kant, N.","Verhaegen, M. (mentor); Vanroose, N. (mentor)","2015","In this thesis a model-based filter is developed to improve the estimates of the lens aberrations.","lithography; adaptive optics; system identification; filtering; noise reduction","en","master thesis","","","","","","","","2020-09-01","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control","","","",""
"uuid:4ffec09d-ca44-4ee1-8dec-6d19fe5b950a","http://resolver.tudelft.nl/uuid:4ffec09d-ca44-4ee1-8dec-6d19fe5b950a","Route programming on a mobile barn cleaner","Mars, J.P.","Hindriks, K.V. (mentor)","2015","The ‘Lely Discovery’ is an automated mobile barn cleaner, which routes are programmed by a ‘teaching method’. The routes are specified by a linear action sequence to be executed by the robot. Teaching a route to the Lely Discovery requires users to execute every action first by walking the robot through the barn. The route teaching method is time consuming and the current interface of the teaching method is not user-friendly. When a route requires changes for optimizing the route, the user has to re-execute all the route actions on the robot to and from the position where he wants to change the route. This thesis explores the implementation of a route drawing system which enables users to draw routes on a map. This system can generate an action sequence from the drawn routes on the map. Users can program routes 2 – 3 times faster than with the teaching method and route changes can be made in short notice. In this thesis the results of a user study are presented and the results of the tested routes on the robot via a first prototype of the drawing system. Although the results are not convincing, they provide useful leads for the design of a second prototype.","route programming; mobile barn cleaner; route teaching; route drawing; Lely Discovery; route actions; action sequence","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Interactive Intelligence","","","",""
"uuid:947f912d-5753-4111-b82f-433cfa13fabf","http://resolver.tudelft.nl/uuid:947f912d-5753-4111-b82f-433cfa13fabf","Optimization of the Impregnation Process of Cellulose Materials in High Voltage Power transformers- susceptibility of high-density materials to partial discharge activity","Irahhauten, N.","Morshuis, P.H.F. (mentor); Chmura, L.A. (mentor)","2015","The manufacturing process of high voltage power transformers consists of several phases such as design, assembly, drying and impregnation process which are time consuming. Before electrical testing, a transformer needs an extra standing time for further impregnation of cellulose material in particular those of high density. It should be noted that the duration of the standing time depends on the ratings of the transformer. The main reason for considering this standing time is to reduce the probability of failure during electrical testing. This means that the reduction of such standing time is a crucial issue. Therefore, the main goal of this thesis is to optimize the post impregnation process of high density cellulosic material. One of the requirements for shortening the post impregnation standing time is that during the electrical tests of the transformer the PD level should not exceed the PD acceptance given by IEC standard level. In this thesis, it is investigated if the standing time is related to the occurrence of partial discharges in high density materials. This is done by performing a number of partial discharge measurement on different samples. Further on, tan? measurements were performed on different samples in the course of impregnation process to investigate whether the losses decrease in the course of time. In this research, the impregnation processes of two different cellulosic materials, namely transformerboard (PSP) an laminated wood (KP) are studied. Based on that, an empirical model that predicts the time needed to fully impregnate a cellulosic sample is developed. This model relates the impregnation time with sample material, dimensions, shapes, impregnation and temperature. The proposed model shows a good agreement with the measurements","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","","",""
"uuid:1637a7c0-02cb-41ad-9be8-6db976fd63b6","http://resolver.tudelft.nl/uuid:1637a7c0-02cb-41ad-9be8-6db976fd63b6","Developing and evaluating a model for Surfactant-Foam Flooding","Knol, M.I.C.","Mar-Or, A. (mentor); Van Kruijsdijk, C. (mentor); Vincent Bonnieu, S. (mentor); Rossen, W.R. (mentor); Heimovaara, T.J. (mentor)","2015","Many reservoirs are considered unsuitable candidates for Enhanced Oil Recovery. Surfactant-Foam flooding, also referred to as Low-Tension-Gas flooding, could be a solution to enhance the oil recovery in tight, saline or high temperature reservoirs. SF-flooding combines reduction of the oil-water interfacial tension by surfactant, with mobility control provided by foam. While coreflood experiments have been performed, no Surfactant-Foam model has been developed yet. Therefore, the objective of this research is to develop and evaluate a conceptual model for Surfactant-Foam flooding. Several available foam- and surfactant flooding models are analyzed. By comparing the various approaches, two models are selected based on the identified modelling objectives, model complexity and available experimental data. In this study a Surfactant-Foam model is developed that combines an implicit-texture foam model with a two-phase effective surfactant model. The model assumes foam is in local-equilibrium and correlates salinity and a minimum surfactant concentration with a lowered water-oil interfacial tension. The model does not explicitly model the micro-emulsion phase. Furthermore, the model includes surfactant adsorption and salt and surfactant dispersion. An attempt to model foam diversion with surfactant dispersion has been made, but results are limited due to numerical instability. For every coreflood the effective dispersion coefficient is determined by fitting the experimental effluent salinity data with a solution to the 1D advection-diffusion equation. The performance of the combined model is evaluated with a reservoir simulator. According to the simulation study the surfactant model affects the gas relative permeability through a correlation with the connate water saturation. The defined chemical connate water saturation always affects the gas mobility, either by reducing the gas relative permeability, or by minimizing the impact of the limiting capillary pressure. The results show that an experimentally applied salinity gradient, correlated with a decrease and increase in water-oil interfacial tension, cannot be modelled with capillary number dependent relative permeability curves. In this research it is assumed that the water-oil interfacial cannot increase, after it achieved an ultra-low value. According to the current desaturation approach, oil prefers to flow in the presence of water in stead of gas at ultra-low interfacial tension. More research is required to investigate if this can be related with physics, or if it is purely a modelling artifact. Furthermore, the simulation study shows that the foam matching parameters of the implicit-texture model require optimization for cores with a mutual difference in connate water saturation, due to variation of the impact of the limiting capillary pressure. Simulations are performed to history match experimental data and are aimed to match the measured pressure drop, oil production and effluent salinity profiles. As Surfactant-Foam flooding is taking its first steps in the laboratory, the accuracy and amount of available data is limited. To improve the validity of the model parameters, coreflood experiments in which the foam and surfactant processes are decoupled were interpreted. The surfactant concentration and water saturation are identified as the main drivers affecting the foam texture. Waterflooding pressure data was used to determine the non-chemical relative permeability parameters. The analyzed Surfactant-Foam corefloods are conducted in tight Indiana Limestone cores. Carbonates often have a complex pore structure due to their dual porosity. The experimental data suggests the presence of heterogeneities such as high permeable zones. Therefore, a one-dimensional model is expanded to a two-dimensional model, represented by two layers. Both the one and two-dimensional Surfactant-Foam model achieve a reasonable history match with the cumulative oil recovery and pressure gradient. The two-dimensional model, with a small difference in flow capacity between the layers, successfully matches the size of the oil cut. With the two-dimensional model an improved match with the effluent salinity profile can be achieved at the expense of the oil cut and recovery match. The contribution of measurement errors and heterogeneities to the effluent salinity profile requires more research. As the experimental data are subject to a high level of uncertainty, more corefloods should be performed to identify the most suitable geological representation of the Indiana Limestone.","surfactant-foam flooding; EOR; reservoir engineering; simulation study; modelling","en","master thesis","","","","","","","","2017-09-18","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering","",""
"uuid:35b05f90-1ecf-4015-a514-18efb648be49","http://resolver.tudelft.nl/uuid:35b05f90-1ecf-4015-a514-18efb648be49","Near Wellbore Salt Precipitation in Gas Reservoirs","Nair, R.","Zitha, P.L.J. (mentor); Egberts, P.J.P. (mentor)","2015","Production wells in gas reservoirs often experience rapid production decline towards the late production stage. In many cases, this behavior can be attributed to salt precipitation in the near wellbore region. Water evaporates in the vicinity of the well bore with pressure drop, leading to an increase in the dissolved salt concentration, causing salt precipitation when the solubility limit is exceeded. Salt deposition causes the blockage of gas flow in the vicinity of the wellbore and increases towards the end of lifetime of a gas field, thus becoming problematic for most North Sea assets. Regular downhole fresh water treatments are required to restore the production back to normal. However, these water treatments are expensive and are required quite frequently. Thus, a better understanding of this phenomenon and the conditions under which it takes place is necessary. Additionally, the question arises whether there exists a better reservoir management strategy that can can improve control over well productivity. The model used to model salt precipitation in this work is a compositional two-phase N- component porous media flow model under isothermal conditions(2pNcmin) developed in the numerical simulator DuMuX. A sensitivity analysis was carried out to measure the sensitivity of the model to certain critical parameters and to understand the phenomenon better. The concept of a drying-transport balance was then developed, which can help understand the salt precipitation trends occurring under differing conditions. A study of the variables controlling the productivity of wells plagued by salt precipitation shows that there does exist an optimized reservoir management strategy which can better the cash flow from such wells. Based on these results, a simplified analytical model was developed to show the scope for optimization. Furthermore, an optimization problem was formulated to maximize the Net Present Value (NPV) of wells undergoing cyclic production and water washing.","salt; precipitation; gas","en","master thesis","","","","","","","","2015-12-25","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering","",""
"uuid:c04ebfb6-4fd8-4856-830d-e26061c25e4c","http://resolver.tudelft.nl/uuid:c04ebfb6-4fd8-4856-830d-e26061c25e4c","Analysis of an Actuator Disc under Unsteady Loading: Validation of Engineering Models using Experimental and Numerical Methods","Hong, Vincent (TU Delft Aerospace Engineering)","Simao Ferreira, Carlos (mentor); Hansen, Martin (mentor); Yu, Wei (mentor); Lind, Leif (mentor); Sciacchitano, Andrea (mentor); Delft University of Technology (degree granting institution); Technical University of Denmark (degree granting institution)","2015","Wind turbines operate in the earth's atmospheric boundary layer - an environment where the climate is turbulent and unsteady. In addition, wind turbines usually operate in clusters (wind farms) and are thus subjected to the unsteady wakes generated by upstream turbines. Because of the unsteady inflow, wind turbines suffer from structural fatigue damages. There is therefore motivation to better analyse and model the unsteady loading in order to design better turbines and reduce the cost of energy. To study the unsteady aerodynamics phenomena, an actuator disc model under unsteady loading was simulated using experimental and numerical methods. Experimentally, a porous disc rotor was constructed and tested in the Open Jet Facility, TU Delft. The design involved a novel method to dynamically control the rotor's thrust coefficient through its porosity. Numerically, a dynamically loaded actuator disc was implemented in the RANS equations and solved by the commercial CFD solver Star-CCM+. Results from the experiment, CFD simulations, as well as a free wake vortex ring model were used to benchmark the performance of dynamic inflow engineering models. Results show that the wake of a wind turbine under steady loading is also turbulent and meandering. The Strouhal number of the wake fluctuation closely agrees to that from a previous experiment of a 2-bladed wind turbine (Medici &amp; Alfredsson, 2006). This suggests that the wakes of blade rotors are comparable to that from disc rotors. The tower shadow effect also bears a major influence on the inner wake profile which causes further velocity deficit and higher turbulent intensity. For rotors with higher CT , the velocity recovery process occurs relatively earlier. This is because of the enhanced momentum entrainment arising from a steeper velocity gradient at the shear layer. For the disc rotor under unsteady loading, the wake is affected by unsteady aerodynamics or the dynamic inflow effect. The transient velocity profile experiences an overshoot due to the passage of the 'old' and 'new' shed vortices, which are generated at the wake edge as a consequence of the uniform load profile on the rotor. The subsequent decay of the velocity in the wake or inflow at the rotor plane to the 'new' equilibrium state is a result of the rate of convection of the 'new' shed vortices to the far field. From this research, numerical results showed lower inflow velocity decay rates (velocity response due to the change in loading on the rotor) than that predicted by engineering models. While this is so, it was found that the decay rate is highly influenced by the ambient flow's turbulence. At high ambient turbulence, the 'new' equilibrium state is achieved in a shorter time due to the enhanced momentum mixing process. It is hypothesised that this is the primary reason behind the higher decay rates predicted by empirical-based engineering models.","Windenergy","en","master thesis","","","","","","","","","","","","Aerospace Engineering","",""
"uuid:01b7f1f5-bdad-4a44-a028-78fdd7960f8f","http://resolver.tudelft.nl/uuid:01b7f1f5-bdad-4a44-a028-78fdd7960f8f","Engineering an Optimal Wind Farm","Mahulja, S.","Larsen, G.C. (mentor); Elham, A. (mentor)","2015","The project covers creation of a framework for optimisation of wind farm layouts. The optimisation is done with an objective of maximising the Net Present Value of the farm. With that aim, both power production and the fatigue driven component degradation are considered. The work is split in two main parts. First, in order to circumvent the use of full DWM model simulations, surrogate models of lifetime equivalent loads and power production are created. In the second stage, an optimisation platform is assembled as a two-stage, multi-fidelity strategy encompassing genetic algorithm for global and gradient based algorithm for local optimisation. The analysis of a wind farm is performed using DWM surrogate model which significantly contributes to the speed of computations. On the other hand, the use of surrogates makes it possible to capture the in-stationary effects of wake meandering with sufficient quality. The objective of the thesis is to show the steps required in assembling a framework such as this. Moreover, through the tests performed on Middelgrunden wind farm, the aim is to show the importance of setting the number of turbines as a design variable.","Windenergy; optimisation; DWM model; surrogate modeling; wind farm","en","master thesis","","","","","","","","","Aerospace Engineering","Wind Energy","","European Wind Energy Master","",""
"uuid:fd5dd6bf-1178-48a0-9bf2-744d0dad2046","http://resolver.tudelft.nl/uuid:fd5dd6bf-1178-48a0-9bf2-744d0dad2046","Iterative feedback tuning of feedforward IPC for two-bladed wind turbines: A comparison with conventional IPC","Mulders, S.P.","Van Solingen, E. (mentor)","2015","The development of sustainable energy production methods is an important aspect in lowering the emission of greenhouse gases and exhaustion of fossil fuels. Wind energy is recognized globally as one of the most promising sustainable forms of electricity generation, but the cost of offshore wind energy does not meet the level of fossil-based energy sources. An opportunity for wind energy cost reduction is the deployment of two-bladed wind turbines at offshore locations. Two-bladed turbines save the mass and cost of one rotor blade, which allows the entire wind turbine construction to be designed lighter, and in effect leads to lower initial costs. Moreover, reduction of harmonic fatigue loads on blades and other turbine parts using Individual Pitch Control (IPC) is a way to extend the turbine lifetime. This type of control, using a feedback control structure incorporating the Multi-Blade Coordinate (MBC) transformation, is capable of mitigating the most dominant periodic loads. It is generally known that significant turbine load reductions can be achieved using IPC, however, it is unclear to what extent the MBC pitch signal is optimal in terms of load alleviations. The main goal of this thesis is to develop a self-learning feedforward IPC strategy for a state-ofthe- art two-bladed wind turbine. This IPC strategy will be compared to the conventional feedback IPC implementation. By making use of properties of the MBC transformation, implementations of yaw control by IPC in different configurations are evaluated in terms of performance and stability. As a preparation for the comparison between conventional feedback and self-learning feedforward IPC strategies, a linear control-oriented model from blade pitch angles to harmonic blade loads is identified and used throughout this work for two main purposes. The first purpose is to reveal the level of interaction between both blades, which turns out to be negligible. On the basis of this reasoning, an appropriate cost-function is implemented for optimization of the feedforward controller. The second purpose of the linear model is the simplified and faster development of the Iterative Feedback Tuning (IFT) algorithm, which is later implemented in high-fidelity non-linear wind turbine simulation software. IFT is a self-learning model-free algorithm, and is used to optimize the rotor-position dependent feedforward IPC implementation. It is shown that the self-learning algorithm succeeds in optimization of the feedforward controller at all constant wind speeds, but also in more realistic turbulent wind conditions in the above-rated region. As the feedforward controller generates a constant amplitude pitch signal for each wind speed, the amplitude of the feedforward pitch signal is gain-scheduled on exogenous load signals, in an effort to improve feedforward performance. Results show that the conventional IPC strategy is optimal in terms of load reductions in steady state wind conditions, as the IFT algorithm optimizes to the exact same pitch signal at various constant wind speeds. In turbulent wind conditions, performance results indicate that the constant amplitude feedforward controller is able to attain significant load reductions, but that the performance of the conventional feedback control method is still superior. Comparing the pitch signals of both controllers in turbulent wind conditions, reveals that the conventional method continuously changes the phase of the implemented pitch signal, which is not driven by the varying rotor speed. To see how these changing pitch periodics have an effect on the load reduction capabilities, the feedforward (rotor speed dependent) pitch signal amplitude is scheduled on exogenous signals in various ways. This scheduling shows only minor performance improvements, and it can be concluded that the frequency changes in the pitch signal imposed by MBC, help to actively mitigate periodic blade loads. Using both the azimuth and blade load measurements, the conventional IPC strategy seems to actively track and mitigate the current present blade load harmonic, and it appears to be a serious challenge to develop control strategies that can improve performance already attained by MBC.","Iterative Feedback Tuning; two-bladed wind turbines; Individual Pitch Control (IPC); yaw control; Multi-Blade Coordinate (MBC)","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","DCSC","","Systems and Control","",""
"uuid:854259f8-7297-4e2d-9a6c-591ed0c7acc7","http://resolver.tudelft.nl/uuid:854259f8-7297-4e2d-9a6c-591ed0c7acc7","High Precision Flow Compensated Thermal Conductivity Detector for Gas Sensing with Read-out Circuit","Abarca, A.N.","De Graaf, G. (mentor)","2015","In this work, a novel approach is presented for in-line flow compensation using a dual-Thermal conductivity detector (TCD). TCDs demonstrate a flow dependence and require a calibration at a fixed flow rate. The dual-TCD is composed of two thin-film sensors on membranes in parallel on the same chip. Both are laterally identical, but with a significantly difference on quasi-static sensitivity to the thermal conductivity of the gas by a different sample chamber. The result is a reduced flow rate dependence when differentially operated. The device is fabricated using both bulk and surface micromachining. The design, simulations, fabrication and measurements of the TCD are presented.","thermal conductivity detector; flow compensated","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Microelectronics","","M.Sc in Microelecronics / Electronic Instrumentation Laboratory","",""
"uuid:518d51ac-647e-44a0-b834-e30432f4ff8d","http://resolver.tudelft.nl/uuid:518d51ac-647e-44a0-b834-e30432f4ff8d","Using Game Structuring Methods to Assess the Reliability of the Departure Order of Sea-Going Container Ships","Van der Schelde, T.","Verbraeck, A. (mentor); Cunningham, S.W. (mentor); Warnier, M. (mentor); Nordbeck, P.M. (mentor)","2015","","Ship Handling; Delays; Port of Rotterdam; Game Structuring Methods; Problem Structuring Methods","en","master thesis","","","","","","","","","Technology, Policy and Management","Policy Analysis","","SEPAM","",""
"uuid:68c79016-3e23-4e03-9755-809889d7375f","http://resolver.tudelft.nl/uuid:68c79016-3e23-4e03-9755-809889d7375f","Hierarchical Fracture Modeling Approach","Pluimers, S.B.","Hajibeygi, H. (mentor)","2015","Naturally fractured reservoirs (NFRs) account for a large fraction of the world water and energy resources. The geological complexity and high permeability contrast of naturally fractured reser- voirs make it very challenging to accurately predict flow trough these reservoirs. Simulating flow through fractured porous media has been a research topic for several decades. The simulators are becoming increasingly powerful and efficient. However, accurate simulations of flow through real field NFRs examples still su?er from limited CPU capacity. Such accurate and efficient models and simulation techniques for flow through fractured porous media are crucial to optimize reservoir management strategies, e.g., in terms of ultimate recovery and NPV for hydrocarbon reserves. The embedded discrete fracture model (EDFM), in which fractures are represented explicitly and coupled to the matrix through a transfer function, leads to accurate solutions with much lower computational costs compared with alternative discrete fracture network models. However, NFRs are geologically too complex to be fully represented in an EDFM, because of limited computational capacity. Therefore, a hierarchical model is utilized which combines the EDFM with fracture upscaling. In the HFM small and medium scale fractures are upscaled into an e?ective matrix rock permeability while large-scale fractures are explicitly represented by the EDFM. Though the basic idea of EDFM has been developed and evolved during the last decade, its application to realistic fields involves answering several concerns mainly related to determination of the scale which is the basis of splitting the homogenized and explicit fractures. For the homogenized fractures, in addition, a method which is efficient (specially when dynamic properties are upscaled, such as relative permeabilities) and accurate (e.g., flow-based upscaling or analytical approaches) need to be explored. A hierarchical fracture modeling approach in the EDFM framework (and other discrete- fracture-network models) is applicable only when these concerns are considered. Unfortunately the EDFM literature does not involve any systematic study addressing these important concerns. As such, this thesis work is dedicated to address them. In the first part of this work the grid sensitivity of the EDFM is studied for both single and multiphase flow. The EDFM showed to be sensitive for matrix and fracture grid orientation as well as the relative grid size resolution. These issues are resolved on a fine grid. The solution’s accuracy showed to be dominated by the matrix grid resolution. The second part consists of a study of di?erent fracture upscale criteria in the HFM framework. Flow-based upscale criteria showed to have a significant advantage over the classical length-based upscale criterion.","Fracture Modeling; EDFM; Hierarchical Fracture Modeling; Fracture Upscaling","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering","",""
"uuid:07f61b1f-6334-45c3-90b9-807d31665e46","http://resolver.tudelft.nl/uuid:07f61b1f-6334-45c3-90b9-807d31665e46","Enhancing the experience of travelling by public transport","Buringa, J.","Vermeeren, A.P.O.S. (mentor); Jepma, E.J. (mentor)","2015","The project’s goal is to develop a product which will enhance the experience of travelling by public transport. In order to reach this goal use has been made of a combination of the brand driven innovation design method, the design for emotion method and the LEAN start-up method. The first phase of the process focuses mainly on the analysis of the targeted user. A set of methods were applied in order to gain deeper insights on the needs of users. The following user research methods were used to unearth pragmatic needs as well as aspirations and latent needs of users during travelling by public transport: In the ‘DESTEP analysis’ a broad analysis of macro factors is done regarding the demographic , economic, social, technological ecological and politics factors which are important for the target group and their matching product markets. For the ‘Quantitative research’ people were asked through an online questionnaire to share their opinion and experiences about travelling by public transport. The method was used to gain a broad and inspiring range of insights about public transport. In the ‘ Ethnographic research’ people who were travelling by public transport were observed in order to see what routines they have and what challenges they are facing. In the ‘Qualitative research’ a traveler was shadowed and interviewed in order to gain more insights about their habits and to talk about their personal opinion regarding travelling by public transport. Overall, insights were gathered about what exactly travelling by public transport means to people and what problems and opportunities could be leveraged from that. Based on those findings three design directions for future products were generated: Relax time: “How to ensure that travelling by public transport can become a moment to recharge yourself?” The challenge with this direction is to make travelling by public transport a moment to relax instead of physically and mentally tiring like it is now. Productive time: “How to achieve professional progress during travelling by public transport?” The challenge with this design direction is to make travelling by public transport an opportunity to accomplish goals instead of a waste of time. Controlled time: “How to ensure that the experience of travelling by public transport fits your work/relax mood?” This design direction is a mix of the first two design directions. The main challenge is to make public transport an opportunity to feel empowered and do whatever you want/ need to do. The project continued with the further elaboration of the design direction ‘Controlled time’. As mentioned before this direction is a mix of the first two design direction, this direction showed the most potential to come up with an innovative product. In the second phase of the project, ideas were generated in a creative brainstorm session. Two ideas were tested and validated through the LEAN-method: ‘Transformer suitcase’ and ‘Hangzone’. Simple prototypes were built and tested by potential users. After evaluating, the Hangzone proved to be the most promising idea direction and will be refined in concept refinement phase. In the concept refinement phase there is describe to what the product mustcomply, regarding the product experience, ergonomics and other requirements. After this several concepts are designed. Ultimately, a choice had to be made between ‘the Fisherman chair’ concept and ‘the foldable Box’ concept. The Box concept showed most potential therefore, there is chosen to continue to develop this concept further in the last phase. In the last phase, the box concept (a the stool/swing combination) is further elaborated. There have been made technical calculations, which clearly showed the weak spots in the design. With these calculations, a material could be chosen which is strong enough to make sure that also the weak points in the product can handle the forces. In this case aluminum 6005 is chosen, this material is often used in ladders. After defining the material, the production process could be defined for the different parts of the product. With the knowledge about the features, the material and the production process an estimation can be made of the cost price and the retail price. The retail price of the stool/swing combination which can be used while travelling by public transport amounts to 39.95 EUR. In the last part of the report the focus was on proofing the functional concept as well as the marketing mix for the new product. Within the marketing mix the distribution channels, the packaging, the price and the promotion of the product were defined.","public transport; stool; swing; travelling; working/relaxing","en","master thesis","","","","","","","Campus only","2016-09-25","Industrial Design Engineering","Industrial Design","","","",""
"uuid:1a5f4c7b-ac02-4ee1-9d3b-eae77da8c603","http://resolver.tudelft.nl/uuid:1a5f4c7b-ac02-4ee1-9d3b-eae77da8c603","Identifying and designing a new application for the Hortimotion robot","Van Zonneveld, P.S.","Brezet, J.C. (mentor); Diehl, J.C. (mentor); Boersen, J. (mentor)","2015","Hortimotion is a startup company, developing automated solutions for horticultural environments. With the robot initially being developed around the cleanligh UV light, this graduation project focussed on identifying and designing a new application for the robot. This because the focus had shifted away from the cleanlight UV light, for good reasons. During the first phase of the project, the analysis phase, many methods were used to analyse Hortimotion and the users and market connected to it. This resulted in the identification of value propositions, a problem definition and a clear vision, which stated: “Designing a reliable crop spraying robot, providing the horticulturist the best crop-protection in an automated and flexible way, with the horticulturist being in control”. This vision resulted from analysing the company and interviewing horticulturists in order to find their biggest pain points. With the company lacking a clear focus and with the results from the analysis phase, it was decided to alter the assignment and focus on identifying and designing a new application for the Hortimotion robot. In the second phase, the concept phase, with the use of the lean startup methodology and validation board, experts in the field of crop protection were consulted to (in) validate the riskiest assumption, made during the first phase, which stated that low volume crop spraying would work. Both of them indicated that using low volume crop spraying technology would have huge advantages. The next step was testing the riskiest assumption from the user’s perspective, so context mapping session were held with four different horticulturists. This gave an insight into their vision of an ideal crop spraying robot and their willingness to use other techniques than what is used by them today. This validated the riskiest assumption and pivoted the solution hypothesis to designing a low volume crop spraying robot. For this solution hypothesis, the company Agricult was asked to join the effort of designing a low volume crop spraying robot, on which they replied positively. With the sharpened solution hypotheses and the inclusion of Agricult, the idea development started with formulating a design brief and organising a creative session with fellow IDE student to generate ideas from a different perspective. With the produced product ideas, a testing session was set up to (in)validate the new riskiest assumption which stated that air support would be necessary in the concept. With the conclusion that air support was indeed needed for achieving a proper spray pattern, a final concept was selected. Phase three started with a visit to Agricult, looking for any improvements that could be made on the design of the crop spraying application. With the results from this meeting and the testing session, the last assumptions could be (in)validated, pivoting the solution hypotheses into designing a single side low volume crop spraying robot with air support. With the new solution hypotheses came a name for the application, AUCROPS. Technical designing of AUCROPS’s components started with a list of requirements summing all previous conclusions. Than an effort was made to design the robot with as many “off the shelf” parts as possible, while at the same time incorporating all the aspects from the list of requirements. With the design choices explained, an energy balance was made to indicate AUCROPS has more than enough energy to spray the targeted 1/4th of an hectare. With the use of a user scenario a typical way of using AUCROPS was then illustrated. To show how this new way of using benefits the user, a customer journey was used to clearly show the benefits for the horticulturist. With the design of AUCROPS finished, the production techniques and cost were than discussed at the end of phase III. With the total sum of all components being way below the amount horticulturists are willing to pay, a healthy margin is left for building a healthy business. Finally, chapter six shows how with the use of 3D printing technology a scale model is made and what the plan is for the build of a working prototype. Both can be used by Hortimotion for further developing the AUCROPS.","Hortimotion; autonomous; crop; spraying; strawberry","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design Engineering","","","",""
"uuid:e1678077-9056-47ac-82e6-2762bfb40a63","http://resolver.tudelft.nl/uuid:e1678077-9056-47ac-82e6-2762bfb40a63","Using endpoints process information for malicious behavior detection","Wijnands, K.J.","Van den Berg, J. (mentor); Verwer, S. (mentor); Dignum, V. (mentor); Warnier, M. (mentor); Boone, M. (mentor)","2015","In the last years the impact of malware has become a huge problem. Each year, more and more new malware samples are discovered [2]. And the malware is becoming more sophisticated, for example ransomware. Ransomware encrypts personal documents, such as photos and word documents, and asks money to be able to decrypt these files, hence the name. Malware is not only used for financial gain at the backs of consumers. Sophisticated targeted attacks at enterprise is not uncommon, for example the Sony hack. Although there are many security solutions which should protect endpoints, malware infections still occur. The reason for this has to due with the way current security solutions work. Most of these security solutions act upon known malware behavior and signatures. However when new malware is released and the behavior and signature is still unknown the security solutions cannot protect the endpoint against these infections. To be able to overcome this problem a new method for malware detection should be developed. This detection method should be able to detect malicious behavior without prior knowledge. In scientific literature this type of detection is called anomaly detection [26, 38, 57, 58]. Anomaly detection uses the gathered data to construct a model for normal behavior. Any deviation from the defined normal behavior is seen as an anomaly. At Fox-IT, an IT security company based in the Netherlands, a new security solution is developed, clled FoxGuard. This security solution has the ability to block and allow process activity based on a set of rules. FoxGuard also has the ability to log very detailed low level information of all the processes running on a system. This information include actions such as filesystem actions and registry actions. For a more detailed explanation of the data FoxGuard can gather read section 4.1. In this master thesis an explorative research is conducted on using anomaly detection to detect malicious process on an endpoint by using the detailed process information FoxGuard can collect. The main research question to be answered is: How can anomaly based detection be used for detecting unknown malicious processes based on the detailed process information gathered on a single endpoint? To answer this question first a literature research was conducted on the use of anomaly detection for detecting malicious process in scientific literature, see chapter 2. The main conclusion from the literature study is that using process information combined with tree based representations, large quantities of data can be stored in a compact representation. These compact representations can aid the security officer in graphically analyzing the processes on an endpoint and hereby possibly spotting deviations. In chapter 3 the design requirements of the developed system are analyzed. The conclusion of this analysis is that the amount of data used should be reduced. Not only does it prevent the chances of generating a detection method in which overfitting occurs, reducing the data also reduces the need for huge amounts of memory, storage, processing power and network data send. The collection and preparation of the data is discussed in chapter 4. We have collected four clean datasets, a complete dataset contains one complete bootcycle, and five malware datasets. To generatethe malware dataset the following malware was used: a banking malware, a Remote Access Trojan and a sample of Zeus. The collected data is aggregated, such that a dataframe remains containing per process the number of times it triggered the following activities: filesystem, registry, process create, thread create, object callback and module load. Furthermore it contained the unique process id of the parent process. As the difference between the number of times process activities were triggered the data was normalized between 0 and 10, such that the data of the process activities becomes comparable between each other. A k-means clustering algorithm was applied on the process activities to assign every process to cluster with likewise processes. The aggregated and processed data is used to generate process trees, section 5.1 and heatmaps, section 5.3. These two tools provide a graphical representation of the processes. In a heatmap a security officer can easily spot the processes with high number of process activities per second compared to other processes. In analyzing the process tree deviations were spotted in the top part of the tree, providing proof that an expert can use the process tree to easily spot deviations in the top level. However due to the huge number of nodes present in a tree and the difference in computer usage each day, finding deviations in the lower levels of the tree proofed to be difficult. Analyzing the process trees from the malware sets proofed again that the process tree can help in finding deviations. The rat malware processes were clearly visible as deviations on the process tree. Further more the analysis showed that all malware samples ran could be found in the same part of the process tree. Chapter 6 explains the three algorithms used to calculate the distances between processes in the clean and malware set. These calculated distance are used for marking a process malicious or benign. A process is marked malicious if it is above a set threshold value. To set these threshold values we used the mean and 75%, 80%, 85%, 90% and 95% quantile. All threshold values and algorithms were test and the True Postive Rate, False Negative Rate and the Accuracy were calculated. The outcome of all experiments is shown in chapter 7. In figure 1 the True Positive Rate, False Positive Rate and Accuracy for all algorithms is shown. As can be seen in the figure the malicious processes of the banking malware and rat malware could partly be detect. The highest True Positive rate gained is 0.917 using algorithm 1 and 3 on the banking malware. However paired with this is a high False positive Rate. However the Zeus malware was not detect. In chapter 8 the conclusion and recommendations of this thesis are presented. The main short-coming for the conducted research is way in which the collection of the data was done. By using two different machines differences in processes from the same executable were noticeable. This had to do with the fact that the running times for these processes differs. For future research this experiment should be repeated by collecting data on one machine. Although the shortcoming had its effects on the collected data the proposed algorithms showed the ability to detect malicious processes from at least two out of the three malware types. Furthermore the analysis of the process trees showed us that, although limited, deviations can be detected.","anomaly detection; process trees; heatmaps; malware detection","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering systems and services","","SEPAM - ICT","",""
"uuid:36fcd092-65ad-485a-b4ef-f06cac02017e","http://resolver.tudelft.nl/uuid:36fcd092-65ad-485a-b4ef-f06cac02017e","Validation and improvement of the OPTI-VFA sensor for online VFA monitoring","Deng, Z.","Van Lier, J.B. (mentor); Spanjers, H. (mentor); Kleerebezem, R. (mentor); Zhang, X. (mentor)","2015","","VFA; online monitoring","en","master thesis","","","","","","","","2016-09-24","Civil Engineering and Geosciences","Watermanagement","","Sanitary Engineering","",""
"uuid:a0ec7070-0bb2-477b-bbb2-1f5fee72921b","http://resolver.tudelft.nl/uuid:a0ec7070-0bb2-477b-bbb2-1f5fee72921b","Development of Brash Ice Growth Models and Estimation of the Energy Needs to Manage Ice in the Yamal LNG port in Sabetta","Chomatas, K.","Metrikine, A. (mentor); Hoving, J.S. (mentor)","2015","Navigation in extreme cold conditions, means repeated ice breaking in navigation channels and harbour basins. The broken ice resulting from this repeating procedure, is called ‘’ Brash Ice’’, in opposite to ‘’sea ice’’ which is formed statically in calm water. Just after an ice breaking event, caused by a vessel transit, there is a lot of water near the surface, exposed to freezing without the insulation that the static ice has with either a snow or a thick ice cover on top. This means faster formation of ice, compared to the case of static or sea ice. The main objective of this graduation study is the development of Brash Ice Growth Models (BIGMs), with the use of both analytical and numerical thermodynamic methods that would be able to predict the thickness of the total ice or brash ice layer developing immediately below a ship track transited by vessels, with the basic assumption being that all the ice grown remains within the ship track.","brash ice","en","master thesis","","","","","","","","2019-06-30","Mechanical, Maritime and Materials Engineering","Offshore Engineering","","MSc in Offshore and Dredging Engineering","",""
"uuid:239d2886-fb7a-4f3e-90ba-db4a49c0c652","http://resolver.tudelft.nl/uuid:239d2886-fb7a-4f3e-90ba-db4a49c0c652","Traffic Safety around Primary Schools","Geerts, B.A.M.","Hagenzieker, M.P. (mentor); Wiggenraad, P.B.L. (mentor); Van Ham, J.C. (mentor); Schouten, A. (mentor)","2015","The research is about traffic safety in school environments. It is investigated how much the subjective traffic safety perceived by the users of the environment can be objectively explained. In that way it can be determined how much influence a municipality has, in its role as road authority, in influencing the perceived traffic safety. In the research a new process desing for municipalities is presented when they want to increase the perceived traffic safety in school zones.","objective traffic safety; subjective traffic safety; school environments; primary school zone; process desing; policy advice","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport, Infrastructure and Logistics","","","",""
"uuid:fbe78dd2-beeb-4d7f-9094-9e6a8c9154fd","http://resolver.tudelft.nl/uuid:fbe78dd2-beeb-4d7f-9094-9e6a8c9154fd","Dynamically balancing a flexure-based scan stage inside a scanning electron microscope","Sebek, P.","Herder, J.L. (mentor); Van Koppen, J. (mentor); Kappelhof, P. (mentor)","2015","Delmic's SECOM platform integrates an optical microscope in a scanning electron microscope. A flexure based scan stage has been designed and produced to be used in the next generation of the SECOM platform. However, the reaction forces of the scan stage produce vibrations in the platform. These vibrations negatively affect the position accuracy and velocity stability of the scan stage, resulting in very poor image quality. To successfully implement the scan stage in the new platform, the errors must be reduced. The errors caused by the reaction forces of the scan stage in the SECOM platform are quantified by modelling, testing and calculations. Conventional methods to reduce errors do not offer a direct solution for the errors caused by the scan stage. Literature is reviewed in search of other disciplines that are capable of reducing reaction force related errors. Dynamic balancing, often used in linkages, can theoretically completely eliminate all reaction forces of a mechanism, and therefore remove the error source in the SECOM platform. The scan stage is redesigned in order to dynamically balance the mechanism. To use dynamic balancing is a challenge; no examples of macro-scale dynamically-balanced flexure mechanisms have been found in literature. Literature is also studied on the design of ultra-precision XY and XY$\Theta$ mechanisms. The most commonly used components are identified, and the optimal design configuration is concluded. The scan stage is designed according to the optimal design guidelines. The end effector of the scan stage is exact constrained, another novel feature when compared to the mechanisms found in literature. Measurements on the performance of dynamically balanced linkage mechanisms are not common, most designs found in literature are purely theoretical. The scanning mechanism is produced and the dynamic performance of the mechanism is verified. Subsequently, the reaction force of the mechanism is measured. Measurements show that the reaction force has successfully been reduced by a factor 50. The errors caused by such a force in the SECOM platform are within acceptable limits. The base of the mechanism was assumed to be infinitely stiff, an assumption commonly made in dynamic balancing literature. However, measurements also show that internal reaction forces of the mechanism cause significant deformations in the base, which can transfer a force. This potentially applies to dynamically balanced linkages as well. Consequently, an additional design guideline has been proposed for creating mechanisms that transmit no reaction forces to other systems or components: The displacements as a result of the inertial forces, and forces caused by the strain of the flexure elements of the mechanism, should be zero with respect to the center of mass of the base of the mechanism, at all mounting points and connection points of the mechanism to other systems. In this thesis, a dynamically balanced flexure-based mechanism is successfully designed and tested. No comparable mechanism has been found in literature, and therefore a paper has been written on the design of the mechanism. The work of this thesis shows that the principle of dynamic balancing can be successfully used in the design of flexure mechanisms. As a result, flexure mechanisms can potentially become cheaper, more compact and easier to implement in systems. This could make way for flexure mechanisms to be used in new technologies, where they are not seen today.","dynamic balancing; positioning mechanism; high accuracy; high bandwidth; flexure; compliant; scanning","en","master thesis","","","","","","","","2017-12-31","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering","","Mechatronic Systems Design","",""
"uuid:25e3f09d-c6b6-40f8-931b-42a3cc59a614","http://resolver.tudelft.nl/uuid:25e3f09d-c6b6-40f8-931b-42a3cc59a614","Adaptability and modularity of a children's stroller","Van Haaren, E.","Van Heur, R.J.H.G. (mentor); Minnoye, A.L.M. (mentor); De Visser, W.J. (mentor)","2015","Joolz is a company that finds itself in the premium section of the stroller market. With a portfolio containing two high end strollers and a fast growth into several new markets, it looks to expand its portfolio in different directions. The future goal is to be a company that can provide complete child care concepts for the first five years of childhood. The stroller market is a highly competitive one and with competitors looking very much alike, there is a real need for a distinctive range of products. The company would like to focus on a sustainable image and product line. Modularity could help in lowering transport volume and a take back of modules for reuse or recycling. The drivers for a modular system have shifted in recent decades from lowering production cost and raising customization options for customers to making easily adaptable and upgradable systems that are far more sustainable because of their longer lifecycle. The shift towards adaptable products which grow with the users’ lifestyle is the next step in modular design. Within the stroller portfolio six different types can be identified, each of them has a set of common functions and a set of distinct functions. When all these functions are mapped, a clear vision on the platform boundaries can be formed. This platform will be the basis of every type of stroller and the modules will further define the products functionality. These are the first two parts of a modular design. The third part of a modular design, the interfaces, will define the use of the product. The aim is to have more tailored products that fit the users’ lifestyle, however, there are several ways to reach this goal. The main question is: will users adapt their product many times, are they likely to do that only a few times or do they not do that at all during its lifecycle? Within this project the vision is that giving users a chance to adapt their product easily will have them choose a Joolz quicker and build up a relationship with the company through repeat purchases. The opportunity to bind Joolz users for a longer time and create an impact in the second hand market as well should be supported by a service system with local development, production and servicing, try-out shops, take-back of obsolete modules and parts and functional sales. Only with a service system like this, the full potential of modularity can be reached. To enable this adaptability, a common integrated product platform has been designed. This platform accommodates the modules that can shift and expand the product functionality. Two Interfaces have been designed also. These are the connections between the modules, they allow the user to change between modules. Both interfaces have been designed with safety, durability and user friendliness in high regard. The final section of the report deals with the validation of both the interface concepts. Two functional prototypes are made and tested. The report finishes with an overview of the process and a set of recommendations for future development going mainly in the direction of function expansion, child-caretaker relationships and smart systems development supporting the well being of the child.","modular; adaptable; stroller; child; platform; interface","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Master of Science Integrated Product Design","",""
"uuid:5e678d66-7b0c-48ee-a6b0-f4abc54e0336","http://resolver.tudelft.nl/uuid:5e678d66-7b0c-48ee-a6b0-f4abc54e0336","Context-Aware Sleep Interventions","Brown, J.L.","Smulders, F.E.H.M. (mentor); Roscam Abbing, E. (mentor); Payne, A. (mentor)","2015","We spend about a third of our lives asleep (Mental Health Foundation, 2011; Bupa UK, 2015). Sleep is a vital dimension of health and wellbeing alongside a healthy diet, exercise and breathing (Bupa, 2007; Mental Health Foundation, 2011). The NHS (2013) claims that getting enough sleep boosts immunity, can help you lose weight, boosts mental wellbeing, prevents diabetes, increases sex drive, prevents heart disease and increases fertility. Even though sleep is a vital part of our health, according to a national Bupa Healthwatch survey, 51% of Australians say they don’t get enough sleep (Bupa, 2007). Lifestyle choices and the context around getting sleep could be contributing factors for people not getting more sleep. This report looks into lifestyle and context reasons associated with sleep deprivation in people without a medical sleep problem. Subsequently, the report explores context-aware solutions for this group of people to get more sleep and better sleep through a design process. The final design solution is a market-validated proof of concept that uses digital technology as a tool for behaviour change interventions to improve sleep in non-medical sleep deprivation, with a holistic goal for improving health and wellbeing and preventing related accidents, illness and chronic conditions. The solution pulls together digital marketing, behaviour change and machine learning to create a context-aware and personalised sleep intervention with a supporting branding, business model and guides for product development through to commercialisation.","sleep deprivation; context-aware; behaviour change","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","Master of Science Strategic Product Design","",""
"uuid:08481ec6-d6df-4162-b254-8b99eeccc6d1","http://resolver.tudelft.nl/uuid:08481ec6-d6df-4162-b254-8b99eeccc6d1","Topology Optimization of Heat Exchangers","Papazoglou, P.","Langelaar, M. (mentor)","2015","Heat exchangers have long been used in a wide variety of industrial applications, such as for energy recovery from by-products, temperature regulation in chemical processes, refrigeration, or cooling of car engines. Typically, each application requires a different type of heat exchanger such as, tube, shell, with/without phase change, mixing/non mixing etc. heat exchangers. Due to their importance, there has been an ongoing interest in reducing the perational/constructional costs and increasing the efficiency. A lot of research has be done in optimizing certain features of heat exchangers (e.g. tube dimensions, fin thickness etc.), but so far none of them investigates the optimization of the whole topology of a heat exchanger. The aim of this thesis is to optimize the structure of a two flow heat exchanger, by means of topology optimization. More specifically we aim to maximize the efficiency of heat transfer, given some predefined pressure drop and dimension constraints. These constraints are necessitated by the need of achieving a reduced operating (pressure drop) and manufacturing dimensions) costs. A heat exchanger, being a multi-physics system, can be described by two physical phenomena: the flow of the fluid and the heat transfer. In this study we focus on heat exchanger governed by an isothermal and incompressible Stokes flow with low Reynolds number, while the heat transfer is assumed to be advective-conductive heat transfer, without internal heat generation, characterised by a relatively high Peclet number. We evaluate two novel models for topology optimization of heat exchangers; the Fluid Tracking Model and the Multi-Material Model. Throughout the experimental evaluation we saw that the Multi-Material Model performs best. The Fluid Tracking Model did not produce optimal results and was unable to enforce non-mixing designs. The Multi-Material Model optimized designs that maximized the heat transfer surface area between the fluids. Furthermore the designs illustrated a wall at the interfaces of the two fluids, keeping the two flows separated. Both 2D and 3D cases were studied. The 3D optimal results achieved a moderate improvement in performance over a simple design of a concentric tube heat exchanger.","topology optimization; Heat exchanger; multi-flow; multi-material","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering","","Structural Optimization and Mechanics","",""
"uuid:b9218b73-ccd5-4cbc-b9d7-326440817b3b","http://resolver.tudelft.nl/uuid:b9218b73-ccd5-4cbc-b9d7-326440817b3b","Retrofitting of Existing Infrastructure Structures with Advanced Cementitious Materials (ACM's)","Wink, H.P.","Hordijk, D.A. (mentor); Reitsema, A.D. (mentor); Savija, B. (mentor); Grünewald, S. (mentor); Houben, L. (mentor)","2015","Existing structures are built according to the valid codes at that time and reaching their design life time. The traffic loads and traffic intensity has also increased. In addition, the existing reinforced concrete structures are exposed to degradation mechanisms, like chlorides and carbonation during their lifetime. Existing concrete road infrastructure can have the need to restore the structural performance or the durability. Advanced Ce-mentitious Materials (ACM’s) has the potential for retrofitting due to the increased strength properties and durability. High Performance Concrete (HPC) and especially Ultra-High Performance Fibre Reinforced Concrete (UHPFRC) have increased strength properties and a dense matrix, which prevents the ingress of detrimental substances. Strain Hardening Cementitious Composites (SHCC) has a high tensile strain capacity, whereby the crack widths are limited. The strengthening effect of SHCC retrofitting is negligible. The biggest potential for retrofitting of existing infrastructure with ACM’s is in retrofitting of durability problems. The structural shear strength problems are less severe than originally thought and traditional structural retrofit solutions like FRP lamellas and external prestressing seems to be more feasible and more economic solutions compared to an ACM layer retrofit system. On the other hand, the reinforcement corrosion of concrete structures is expected to increase. Moreover, HPC and UHPFRC both have a dense matrix to prevent the ingress of detrimental substances. SHCC exhibits a large tensile strain capacity and limited crack widths. A specific durability ACM retrofit solution is used in a numerical model (see Figure 1). The durability retrofitting is based on a cast in situ ACM layer on a vertical existing surface, where the time-dependent development of the ACM properties and the interface between the ACM layer and old concrete determine the structural behav-iour. The mechanical properties develop due to the on-going hydration. Shrinkage is the cause of the main stresses in the model and the stresses are partly relaxed by the tensile creep behaviour of the ACM layer. The basic failure modes are debonding of the ACM layer at the free ends and transverse cracking in the ACM layer. A vertical existing surface is used, because vertical surfaces of road infrastructure structures can exhibit dura-bility problems due to de-icing salts in splash water and also tidal zones of marine structures. A parameter study is performed to enhance the understanding of the structural behaviour due to the time-dependent behaviour of the new ACM layer. A thicker ACM layer causes more debonding and transverse crack-ing behaviour, because the total shrinkage displacement is higher which causes higher stresses. A larger shrink-age strain causes higher stresses and more cracking. The creep in the ACM layer causes a stress relaxation. Increased creep compliance results in less severe cracking. The influence of the thickness of the old concrete structure and the length of the ACM retrofitting is limited. A decreased tensile strength and fracture energy of the debonding interface causes a more severe debonding of the ACM layer, because the interface is made weaker. A new ACM layer thickness of 50 mm can be applied without excessive debonding and transverse cracks based on the parameter study of the numerical model. UHPFRC is advised to use for durability retrofitting. It has limited cracking behaviour and a layer of 10 mm is sufficient to prevent the ingress of detrimental substances. The numerical model exhibits in general the expected behaviour. The model has to be improved for a detailed analysis of an ACM retrofit solution. Further research is necessary to the debonding behaviour in the debonding interface. In addition, an experimental program would be necessary to investigate the development of the time-dependent properties of a specific ACM.","retrofitting; existing structures; Advanced Cementitious Materials","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Concrete Structures/Structural and Building Engineering","",""
"uuid:335903c0-60b1-48fc-be72-292c87404f25","http://resolver.tudelft.nl/uuid:335903c0-60b1-48fc-be72-292c87404f25","Performance of a KID: On the influence of the resonator geometry on the noise and Q-factor of a kinetic inductance detector","Scholten, K.","Baselmans, J.J.A. (mentor); Thijssen, J.M. (mentor)","2015","Space-based astronomy is in need of a photon-noise limited detector with multiplexing opportunities. The kinetic inductance detector (KID) is capable of both properties. An incoming photon breaks Cooper pairs in the superconducting layer of the KID, which leads to a change in inductance, which can be detected with phase- and amplitude read-out. Photon noise, generation-recombination noise are inevitable noise sources, but two-level system noise can be minimized by adjusting the resonator and its geometry. To gain insight in the noise and loss sources this thesis investigates the difference between microstrip and co-planar waveguide resonators; and the difference between a silicon substrate and a silicon-on-insulator substrate. Numerical analysis shows that: noise in all types of resonators are dielectric bulk dominated; an Al superconducting layer shows has different losses in terms of Q-factor, compared to a NbTiN superconducting layer, but quantitatively the same internal Q-factor in the order of 105 is achieved; an Al superconducting layer in a microstrip has 5dBc/Hz and 2dBc/Hz higher noise in the metal-air and metal-edge interfaces, respectively, compared to a NbTiN superconducting layer; an Al on Si CPW and microstrip have the same Q-factor performance; an Al on Si CPW has a 2-8dBc/Hz more noise at all interfaces compared to an Al on Si microstrip; a buried oxide layer has no influence on the loss in terms of the Q-factor for 100nm Al on Si substrate CPWs; and that the noise of an Al on SOI CPW is bulk dominated, and the dielectric bulk contributes 10dBc/Hz more to the noise, compared to an Al on Si CPW. Experimental analysis shows that a gapwidth of wgap ? 10?m for Al on SOI CPWs shows no response, due to high two-level system noise. Low Q-factors in the order of 104 have been measured for Al on SOI CPWs, which is low compared to literature. A high set-up noise of -85dBc/Hz and divergent relationships with the internal power indicate that TLS-noise does not dominate the measurements, and set-up negatively influences the measurements. In conclusion, the resonator geometry is an important factor for the performance of a KID in terms of noise and Q-factors. A buried oxide layer has no consequence for the Q-factor, and negatively affects the noise of a CPW.","MKID","en","bachelor thesis","","","","","","","","","Applied Sciences","Terahertz Sensing Group","","","",""
"uuid:9b1fda46-0cf9-4e87-ac25-690289eecd29","http://resolver.tudelft.nl/uuid:9b1fda46-0cf9-4e87-ac25-690289eecd29","Removing iron under anoxic conditions","Schoonenberg Kegel, F.","Van der Meer, W.G.J. (mentor); Olsthoorn, T.N. (mentor); Halem, D. (mentor); Hofs, P.S. (mentor); Van Paassen, J.A.M. (mentor)","2015","Aeration and rapid sand filtration are common practice for the treatment of iron, manganese and ammonium containing ground water for more than a century. Nevertheless, this process has not become obsolete. On the contrary, this thesis assumes that enhanced rapid sand filters remain standard treatment technology in the 21st-century. Iron can be removed by homogeneous, heterogeneous and biological oxidation. The potential advantages of heterogeneous iron oxidation over homogeneous iron removal are high filtration rates, excellent iron removal and reduced losses of backwash water as demonstrated by the excellent performance of the pre-filters of water treatment plant Grobbendonk. Heterogeneous iron oxidation dominates at Grobbendonk because the supernatant is slightly anoxic and the pH is low. Thus adsorption and oxidation occur simultaneously. The target feed water in this thesis is deep anoxic ground water, characterized by high iron and methane concentrations. The typical high methane values demand intensive aeration to prevent excessive formation of mucus or slime by methanotrophic bacteria. This research focuses therefore on heterogeneous iron oxidation that can be applied by running an adsorptive filter under anoxic conditions and regenerate it intermittently under oxic conditions. In other words, adsorption and oxidation are separated in time. This work consists of three main parts: characterization of filter medium, batch experiments and pilot plant experiments. Sequential extraction provided useful information about the accumulation on filter medium grains. The accumulation on sand/anthracite grains of the two investigated production sites contained predominantly iron(hydr)oxides e.g. ferrihydrite, which was in agreement with literature. A method was developed to determine Freundlich isotherms constants that describe the adsorption of iron or manganese on filter medium of water treatment plants Holten and Spannenburg. These Freundlich isotherms allowed a rough estimation of the maximum operating time of a rapid sand filter in intermittent regeneration mode, which amounted to 189 bed volumes. Pilot plant research was initiated following up on these positive results. The pilot plant consisted of two columns. The first column was filled with iron-coated sand of Holten as an adsorbent. The aim of the experiments was to determine the optimal regeneration time and the influence of the Empty-Bed Contact Time (EBCT). An increase of the EBCT, e.g. a reduction of flow rate, resulted in an increase of the iron removal. Nevertheless, the performance was lower than estimated from the batch experiments. The explanations are: 1) there was no sharp-edged adsorption front, 2) calcium adsorption competed with iron adsorption. The absence of a sharp-edged adsorption indicates that kinetic limitations play an important role. Furthermore, a salt spiking experiment showed that the filter column did not show ideal plug flow reactor behavior. The anoxic process requires a considerable amount of oxic water for its regeneration. Oxic water beyond that to oxidize adsorbed iron(II) was necessary to expel methane. The slow start-up with virgin sand is an additional drawback, because it results in high water losses. However, a start-up with iron-coated sand is, of course, an option to be further investigated. The second filter column was filled with virgin sand and then used to determine the time that the process requires to get going, i.e. the startup time. It took more than 123 cycles, which implies a water loss of 3295 bed volumes, before iron became significantly removed. The research proved that a conventional oxic filter with aerators performs better than an anoxic filter in intermittent mode. With equal iron removal efficiencies, the number of bed volumes that can be treated and the accumulated iron load of a conventional filter are much higher. It is plausible that the anoxic pilot plant performs better at an even lower EBCT. However, this comes at the expense of one of the potential advantages of heterogeneous iron oxidation, a high filtration rate. Therefore, the outcome of this thesis does not yet warrant a change of the process configuration. When we furthermore compare the outcome of the pilot plant experiments with the performance of the pre-filters of Grobbendonk, we have to conclude that heterogeneous iron removal performs better when adsorption and oxidation occur simultaneously, instead of separated in time.","water treatment; drinking water; iron removal; homogeneous iron removal; heterogeneous iron removal; rapid sand filters; subsurface iron removal; adsorption; freundlich isotherm","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","Sanitary Engineering","",""
"uuid:9207ac3c-7867-4791-84f4-ea1b39c55b8d","http://resolver.tudelft.nl/uuid:9207ac3c-7867-4791-84f4-ea1b39c55b8d","Modelling the effect of oil on foam for EOR: Local equilibrium behavior","Ansari, M.N.","Rossen, W.R. (mentor); Tang, J. (mentor)","2015","Enhance Oil Recovery (EOR) techniques are theoretically very promising but in real life suffer the effects of many physical complications in the subsurface which are yet to be understood in detail. One such complication is the detrimental effect of oil in foam in foam EOR. Current-generation reservoir simulators represent these effects in an approximate way. The STARS simulator is one such simulator. Though nearly 20 years old, till now there has not been a detailed study on how its parameters predict foam behavior without oil. We investigate in detail the effect of the oil-related parameters in the STARS simulator by studying the behavior of foam in the two foam-flow regimes, as identified by Osterloh and Jante and Alvarez et al, on steady state behavior of foam without oil. The focus of this thesis is to study the shift in the two foam-flow regimes with oil present. This is achieved by fixing oil saturation, fixing oil superficial velocity or by fixing the oil to water superficial velocity ratio. Initially we employ a Corey-type relative- permeability function. We investigate the effects of oil-related parameters with fixed limiting water saturation (wet foam model) but later study the effects of changing limiting water saturation (dry-out foam model). We then proceed to understand the model behavior for three-phase oil relative permeabilities by implementing Stones Model II oil relative permeabilities in both the models (STARS Foam Simulator). Additionally, we generate 3D plots of pressure gradients as a function of phase saturations to examine the effect of oil-related parameters of the STARS simulator on foam. We capture and understand the behavior of all above mentioned cases on gas foam mobility reduction factor (FM) plots to predict foam performance under Corey’s and Stone’s saturation profiles. The study reveals combinations of oil, water and gas superficial velocities where the steady-state saturations and foam state are not unique. We study these cases further using a simple 1D in-compressible simulator.","foam; enhanced oil recovery; effect of oil on foam; foam simulation; mobility control","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering/Applied Earth Sciences","",""
"uuid:193db2ba-f5ab-4831-a034-64ea5ae5ba3f","http://resolver.tudelft.nl/uuid:193db2ba-f5ab-4831-a034-64ea5ae5ba3f","Modeling Electromagnetic Fields in Strongly Inhomogeneous Media: An Application in MRI","Koolstra, K.","Remis, R. (mentor); Brink, W. (mentor); Van Gijzen, M. (mentor)","2015","Modeling electromagnetic fields in MRI involves two main challenges: the solution of the scattering problem resulting from Maxwell's equations has to be accurate and it has to be obtained within short computation time. In this thesis a method that meets both requirements is searched for. The method of moments with rooftop basis functions is used to discretize different formulations of the volume integral equation corresponding to Maxwell's equations. A simple two-layer cylinder test case is used to compare each solution with a derived analytical solution for scattering on a two-layer cylinder. Two types of errors are analyzed. The good performance of a staggered grid with respect to a non-staggered grid shows that the way of treating the mixed derivative terms is of great importance. The performance of a higher order approximation scheme on a non-staggered grid is close to the performance of a second order approximation scheme on a staggered grid. A contrast study shows that these two methods are particularly beneficial for high contrasts and on low resolution. The performance of the iterative solvers IDR(s) and GMRES is tested for each discretization method. IDR(4) shows excellent performance in reducing the computation time that is obtained with GMRES. Finally, human body simulations confirm the findings from the two-layer cylinder test case.","electromagnetic scattering; high permittivity materials; magnetic resonance modeling; volume integral equation; Galerkin's method","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Numerical Analysis","","Applied Mathematics","",""
"uuid:765b74fb-ef1b-4a30-96fe-303696714738","http://resolver.tudelft.nl/uuid:765b74fb-ef1b-4a30-96fe-303696714738","Seismic Sensitivity Analysis of Different Carbonate Lithofacies: A Full-elastic Simulation","Farish, D.R.","Luthi, S.M. (mentor); Feng, R. (mentor)","2015","Using a complete 2D full-elastic seismic simulation from geological modelling, to 2D seismic data forward modelling, to non-linear full-waveform inversion – the interaction of various carbonate lithofacies are observed. Specifically, three different environments of deposition within a carbonate shelf along with their pore-types ranging from partially cemented moldic macropores, open vugs, to the intergranular pores of coral facies are of key interest. To fully grasp the interaction, lithofacies determination of the inverted parameters are characterized to see if the different lithofacies can be matched to their true counterpart of the geological model or distinguishable based on crossplot clustering. In the end, our findings show that individual pore-types are unable to be detected using only inverted surface-seismic data – rather, various facies groups of depositional environments are capable of being characterized by the crossplot clustering of elastic parameters and seismic velocities.","reservoir evaluation; carbonates; seismic; forward modelling; full-waveform inversion; reservoir characterization; full-elastic","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Applied Geology","",""
"uuid:611583f1-b200-4851-915e-76a43c42fd46","http://resolver.tudelft.nl/uuid:611583f1-b200-4851-915e-76a43c42fd46","Attack pattern ontology: A common language for attack information sharing between organizations","Zhu, Y.","Janssen, M.F.W.H.A. (mentor); Pieters, W. (mentor); Oey, M.A. (mentor); Hadiosmanovi?, D. (mentor)","2015","Cyber attack nowadays is increasingly being reported. Defenders need a good understanding of attacker’s perspective in order to accurately anticipate threats and effectively mitigate attacks. This understanding can be obtained through sharing attack pattern. However, in the existing researches the consideration about information sharing is not integrated into the attack pattern concept. In this paper, we propose an attack pattern ontology as a common language of information sharing; the goal is to demonstrate how this ontology may effectively support cyber security information sharing. Based on the existing theories about attack pattern, we developed an ontological model to present attack information. The research can be further developed to integrate attacker profile ontology with the attack pattern ontology, which enables more systematic analysis of cyber attacks.","cyber security; information sharing; cyber attack; attack pattern; ontology","en","master thesis","","","","","","","","","Technology, Policy and Management","Information and communication technology","","","",""
"uuid:6c6d6681-7327-4fa8-b29f-c5abaad8abc4","http://resolver.tudelft.nl/uuid:6c6d6681-7327-4fa8-b29f-c5abaad8abc4","Planning the large-scale distribution of smart energy meters","Fransen, J.J.F.","Janssen, M.F.W.H.A. (mentor); Van Duin, J.H.R. (mentor)","2015","","","en","master thesis","","","","","","","","2017-09-24","Technology, Policy and Management","Engineering Systems and Services","","","",""
"uuid:787fc710-52d0-4de6-9669-c2de10ba8018","http://resolver.tudelft.nl/uuid:787fc710-52d0-4de6-9669-c2de10ba8018","Zakkende grond op ankers","Dijkstra, S.D.","Jonkman, S.N. (mentor); Bakker, K.J. (mentor); Abspoel, R. (mentor); Meinhardt, G. (mentor); Larsen, H. (mentor); De Gijt, J.G. (mentor); Van Schaik, C.N. (mentor)","2015","The graduation thesis focuses on providing insight in the consequences of subsiding ground on soil anchors. With the years, the bars of an anchored sheet pile walls for example, can be unfavourably loaded due to subsiding soil. As a result of these lateral loads, the bar will deform and elongate, causing the stresses to increase. The applicability of the current design method of CUR 166 is limited. Hereby referring to using it for determining the deformation of anchors in common situations, such as inclined anchors crossing multiple layers of soil. Because of the ambiguities and limitations of this method, the report will verify and modify the design method of CUR 166. While looking at the issues regarding subsiding soil, distinction will be made between the soil behaviour around the anchor and the way in which the anchor bar deforms due to the soil load. As a way of gaining insight in the consequences of subsiding soil on anchors, Deltares has conducted model tests on small sized bars and measured the resulting loads. By simulating these tests with Plaxis 2D has the way subsiding soil loads the bars been defined. In contrast to the current method of CUR 166, the value of the shear strength should depend on the drained or undrained behavior of the soil. The result is an almost constant value for the factor to determine the load with, this in contrast to the currently used variable / unclear factor. The subsidence process loads the anchor laterally and causes it to deform. A new design method (vernieuwde rekenmethode) has been created in order to approximate the deformation and stress–strain curve of the anchor with along the free anchor length. This method is applicable for a wide range of situations, including the loading of inclined anchors crossing multiple layers of soil. The current method uses an elastic calculation to verify the stresses with the regulations. The possibilities of using a plastic calculation have been researched. Due to the displacement controlled nature of subsiding soil loading the bar, can the bending moments be absorbed by the yielding of the bar. As long as the curve of the yielding anchor bar is not obstructed by supports, the rotational capacity of the anchor will be sufficiently large to allow for yielding of the steel. In Plaxis 2D and 3D has the applicability of an embedded pile as an anchorage been researched in order to verify it with the vernieuwde rekenmethode. When comparing the different methods, it can be concluded that the resulting increase in axial anchor force due to subsiding soil, defined with the CUR 166, is conservative compared to the Plaxis simulation and the vernieuwde rekenmethode.","CUR 166; ankers; grond; zetting; embedded pile element; Plaxis 2D; Plaxis 3D","nl","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","","",""
"uuid:81b3059f-42b9-47b6-b60e-c610fee0a07d","http://resolver.tudelft.nl/uuid:81b3059f-42b9-47b6-b60e-c610fee0a07d","Networked Indoor Lighting Controls with Visible Light Communication","Warmerdam, K.P.","Zuniga, M. (mentor)","2015","","lighting control; visible light communication; MIMO system","en","master thesis","","","","","","","","2016-01-19","Electrical Engineering, Mathematics and Computer Science","Embedded Software","","Embedded Systems","",""
"uuid:cb1d7058-2cfa-439a-bb2f-22a6b0e5bb2a","http://resolver.tudelft.nl/uuid:cb1d7058-2cfa-439a-bb2f-22a6b0e5bb2a","Model Order Reduction for Non-Linear Structural Dynamics","Jain, S.","Van Gijzen, M.B. (mentor); Tiso, P. (mentor)","2015","","Model Order Reduction; Nonlinear Manifold; Projection based MOR","en","master thesis","","","","","","","","2015-09-16","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering","","Engineering Mechanics","",""
"uuid:776bdbdc-919d-4322-9ef4-b86d636c6851","http://resolver.tudelft.nl/uuid:776bdbdc-919d-4322-9ef4-b86d636c6851","Redesigning KLM's services regarding Passengers with Reduced Mobility","Veeger, S.","Santema, S.C. (mentor); De Lille, C.S.H. (mentor)","2015","PRMs, Passengers with Reduced Mobility are becoming a larger part of KLMs customers. European airlines are by law obliged to offer every passenger the chance to fly, European airports are by law obliged to offer PRMs a hassle free journey, which means in the case of PRMs they offer an assistance service. There are many different kind of mobility issues, and therefore it is impossible to offer one kind of service. The complexity of the PRM’s needs, the fact that offering this service is mandatory and a forced cooperation between KLM and airports are causing problems for KLM and very bad travel experiences for PRMs. Wheelchairs get forgotten or break down, elderly passengers get lost on the airport and some PRMs are faced with humiliating situations because their handicaps aren’t taken into account appropriate. Using service design tools like Customer Journey Mapping and Service Blueprinting will create overview of the process and customer experience. Based on this overview there have been identified three design scopes which focus on different parts of the PRM journey. The focus of the design phase lies on the pre-travel phase. The final proposal contains several improvements to the booking flow and the introduction of two new services. These concepts together were presented as the improved pre-travel phase to different PRM’s in order to validate them. This thesis concludes with an overview of potential improvements for the coming years, and an evaluation of the designed concepts.","KLM; Service Design; Passengers with Reduced Mobility; Aviation","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","Master of Science Strategic Product Design","",""
"uuid:82386dcf-cf46-4c0a-b9d8-a3cb2423ba29","http://resolver.tudelft.nl/uuid:82386dcf-cf46-4c0a-b9d8-a3cb2423ba29","Theory and Practice of Waveforms for Compressive-Sensing Radar","Gourova, R.N.","Pribic, R. (mentor); Yarovoy, A. (mentor)","2015","The main problem discussed in this Thesis is waveform design for compressive sensing radar. Radar systems are used for a variety of different applications ranging from medicine to underground detection. The choice of waveforms determines the capabilities of the radar to distinguish between different closely spaced targets and frequencies. If chosen correctly it can improve the detection performance of the system. In the past ten years a new paradigm has emerged, called compressive sensing. It has attracted researchers from numerous fields as it promises to improve data collection as currently known. Radar is one of the fields interested in utilizing compressive sensing for improving, among others, resolution and detection performance. Waveform design plays an important role again due to the particular requirements for applying compressive sensing. In this work three different waveforms -LFM, Alltop and OFDM, are chosen and studied in the context of suitability for compressive sensing radar. Different performance measures, such as processing gain and point spread function gain, are investigated and various scenarios testing numerous capabilities of the waveforms are defined. The experiments are designed to test the waveforms in presence of a single target, to exploit resolution capabilities in presence of extended and point target, and not last to check their Doppler performance. The outcome of this Thesis is studying the waveforms in a realistic setting and presenting the results for both traditional and sparse signal processing.","","en","master thesis","","","","","","","","2065-09-23","Electrical Engineering, Mathematics and Computer Science","Microwave Sensing, Signals and Systems","","Electrical Engineering, track Telecommunications and Sensing Systems","",""
"uuid:031d9ee5-aaff-4d24-baf4-cbf2c343fdd5","http://resolver.tudelft.nl/uuid:031d9ee5-aaff-4d24-baf4-cbf2c343fdd5","Developing a workflow for a study of polymer flooding in heterogeneous reservoirs","Boekhout, S.G.","Van Kruijsdijk, C.P.J.W. (mentor); Glasbergen, G. (mentor); Elewaut, K. (mentor); Zitha, P.L.J. (mentor); Bruining, J. (mentor)","2015","Polymer flooding can significantly improve the sweep efficiency in heterogeneous reservoirs compared to a water flood. This is caused by the improved mobility of the polymer flood, which has an effect on the crossflow mechanisms caused by viscous, capillary and gravitational forces in the reservoir. However it is not yet fully understood how the different physical and geological parameters which play a role in polymer flooding in heterogeneous reservoirs, interact with each other. Therefore a workflow has been developed to perform a sensitivity study on the influence of different parameters on the performance of a polymer flood. This workflow incorporates the ability to optimize well control parameters based on the project Net Present Value. The workflow is applied to perform a sensitivity study on two different types of models in order to study different types of heterogeneity: 2D layered reservoir models and 3D simplified turbidite reservoir models. The parameters of the sensitivity study are based on a literature review, from which research gaps are defined. The 2D study is performed with 1300 different models with varying physical parameters and architectures with the same permeability distribution. The following results have been found: Firstly, the effect of different oil viscosities is influenced by the optimized flowrate. As a result the pressure constraints have a larger effect on the models with a high oil viscosity than on models with a low oil viscosity. Consequently the models with a high oil viscosity have less benefit from a polymer flood. Secondly, the capillary pressure has a significant effect on the performance of a polymer flood compared to a water flood, despite of assumptions made in literature that capillary pressure may be ignored. The most important influencing parameter for this 2D study is the geology of the model. This study shows that heterogeneity factors such as the Dykstra-Parsons factor are not sufficient to capture the effect of geology on the performance of both a polymer and a water flood. Hence a new heterogeneity factor has been developed, which captures the trends visible from this study fairly well: the Sequence Factor. This factor incorporates differences in layer ordering, layer thicknesses and vertical permeability ratios. The 3D study is based on 9 different realizations of a simplified channelized turbidite reservoir model, consisting of channels and surrounding levees with different properties. The realizations are obtained by varying geometrical input parameters. Two different optimization runs have been performed on one of these models: an optimization which includes optimizing the flowrate and an optimization in which the flowrate is not optimized. Instead the flowrate is fixed at a maximum of 1.5 pore volumes of water injection over 10 years. The other 8 models are simulated with the optimized input parameters of the latter optimization. The results of this 3D study show that when the flowrate is optimized, the polymer flood is affected by the pressure constraints. Hence the polymer flood has almost no benefit compared to the water flood. When the flowrate of the simulation is fixed, the benefit of a polymer flood is significant. For that model the pressure constraint is not impacting the polymer flood, which leads to the flowrate staying constant instead of decreasing. Simulation of the other 8 models show that 1) well placement in a non-optimal location with regards to connectivity to the reservoir may lead to injectivity issues during the polymer flood, and 2) that the connectivity in the geological model itself may have an influence on the performance of the polymer flood. In order to find a proper relation between reservoir connectivity and the performance of a polymer flood, more data are required.","polymer flooding; EOR; reservoir engineering; heterogeneous reservoirs; sensitivity study","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering","",""
"uuid:b6e70cef-fabc-4ebf-983d-c491c69c768a","http://resolver.tudelft.nl/uuid:b6e70cef-fabc-4ebf-983d-c491c69c768a","Influence of Thermodynamic Property Perturbations on Nozzle Design and Non-Ideal Compressible Flow Phenomena","Iyer, S.V.","Pini, M. (mentor); Head, A.J. (mentor); Vitale, S. (mentor)","2015","In recent years, the scientific community has shown increased interest in the use of uncertainty quantification techniques for complex thermodynamic systems. One such complex system is the Organic Rankine Cycle (ORC) power system for which a turbine is often the prime mover. These systems operate in a region close to the critical conditions of the fluid which makes their thermodynamic behaviour complex. Additionally, the organic fluids are commonly characterized by low speed of sound which induces compressibility effects such as shock waves as the fluids expand through a turbine. In order to design optimum turbine blades and to accurately estimate the performance of these machines, it is imperative to gain a better understanding of these compressible flow effects. The flow through a de-Laval nozzle is a simple representation of flow through a turbine. Thus the objective of this research is to gain an improved understanding of the influence of uncertainty in the thermodynamic properties on nozzle design and compressible flow phenomena. The study is realized by performing an uncertainty quantification analysis using DAKOTA which is coupled with Matlab. Quite often a model such as a wedge can be placed in the nozzle to induce the required fluid dynamic phenomena. In this study, these phenomena are characterized in terms of the angle and intensity of the shock waves generated. An Euler shock wave simulator code is used to determine the shock wave angle and shock wave intensity. Two set of simulations are performed depending on the position of the wedge in the de-Laval nozzle. In the first set of simulations, the wedge is placed at the exit of the nozzle in a region which predominantly shows ideal gas behaviour. In the second set of simulations, the wedge is placed at 7.8 mm from the nozzle throat to capture the influence of real gas effects on the computed flow dynamic quantities. In addition, the deviation in the nozzle profile due to the uncertainties in the input parameters is also estimated. The two sets are further subdivided into two cases depending on the choice of the uncertain input variables. In the first case the fluid dependant parameters and the geometric parameters are considered to be uncertain. The fluid dependant parameters are the critical temperature, critical pressure, acentric factor, $\kappa_1$ parameter used in the iPRSV equation of state and the four coefficients of the ideal gas isobaric heat capacity of MM (hexamethyldisiloxane) while the geometric parameter considered is the wedge angle of the geometry. In the second case the total pressure and total temperature which are the operating conditions are also uncertain in addition to the fluid dependant and geometric parameters. The results of the two set of simulations are presented in terms of deviation in the shock wave properties from the nominal. For both the cases with the wedge placed at the exit of the nozzle, the shock wave angle and the intensity is found to vary by 0.17\% and 0.06\% respectively. With the wedge near the throat, for the first case the deviations are computed as 2.2\% and 2.7\% in the angle and intensity of the shock waves respectively while for the second case the properties of the shock wave vary by 2.6\% and 3.1\% respectively. In addition the average deviation in the nozzle profile for the both the cases is found to be 0.29 mm. These results indicate that in the real gas region, the effect of the uncertainty in the input parameters is amplified which causes a large deviation in the compressibility effects. The deviation in the shock wave angle is verified by performing CFD simulations in SU2 for the cases that yield maximum deviation. The results are found to be in good agreement with an average deviation of 0.33\% with the wedge at the exit and 1.2\% for the wedge at 7.8 mm from the throat. Finally the analysis performed on the nozzle is extended to a radial outflow turbine. Nine cases are simulated by varying the critical temperature and pressure of MM to estimate the deviation in the static enthalpy loss. The simulation results indicate a large deviation in the loss which denotes that the critical point values have a significant effect on the losses in an ORC turbine. The results obtained from this study thus provide a deeper insight into the thermodynamic properties that influence the behaviour of shock waves and nozzle design.","uncertainty quantification; non-ideal compressible flow; nozzle design","en","master thesis","","","","","","","","2020-09-23","Mechanical, Maritime and Materials Engineering","Process and Energy","","Energy Technology","",""
"uuid:566654b7-0215-496b-be4e-628285244184","http://resolver.tudelft.nl/uuid:566654b7-0215-496b-be4e-628285244184","Pitch-Crafting Guide for Entrepreneurs","Pavlic, V.","Smulders, F.E.H.M. (mentor); Van Oorschot, R. (mentor)","2015","Startups pitch their business to different audiences, for different reasons. In high-stakes situations, it is a well-crafted pitch that decides if they are going to win the competition, get funded or hire the best talent. Pitch is a persuasive attempt to convince, promote or sell, often in a high-pressure manner. High stakes pitches are often supported by slides and may take anywhere from 1 to 20 minutes. The opportunity was observed from personal experience with entrepreneurship: many startups have boring, predictable and uninspiring pitches which do not make their audience act in a desired way. The opportunity seemed like a great challenge: How might I help entrepreneurs craft more effective, more persuasive pitch for high-stakes situations. The problem was tackled using a combination of design thinking and lean startup approach. First, pitching in a startup context was explored by interviewing entrepreneurs and filming their pitches. Video recordings were studied and common practices identified. Extensive literature research helped form an understanding about how human brain works and how psychological principles of influence and persuasion affect our decision making. A lot of knowledge about pitching was gathered from Aristotle, famous pitching coaches and investors. Research was captured in the onion model of the pitching process, developed in this project. The first minimum viable product (MVP) was developed based on insights from the research. MVP consists of three parts: the pitch-crafting process, facilitator/consultant (me) and an existing virtual environment called RealtimeBoard. The most emphasis was put on developing a pitch-crafting process, which consists of 5 stages, most of which are divergent-convergent thinking processes. First, in order for me as facilitator to get familiar with the startup, they fill in some canvases about their business (such as business model canvas). Then, they capture findings about the context and the audience. In the third stage, a lot of ideas about what the audience might need to hear are generated and then the most appropriate ones are selected. The fourth stage is about coming up with multiple ideas about how particular key-points can be communicated. The best pieces are selected and combined in a pitch storyline. In the last stage, supportive (visual) content is created and narrative is drafted. The final outcome of the process is a complete pitch shown on a timeline. The minimum viable product of the guide was tested in real use case with a startup called Pearltect, who competed for the main prize worth 50.000€ in the finals of the Philips Innovation Award 2015. They did not only win the competition but also got the audience award. The project serves as a basis for my own company.","startup; startups; start-up; start-ups; MVP; pitch; pitching; present; presenting; presentation; minimum viable product; pearltect; philips innovation award; phia; phia 2015; design thinking; lean startup; entrepreneurship; business; product development; guide; pitch-crafting; entrepreneurship annotation","en","master thesis","","","","","","","Campus only","2017-09-23","Industrial Design Engineering","Product Innovation Management","","Master of Science Integrated Product Design","",""
"uuid:5a018a84-0cd1-4da7-8c45-fe6db6ce84de","http://resolver.tudelft.nl/uuid:5a018a84-0cd1-4da7-8c45-fe6db6ce84de","Loop, a vision on consumption: Stimulating sustainable consumption behavior through design","Serrarens, J.H.","Van Dijk, M. (mentor); Tromp, N. (mentor)","2015","The stimulation of sustainable consumption behavior through design. “How can we develop a service-design that frames sustainable consumption as a personally valued asset, rather than a collective one?” Central to this research through design project was the exploration of new ways of sustainable consumption.This reasearch through design project resulted in both a product service combination, and a social mechanism defining how the desired sustainable consumption in this service the Loop is stimulated.","sustainability; behavior; social design; transformation of concerns; consumption","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Master of Science Design for Interaction","",""
"uuid:1291f359-7201-4694-b332-e76ef97616b1","http://resolver.tudelft.nl/uuid:1291f359-7201-4694-b332-e76ef97616b1","Building-with-Nature Solutions for Hurricane Flood Risk Reduction in Galveston Bay, Texas: Conceptual Design Study","De Boer, R.","Jonkman, S.N. (mentor); De Vries, S. (mentor); Labeur, R.J. (mentor); Van Ledden, M. (mentor)","2015","The enormous damage caused by hurricane Ike in the year 2008 has been the main incentive for several structural flood risk reduction plans in the Houston-Galveston Bay area. One of them is the ’Ike Dike’, a large-scale coastal barrier which closes Galveston Bay to prevent penetration of hurricane surges. A momentum gain for flood risk reduction plans enables opportunities for nature-based or Building-with-Nature (BwN) solutions in the area. However, no structured approach with respect to BwN for flood risk reduction has been followed for Galveston Bay yet. BwN focuses on working with processes of nature, promoting and using habitats and increasing the contribution to ecosystem services and other potential benefits. In order to apply this principle to flood risk reduction, more understanding of the hydrodynamic effect of BwN measures under hurricane forcing is required. The main topic of this research is to explore the opportunities for BwN solutions for flood risk reduction in the Galveston Bay area. This study focuses on two aspects: flood risk and natural value. Thereby, the goal is to create solutions which add value to both aspects. Concerning flood risk, surge and waves are considered key processes because they contribute greatly to damage during hurricane conditions. In this study a conceptual design is made and evaluated by means of a qualitative evaluation and a quantitative evaluation. The qualitative evaluation involves two toolboxes. The quantitative evaluation involves a 2DH hydrodynamic model. The Galveston Bay system is a large estuary, which is prone to hurricanes and vulnerable to flooding. Important variables that affect surge and waves inside Galveston Bay are fetch, depth, landfall location and hurricane surge at the open coast. Fetch and depth can be influenced by BwN measures. Galveston Bay is a productive ecological system and its most important habitats are wetlands (e.g. salt marshes) and oyster reefs. However, significant erosion of shorelines and wetlands in Galveston Bay as a result of relative sea level rise and insufficient sediment supply has been observed. A BwN solution for flood risk reduction in Galveston Bay can consist of several elements or ’building blocks’: nourishments, wetlands, oyster reefs and an eco-island. Oyster reefs are considered as three-dimensional structures. Moreover, an eco-island is defined as a large island with ecological development and e.g. recreational functions. These building blocks have been identified because they have the potential to reduce flood risk as well as provide natural benefits to the system. In order to qualitatively evaluate these building blocks for Galveston Bay, a framework has been developed. This framework involves two toolboxes and the formulation of a global, conceptual BwN design. The toolboxes emphasize on flood risk and natural value of the building blocks. Simplified one-dimensional calculations, where possible, and qualitative literature are used to formulate the toolboxes. The potential effect of a building block is illustrated by a color classification and a description. The effect on flood risk is assessed for surge and waves. The effect on natural value is assessed for five relevant criteria. The framework has been proved to work well for evaluation of building blocks for Galveston Bay. On the one hand, with respect to surge, the toolbox shows potential for a reduction of several decimeters to a meter in Galveston Bay for emerged nourishments which compartmentalize the Bay by limiting fetch and therefore wind set-up. The eco-island, however, is significantly less effective in surge reduction as water can easily flow around it. Wetlands and oyster reefs are considered not effective for surge reduction in Galveston Bay at all. On the other hand, with respect to waves, wetlands are promising for attenuating waves nearshore. However, wide stretches are required and the quantification of their effect is difficult due to various parameters like vegetation type, stem density and stemstiffness. At the shore, oyster reefs are effective in attenuating waves and reducing erosion as well, although the effect in hurricane conditions requires more investigation. Concluding, the evaluation of the toolboxes shows that most promising flood risk reduction measures are least beneficial to the natural value in Galveston Bay and vice versa. The most effective design according to this qualitative evaluation method is a continuous emerged island that reduces peak water levels close to the western and northwestern shore of the Bay. This can be combined with oyster reefs along the western shore until Texas City and wetlands at the foreshore of the Texas City Levee for wave attenuation and erosion reduction. The qualitative evaluation showed promising solutions for surge reduction. However, quantification has been difficult. Therefore a hydrodynamic 2DH model has been created that incorporates the complex dynamics of a hurricane, two-dimensional flow mechanisms and the bathymetry of the Bay. This model is used to evaluate the system behavior and surge reduction measures for different hurricanes, with or without Ike Dike. Three hurricanes have been applied: a ’regular’ Ike as reference hurricane, and two shifted Ike tracks in which landfall occurs to the southwest of Galveston. These two shifted hurricanes cause higher storm surge levels in Galveston Bay due to higher surge levels at the open coast and stronger onshore winds in Galveston Bay. The model has been calibrated with measurements of peak water levels during Ike. Although this model is a coarse resolution model with limited simulation time, its performance is acceptable. Simulations show that peak surge levels of 3.5 - 7 m are expected in the Bay for different storms in the open Bay situation. However, in the case of a closed Bay (Ike Dike) peak surge levels are limited to 1.5 - 2.5m. Surge measures are evaluated for three different areas of interest: West, Northwest or Houston Shipping Channel (HSC) and Northeast Bay. Outcomes of the model show that peak surge level can be reduced by emerged islands which are as continuous as possible. For an island to be emerged even in the worst simulated hurricane conditions an elevation of 6 m above mean sea level (MSL) is required. In that case, peak surge level reductions with a minimumof 0.5mand amaximumof 1mare expected with emerged islands in theWest and the Northwest (HSC) for different storm tracks in the open Bay situation. The measures in the case with an Ike Dike are generally less effective in reducing peak surge levels. A final, optimized design for surge reduction is defined as the combination of a continuous island forWest and Northwest (HSC), the two most economically valuable areas. This design is presented in Figure 1, along with results for peak water level reduction without Ike Dike. The conclusion can be drawn that various limitations are found with respect to BwN solutions for flood risk reduction in Galveston Bay. Bwn solutions can significantly improve natural conditions in the Bay. However, they cannot eliminate flood risk inGalveston Bay, because a large emerged structurewith a few open passages would reduce peak surge levels at thewestern and northwestern shore by a maximum of 1.5 m. The presented BwN design is a significant intervention in the system and might have limited benefits for natural value. Furthermore, flood risk reduction of BwN measures highly depends on whether an Ike Dike will be constructed and which storm is considered because the landfall location is highly influential to peak surge levels in the Bay. With respect to waves, oyster reefs and wetlands along the shore are considered to be promising measures as they clearly add value to the natural system, protect the shore and attenuate waves. However, the effect of these measures requires quantification. To reduce flood risk significantly, BwN solutions in Galveston Bay should be constructed alongside of hard flood protection structures.","Building-with-Nature; Hurricanes; Galveston Bay; BwN; Texas; Flood Risk Reduction","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","","","29.3, -94.8"
"uuid:0d54e5e6-133b-4c7b-a2b7-6be6e9640ed0","http://resolver.tudelft.nl/uuid:0d54e5e6-133b-4c7b-a2b7-6be6e9640ed0","A modified POD procedure with patterns in time for parameter estimation","Le Coz, C.","Verlaan, M. (mentor); Garcia Triana, I. (mentor); Heemink, A.W. (mentor)","2015","Proper orthogonal decomposition (POD) is a well established model order reduction technique, however its efficiency is limited for advection dominated model. In this report, we explore a modified proper orthogonal decomposition procedure with patterns in time instead of the usual patterns in space. This modified procedure is more suitable for this kind of model. Its efficiency is studied in the context of parameter estimation.","master Thesis; proper orthogonal decomposition; data assimilation; Kalman smoother; ensemble Kalman smoother","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Mathematical physics","","master programme","",""
"uuid:ac4174d1-ee21-43b1-a023-c4d99f28eebc","http://resolver.tudelft.nl/uuid:ac4174d1-ee21-43b1-a023-c4d99f28eebc","Model-predictive control of the lane configuration at signalized intersections","Anema, F.J.C.","De Schutter, B. (mentor); Van Katwijk, R.T. (mentor)","2015","The topic of this study is the control of signalized intersections, specifically the control of switching lanes, which are approach lanes of which the traffic assignment can be changed. When the demand at an intersection varies considerably, being able to change the lane configuration can reduce delay, especially in saturated conditions. Currently five intersections in the Netherlands already have a single switching lane that switches at two fixed times per day. However, using multiple switching lanes and being able to switch when needed may further reduce delay. Previous work on this topic was fairly limited in scope, did not use predictive control and did not integrate the control of traffic signals and switching lanes. In this study two model-predictive controllers for integrated traffic signal and switching lane control of a single intersection were designed. The first aims to minimize delay while the second aims to minimize the queue length. The main differences are that the first features a more elaborate prediction model and uses longer time step durations, while the second emphasizes faster measurements and computation and uses shorter time step durations. Both controllers were evaluated using PTV Vissim, a microscopic traffic simulation program. Fictional data consisting of simple signals as well as real-world data were used. In the fictional data tests, compared to the static situation, switching reduced delay for both controllers. In the real-world data tests no significant difference between static and switching was found. A state-of-the-art signal controller used for comparison performed equal or better in all cases. Considerable insight has been gained concerning the working of a dynamic lane configuration. The potential for delay reduction by using a predictive controller has been shown, but more research is needed to make the controllers suitable for handling real-world situations. The most important recommendation for future work is to improve the signal control part of both controllers.","control; intersection; traffic signals; dynamic lane configuration; model-predictive control","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","DCSC","","","",""
"uuid:87c6e16a-d02b-4cf1-a21d-ace29279ba60","http://resolver.tudelft.nl/uuid:87c6e16a-d02b-4cf1-a21d-ace29279ba60","Smart Reefer System: Modeling Energy Peaks of Reefers Connected at Terminals and thereby suggesting peak shaving solutions to reduce cost","Nafde, T.R.","Van Duin, J.H.R. (mentor); Verbraeck, A. (mentor); Oey, M. (mentor)","2015","The increase in population, high standard of living and rapid urbanization has led to an increasing demand for food across the globe. The global trade has made it possible to meet this demand by enabling transport of different food products from one part of the world to another. In this trade, refrigerated containers or reefers have played an increasingly important role due to their ability to maintain the quality of product throughout the journey. However, this operation of reefers requires constant supply of power throughout the supply chain. This results in energy consumption by reefers. When a large number of reefers are involved, this results in high amount of energy consumption at terminals. Also, the monthly throughput of reefers is not uniform due to the seasonality of food products. Thus, the growth of reefer trade, the seasonality of food trade and the special requirements of reefers has led to an increase in the peak power demand at terminals. Because extra charges are applied for the highest observed peak demand, it is beneficial to keep this demand as low as possible to reduce energy costs. To investigate the opportunities for container terminals to reduce their peak demand, an energy consumption model is developed after taking into consideration the modus operandi of a reefer, the different terminal operations, additional data requirements and some assumptions. The simulation model visualizes the energy consumption by reefers at container terminals over period of one year and one month. From this, the peak power is determined to be 14831 kW which is beyond the allowed threshold value of 14000 kW. Also, the total energy consumption and energy costs are 12,1 Million kWh and 1,09 Million respectively. From this model, the problem is analyzed and solutions are proposed to reduce peak power demand. The solutions deal with changes in the operational procedures of terminal to reduce the peak power demand. Two rules of operation are tested to analyze their impact on peak demand: 1) Intermitted distribution of power among reefer racks; 2) Restriction of peak power consumption among operating reefers. In the first operation, two cases are considered. In the first case, the power is supplied in the timeslots of 15 minutes. This reduces the peak demand to 8266 kW. In the second case, the power is supplied in 5 minutes timeslots. This leads to even further reduction in peak power demand to 2763 kW. In both the cases, the total energy consumption and thereby the energy cost are also reduced. Thus, this solution results in annual savings of up to 1 Million. However, its downside is that it leads to increase in the reefer temperature during the power off mode. This temperature increase is smaller if shorter timeslots are used. Hence, appropriate timeslots can reduce the risk of product damage in the reefers. However, in order to avoid product damage, proper precautions are required during implementation of this solution. The second operation reduces the peak power demand to 13760 kW. This results in annual savings of more than quarter Million Euros. Furthermore, it has minimal impact of food temperature due to its operation within the allowed temperature bandwidth. Hence, this solution, though less impactful, is highly reliable. Finally, the combined operations of these two solution is recommended to effectively reduce the peak power demand by reefers at terminals. This involves using the power distribution solution at less ambient temperature, during the night time and in combination with Reefer Monitoring and Control System. No restrictions are imposed on power restriction solution.","reefer containers; peak power demand; energy consumption modeling; peak shaving solutions; energy costs savings","en","master thesis","","","","","","","","2018-09-22","Technology, Policy and Management","Engineering and Policy Analysis","","Engineering and Policy Analysis","",""
"uuid:2da969ec-dd1f-4b13-83c7-7dd7dfd3501c","http://resolver.tudelft.nl/uuid:2da969ec-dd1f-4b13-83c7-7dd7dfd3501c","RE_VALUE and LINKAGE of REGENERATING URBAN VOIDS: Design and Planning strategies to counteract local socio-spatial fragmentation, in the Iquique urban system","Pantoja Navarro, M.A.","Harteveld, M.G.A.D. (mentor); Sepulveda Carmona, D.A. (mentor); Corominas, M. (mentor)","2015","At the Chilean city of Iquique, the decline of the significance of public space is attributed to the reduced availability and functions attached to it,at the same time to the ongoing desire to control the own space (market oriented development).This project aims to deal with the fragmented city, as a result of the influence performed by the presence of urban lost spaces (Voids') over the morphological structure of the city, through an Strategic integrated plan for public space management, which can recognize the dynamic and multiscale values embedded in these urban voids (lost fragments of public space) as platform to re_link and activate a meaningful cohesive public space network.","Social Urbanism; Strategic Planning; Socio-Spatial fragmentation; Urban Void Strategy; Integrating Urban Space; Participatory Development; Developing Country Planning; Chile","en","master thesis","","","","","","","","","Architecture and The Built Environment","Urbanism","","EMU European Postgraduate Masters in Urbanism","",""
"uuid:0f3f5155-88ec-4f66-bb3b-4a60fec61863","http://resolver.tudelft.nl/uuid:0f3f5155-88ec-4f66-bb3b-4a60fec61863","Diffusion of Automated Vehicles: A quantitative method to model the diffusion of automated vehicles with system dynamics","Nieuwenhuijsen, J.A.H.","Van Arem, B. (mentor); Correia, G. (mentor); Van Daalen, C. (mentor); Milakis, D. (mentor); De Vreeze, M. (mentor)","2015","This research presents a novel simulation model that shows the dynamic and complex nature of the innovation system of vehicle automation in a quantitative way. The model looks at the system of automated vehicles from a functional perspective and therefore categorizes vehicle automation into six different levels. Each level is represented by its own fleetsize, its own technology maturity and its own average purchase price and utility. These components form the core of the model. The feedback loops between the components form a dynamic behavior that influences the diffusion of automated vehicles. The model can be used with different datasets and can be enriched with new data in the future. Policymakers and the industry can use the model with their own dataset to gain insight in the speed and direction of the diffusion of automated vehicles.","vehicle automation; innovation diffusion; system dynamics","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport & Planning","","Transport, Infrastructure & Logistics","",""
"uuid:d90ce20c-a03a-4ef5-b596-ed0fe59950bc","http://resolver.tudelft.nl/uuid:d90ce20c-a03a-4ef5-b596-ed0fe59950bc","Highly Accurate Synchronization Over Ethernet","Somers, J.M.","Charbon, E. (mentor)","2015","Time and timing is everything and everywhere from catching a train to synchronizing machines in a production hall. The key issue of all communicating systems is synchronization. Today’s sensor networks are growing larger and more complex every day, creating a demand for simple and scalable networks, whereby data and clock distribution is shared by means of one single interface. High end systems like PET scanners demand a high level of synchronization up to 50 picoseconds or less. This thesis focuses on highly accurate, sub nanosecond synchronization of system clocks for large sensor networks using a single interface for both high speed data communication and clock distribution. In this work different technologies for frequency matching, phase steering and phase measurement for FPGA implementations are compared. The verification and optimization of today’s best technology is presented, including a novel Wheatstone based technique.","synchronization; TDC; DMTD; picosecond","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Microelectronics & Computer Engineering","","Embedded systems","",""
"uuid:5b21c91a-2f64-4117-9ad8-c9487e92b72b","http://resolver.tudelft.nl/uuid:5b21c91a-2f64-4117-9ad8-c9487e92b72b","Understanding the threat landscape in e-government infrastructure for business enterprises","Pushpakumar, H.","Van Eeten, M. (mentor); Pieters, W. (mentor); Hadziosmanovic, D. (mentor); Klievink, B. (mentor)","2015","Cyber threats are becoming more sophisticated and varied. The range of possible attacks that organizations face is higher than in the past. Analysis shows that the number of cyber incidents involving government agencies has increased by 35 percent between 2010 and 2013. E-government is a potential target to attacks of various kinds from a range of adversaries. The rising number of cyber threats and the increasing complexity of e-government implementations call for enhanced security of e-government infrastructures. The preparedness of organizations to future cyber-attacks depends on the awareness of the organizations about their cyber threat landscape. The threat landscape of an organization shows the range of threats that the organization faces from a security perspective. We define a threat landscape as the characteristics (attributes), the likely threat actions (methods), and objectives of the different types of threat agents who may act against the assets of an organization. In this research we focus on understanding the threat landscape of e-government infrastructure for business enterprises. Contemporary research shows a gap in understanding the threat landscape of e-government infrastructures. A systematic methodology for understanding the threat landscape of e-government infrastructures is also lacking. We argue that a threat assessment methodology can be used to understand the threat landscape of e-government infrastructure for businesses. Our analysis of the state of the art in threat assessment methodologies shows that the Threat Agent Risk Assessment (TARA) methodology developed by Intel is suitable for understanding the threat landscape of organizations. However applying the TARA methodology to e-government infrastructure for businesses is only possible by overcoming the limitations of the Threat Agent Library (TAL) and the Methods & Objectives Library (MOL) associated with it. We address the limitations of the TARA methodology by tailoring the TAL and MOL for e-government infrastructure for businesses. We use knowledge from information security literature and cyber security experts in the public sector to perform this. The outputs of the research are the tailored TAL and MOL for e-government infrastructure for businesses. We also apply the tailored TAL and MOL using TARA methodology to the Public Key Infrastructure (or Key Management System) of Digipoort PI as a practical case study. The results of the application help us in understanding the threat landscape of the PKI of Digipoort PI, and reflect and learn about the tailored TAL and MOL we designed. This research contributes to the field of Information Security by providing a tailored library of threat agents for the e-government domain. We also summarize the methods and objectives of the threat agents in the corresponding library of methods and objectives. Organizations wanting to enhance their understanding of the threat landscape of e-government infrastructure for businesses can use these libraries as a starting point for threat assessment. Furthermore, this research also provides opportunities for future research in this area as the libraries can be tailored for applying to other infrastructures in the e-government domain or new domains itself.","threat landscape; threat agent; e-government; threat assessment; cyber","en","master thesis","","","","","","","","2015-09-22","Technology, Policy and Management","ICT","","Management of Technology","",""
"uuid:bc8edb06-1c95-4a63-a09c-76d2dc27b6dd","http://resolver.tudelft.nl/uuid:bc8edb06-1c95-4a63-a09c-76d2dc27b6dd","Next generation Public Lighting Luminaire","De Groot, K.P.W.","Song, Y. (mentor); Bruens, G.N. (mentor)","2015","This project was about designing a next generation public lighting luminaire with LEDs. The product had to fulfil the human desires of society, be technical feasible, but also financially interesting in terms of being the first product of a start-up company. Combining the design for assembly with state of the art technology led to a product which has a lot competitive advantages. The product became lighter and cheaper, while the lighting output and sustainability improved.","design; lighting","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Master of Science Integrated Product Design","",""
"uuid:184e00e0-c53b-46ff-b889-83250faaa227","http://resolver.tudelft.nl/uuid:184e00e0-c53b-46ff-b889-83250faaa227","Power quality analysis for high pressure sodium lamps in low voltage networks","Mier, G.A.","Popov, M. (mentor)","2015","Unaware connection of high pressure sodium lamps to the grid, represent a relevant problem for the grid operators. They have several negative effects in the network, they can overload the circuits and affect the quality of the energy delivered to customers by the injection of harmonics. This thesis aims to answer the question: Can the high pressure sodium lamps connected to the grid be detected through measurements at a centralized location such as a substation? The hypothesis that the lamps have specific electrical characteristics which can be used for its detection, was stablished and validated. Current harmonics and power characteristics of several high pressure sodium lamps were measured. The measured values were used for simulations of the harmonic flow in a low voltage network, and to emulate aggregated measurements combining the lamps with several loads. These calculations were compared with field tests done with high pressure lamps in a low voltage network. The results prove that within certain limitations, in a combination of several loads, the high pressure sodium lamps characteristics can be enhanced in order to be detected and identified from the aggregated data.","HPS lamps; harmonics; harmonic power flow","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Intelligent Electrical Power Grids","","Electrical Sustainable Energy","",""
"uuid:d75af5a9-5f75-4af4-9ebf-bb1b0d6cbe51","http://resolver.tudelft.nl/uuid:d75af5a9-5f75-4af4-9ebf-bb1b0d6cbe51","The Future of Manufacturing in India: Identifying Sectors which would lead to Sustained Growth","Savla, M.R.","Storm, S.T.H. (mentor); van Beers, C. (mentor); Cunningham, S.W. (mentor)","2015","This study uses the product space map approach to study the future of manufacturing in India. It uses value added export data to predict structural transformation of the country's manufacturing sector.","product space; value added exports; structural transformation; India","en","master thesis","","","","","","","","","Technology, Policy and Management","Values, Technology and Innovation","","Economics of Technology and Innovation","",""
"uuid:f2f44ac0-2bd3-4e20-a530-67c39c3e4afd","http://resolver.tudelft.nl/uuid:f2f44ac0-2bd3-4e20-a530-67c39c3e4afd","Using Clickstream Data to Enhance the Demand Forecasting in the Online Retail Sector","Van 't Hart, E.R.","Verbraeck, A. (mentor)","2015","","online retail sector; clickstream data; demand; forecasting; consumer behaviour","en","master thesis","","","","","","","","2016-09-21","Technology, Policy and Management","System Engineering","","Systems Engineering, Policy Analysis and Management","",""
"uuid:052c8d6f-7ba0-4834-8b85-5eddb94b8870","http://resolver.tudelft.nl/uuid:052c8d6f-7ba0-4834-8b85-5eddb94b8870","Joint unloading ankle brace to aid cartilage regeneration","Natenstedt, J.","Tuijthof, G.J.M. (mentor); Dankelman, J. (mentor)","2015","Ankle injuries are one of the most common sports injuries that often lead to further complications such as cartilage defects. Recovery from these injuries can take a long time and a solution that could aid in the rehabilitation of these injuries is beneficial to the young and active patient group. The goal of this study is to design a device that can unload the patient’s ankle to such an extent that recovery is promoted. Using human motion analysis and a mathematical descriptive model, an unloading mechanism is designed that modifies the forces on the ankle joint. The resulting device consists of an attachment to the lower leg onto which a mechanism is attached that transfers a part of the load of the foot to the lower leg. The device is tested using a force plate set-up. The results are that the device can provide an unloading force throughout the stance phase of gait, reducing the maximum load on the ankle from 1.2 BW to 0.94 ± 0.04 BW. The proposed design is a wearable device that could be used during the rehabilitation of a patient’s ankle. The manner in which this device should be attached to the user’s leg needs further research; when these limitations are solved further testing can be initiated.","Ankle brace; Joint unloading; cartilage; repair; rehabilitation","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","BioMechanical Engineering","","Medical instruments and Medical saftey (Biomedical engineering)","",""
"uuid:e7e16208-8b9a-457e-9bdc-87e65e4c9135","http://resolver.tudelft.nl/uuid:e7e16208-8b9a-457e-9bdc-87e65e4c9135","Horizontal Tailplane-Tip Mounted Tractor Propeller Interaction Effects","Candade, A.A.","Sinnige, T. (mentor); Veldhuis, L.L.M. (mentor)","2015","Advanced propeller propulsion systems potentially provide a significant reduction in fuel burn compared to traditional turbofans. An alternative to the conventional aft-fuselage mounted pusher layout is the horizontal tailplane-tip mounted tractor propeller concept. The aim of this thesis is an experimental investigation of the aeroacoustic and aerodynamic interaction effects of the tailplane-tip mounted tractor propeller configuration, including the effects of elevator deflections. The experimental study was conducted at TU Delft’s Low Speed Laboratory in the Vertical Tunnel and the Low Turbulence Tunnel with two different models. From the aeroacoustic study, it was concluded that the installation of the pylon behind the propeller affects both the directivity and the tonal levels of the propeller noise field, with the broadband acoustic levels remaining unchanged. It was determined that the overall sound pressure level (SPL) across the range of directivity angles considered is inversely proportional to the propeller-pylon spacing. For a spacing of 50% propeller diameter, the overall SPL was comparable to the case of the isolated propeller. A unique characteristic of installation of the pylon was the development of a trough in the directivity for an observer position in the pylon plane caused by the cancelling of the steady noise field by unsteady blade loading noise. This arises due to inflow distortion due to potential effects caused by the pylon. This unsteady blade loading is a function of the propeller-pylon spacing, and hence the levels in the trough decrease with decreasing propeller-pylon spacing. For directivity angels in the pylon plane, for spacing below 30% of the propeller diameter, the unsteady loading is further influenced by the elevator deflection and was the main mechanism of the interaction noise. For propeller-pylon spacing’s above 30% of the propeller diameter, the interaction of the slipstream (either the propeller noise field, or the slipstream impingement) with the elevator was determined to be the main interaction mechanism. From the PIV and performance evaluations, it was concluded that for the given propeller-pylon spacing (43% and 85% propeller diameter), there was indeed negligible upstream interaction effect due to the trailing pylon, including the case of the deflected elevator Pylon loads obtained from an external balance showed that for symmetric inflow conditions, operation of the thrusting propeller increased elevator effectiveness by 20% compared to the case with no propeller present. A numerical simulation using XROTOR was used for the validation of the test data and had a relative error of 3% with the experimentally evaluated propeller thrust for the lowest advance ratio. A slipstream propagation model based on the computed propeller induced velocities showed acceptable trends when compared to the experimentally determined induced propeller velocity profiles. However, the numerical model overpredicts the velocity profile in the tip region, owing to the tool’s limitation in predicting stall at the blade tip. A VLM based numerical analysis which included the effects of the propeller slipstream, was able to predict the pylon lift to within 3% of the lift computed from the surface pressure measurements, but failed in the prediction of the drag of the model.","propeller; propulsion integration; vortex interactions; aeroacoustics","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy, Flight Performance and Propulsion","","Flight Performance and Propulsion","",""
"uuid:c06b142d-bb3a-4e47-bd14-d42387cb17d9","http://resolver.tudelft.nl/uuid:c06b142d-bb3a-4e47-bd14-d42387cb17d9","Simultaneous injection of water above gas for improved sweep in Gas Enhanced Oil Recovery (EOR): An analytical and simulation study on non-uniform injection and sweep","Ranjan, R.","Rossen, W.R. (mentor)","2015","Gas EOR processes can have microscopic displacement efficiency as large as 100%, but suffer from poor sweep of the reservoir. Injected gas has tendency to segregate at the top of the reservoir due to density difference and gravity. Water-alternating-gas injection was proposed by Caudle and Dyes (1958) to reduce the effect of adverse mobility ratio. Stone (2004) proposed a new injection scheme (sometimes called “modified SWAG”) in which water and gas are injected simultaneously from parallel horizontal wells, with gas being injected from the bottom of the reservoir and water is injected from top from a site directly above the gas well. The water injected from the top impedes the vertical flow of gas, allowing it to move horizontally before the gas segregation happens. This gives a deeper penetration of gas before gravity segregation than simultaneous co-injection water and gas from the same well (“SWAG”). A recent study by Jamshidnezhad et al. (2010) examined the performance of the “modified SWAG” process in 3D and an MSc thesis (Mahalle, 2013) followed up on this work. The first study found that the gas injection was non-uniform in nature, even in homogeneous reservoirs. The more-recent MSc thesis (Mahalle, 2013) appears to show that the non-uniform injection behavior may have been a result of poor grid refinement near the injection well. It was suggested to further investigate the effect of grid-block size on non-uniform nature of gas injection. The MSc thesis also shows that if the instability develops, it depends on complex interactions among the grid blocks along the well: the injection rate of one segment of the well depends on that in its neighbors, and the instability grows along the well from one end to the other. This project extends the earlier studies, first, by examining the effects of grid refinement near the well. We conclude that grid refinement around the injection well doesn’t seem to stop non-uniform injection and the non-uniformity of gas injection is not a simulation artifact of using grid blocks that are too large. Increasing gas-injection rate correlates with increasing non-uniformity of gas injection. Decreasing gas saturation exponent (ng) in Corey’s 2-phase model and decreasing gas viscosity (?g) leads to more-uniform injection. When there is no connection between neighboring grid blocks along the gas injection well, the gas injection behavior is more uniform. We further examine the effect of gas flow on hydrostatic pressure and conclude that hydrostatic pressure is involved in the non-uniform behavior of gas injection. The results from this study indicate that the non-uniformity in gas injection is a result of coupling of various factors, such as, gas saturation, gas relative permeability, gas injectivity, effect of gas flow on hydrostatic pressure, and effect of adjacent grid blocks. When coupled together they form the self-reinforcing cycle leading to non-uniform behavior of gas injection, with most of the gas being issued from one end of the well. Segmenting the well into multiple segments helps ensure that gas issues from most of the perforations along the gas-injection wells and thus ensures better sweep.","SWAG; modified SWAG; simultaneous Injection of water above gas","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering","",""
"uuid:cae7d0de-9d2d-483b-ac2f-92caa2e8965b","http://resolver.tudelft.nl/uuid:cae7d0de-9d2d-483b-ac2f-92caa2e8965b","Room Shape Estimation from Acoustic Echoes using Graph-based Echo Labeling","Jager, I.","Heusdens, R. (mentor)","2015","Some vision impaired people can hear the shape of a room using acoustic echoes. A computer being able to do the same could benefit applications such as auralization, virtual reality and teleconferencing. This thesis describes the process of estimating the room shape from acoustic room impulse responses to finding geometry from image sources. As we use multiple microphones, the biggest challenge is to find the echoes from different microphones that correspond to the same image source. We present a new method to disambiguate the echoes using graph theory. We model combinations of echoes as nodes in a graph. The maximum independent set in the graph yields the disambiguated echoes. The disambiguated echoes are transformed to time-difference-of-arrival data so that we are able to calculate the locations of the sources and image sources in a closed-form fashion. From the estimated sources and image sources we finally infer the room geometry using the image source model. The experiments, which are limited to simulated shoe box shaped rooms show that we can reliably estimate room shapes within seconds on contemporary hardware. We achieve a sub-centimeter precision on finding the vertices of the room.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Signals & Systems","","Acoustic Signal Processing","",""
"uuid:3a443f31-a20f-4cd6-a616-0cbd25da47c6","http://resolver.tudelft.nl/uuid:3a443f31-a20f-4cd6-a616-0cbd25da47c6","REV’ENTUROUS! A strategy for an inspiring online platform","Wijnveldt, D.","Mugge, R. (mentor); Bakker-Wu, S. (mentor); Aarts, G. (mentor)","2015","REV’IT!, a Dutch company that creates motorcycle apparel. The challenge REV’IT! is facing is how to effectively communicate with their end users and enhance their brand equity. The preference of REV’IT! lies in an online solution that builts upon their value: Inspire to ride. Therefore, the assignment was formulated as follows: How can REV’IT! inspire people to ride - while building upon their brand values and qualities - via an online platform. Concluded from the analysis it became apparent that the touring and adventurous part of the community has the most to offer in terms of participation and the willingness to be part of a community. At the same time, travel experiences and stories really inspire people to go ride themselves. Hence, incorporating travel with the Dynamic platform will create a starting point for riders who want to get inspired or is looking for routes and locations to visit. From this thought, the platform REV’ENTUROUS! emerged. REV’ENTUROUS! is designed for motorcyclists who love to explore the world. Every trip is an adventure and worth sharing. Get inspired by stories of other riders, browse through routes from all over the world and plan your own trip to start your own story. The prototype can be seen here: https://marvelapp.com/b1c349. Aside from the stories that are uploaded by the riders, there will also be articles about riding and adventure on a motorcycle. These will be made by both REV’IT! staff and knowledgeable community members with experience. The aim of these articles is to educate people on travelling with a motorcycle. Topics of the articles will variate; riding techniques, gear, areas or countries to ride in and articles with tips for riders or planning trips.","strategy; platform; online; inspiring; brand; experience; REV'IT!; motorcycle","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","Strategic Product Design","",""
"uuid:ba0f0e18-c236-4b37-998e-2445432d05c6","http://resolver.tudelft.nl/uuid:ba0f0e18-c236-4b37-998e-2445432d05c6","Redesign of Bright Stamps for Chinese Market","Fu, L.","Schoormans, J.P.L. (mentor); Yoon, J. (mentor)","2015","The project was commissioned by IceMobile to invest the potential opportunities of adapting digital stamps-based loyalty programme in China. Stamps-based loyalty programme is one of the basic and mostly used loyalty programme types. With the popularity of smartphones, digital stamps programme now becomes a new trend in loyalty marketing industry. The research draws attention to the characteristics of Chinese consumers’ shopping behavior and shopping experience in supermarkets, and the components for a loyalty relationship between customers and retailers. A model of building loyalty relationship between Chinese consumer and retailer was presented. The concept design of this project aims at elicit a sense of belongingness and engagement by making the experience social and vibrant. The final concept Easy sharing & Early Birds Discount intended to stimulate social interaction and improve users’ awareness of the programme. It enables customers to share digital stamps through QR code scanning and offers dynamic discount for customers by which they can redeem earlier with fewer stamps in limited time periods.","loyalty programme; Chinese supermarket; brand loyalty; digital stamps-based loyalty programme","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","Master of Science Design for Interaction","",""
"uuid:4ea857e9-1539-4939-a6fb-f7fdabcd3060","http://resolver.tudelft.nl/uuid:4ea857e9-1539-4939-a6fb-f7fdabcd3060","Analysis of Pile-Concrete Connections in Near-Shore Applications","Belfroid, S.","Bijlaard, F.S.K. (mentor); Abpsoel, R. (mentor); Braam, C.R. (mentor); Hendriks, M.A.N. (mentor); van der Woerdt, F. (mentor)","2015","Tubular steel sections are used as foundation piles in near-shore applications like jetties or bridgeworks. The steel tubes are connected to a concrete capping beam or pile cap which transfers the forces from the structure to the piles. Multiple options for this connection type have been developed over the years with varying specifications. Constructability, costs and construction time are of high importance when adopting an alternative. In this report, the alternatives are listed and a trade-off is made. Furthermore, a more detailed investigation is performed regarding the chosen alternative.","pipe pile; bond; shear keys; capping beam; pile cap; plug connection; concrete plug","en","master thesis","","","","","","","","2015-09-18","Civil Engineering and Geosciences","Structural Engineering","","","",""
"uuid:52e891f7-4588-4c18-bad4-0e4988d92f7c","http://resolver.tudelft.nl/uuid:52e891f7-4588-4c18-bad4-0e4988d92f7c","Opening and closure of intermittently open natural inlets","Rijkenberg, L.","Stive, M.J.F. (mentor); Nielsen, P. (mentor); Reniers, A.J.H.M. (mentor); Van Rooijen, A.A. (mentor); McCall, R.T. (mentor)","2015","Tidal inlet systems are found all over the world. These highly dynamic coastal features can be classified into three main types wave-dominated, tide-dominated or river-dominated tidal inlet systems. A tidal inlet is described as an opening in the shoreline that provides a connection between the ocean and a bay and is maintained by tidal currents. Tidal inlet systems are the whole area of the near shore ocean, inlet and bay with river catchment area. Recently, Thuy (2013) developed a dimensionless hydraulic parameter tool to classify tidal inlet systems. The study focusses on wave-dominated tidal inlet systems and specifically the sub-class intermittently open natural inlets. The question arises if the intermittently open natural inlets can be distinct by use of the parameter. This type of sub-class has an alternately open or closed connection to the ocean, which is indicated by the word intermittently. A natural inlet is defined as an inlet with no other human interventions at the inlet entrance other than artificial breaching of the entrance berm when the lagoon water level exceeds a critical value. The distinction for the sub-class can be integrated into the wave-dominated classification parameter, therefore the overall objective is to make a contribution to the wave-dominated classification parameter of Thuy (2013) to make a more accurate classification for the sub-class intermittently open natural inlets. The overall objective is too extensive to be answered completely by only this study, therefore a specific objective is formulated. The specific objective is to predict the lagoon water level of intermittently open natural inlets forced by waves, tide and river discharge. The prediction of the lagoon water level will indicate the important forcing working on the inlet, while the entrance is in an open state. In total nine opening/closure events are found in the data and analysed. Data included, offshore wave climate, tidal water levels, lagoon water level and rainfall. To get a more accurate representation of the wave climate near the research sites, the offshore wave climate is adjusted to get the near shore wave climate. Based on the analysis the following main conclusion is found. The lagoon water level is a crucial parameter for prediction of the entrance state open or closed. Therefore, it is important to understand in which way the high waves will affect the elevation of the lagoon water level. Two analytical models are introduces to understand the forces that work on the open inlet. A basic energy equation, after Nielsen (2009), is the starting point of both models. The basic energy equation describes the river discharge and flood tide as inflow parameters, but the effect of waves is not taken into account. The first introduced model is the extended energy equation. The data analysis indicated that shore-normal waves are dominant during the elevation of the lagoon water level. The shore-normal wave forcing is introduced by means of a wave overwash discharge. The outcome of this model gives an underestimation of the lagoon water level, although the model performs better than the basic energy equation. The second introduced model is an alternative approach of the basic energy equation. The data analysis indicates that the near shore wave heights are very important for the elevation of the lagoon water level. Accordingly the elevation of the lagoon water level is assumed to be a function of the near shore wave height. The alternative model gives a better performance of the lagoon water level than the basic and the extended model. Based on this study it is recommended to further develop the alternative model for further research of different tidal inlet systems. A meaningful addition to the model could be the introduction of an entrance width that varies in time. During closure the inlet entrance gets narrower, which will result in different forcing and a different response of the lagoon water level.","intermittently; tidal inlets; opening; closure; overwash; wave-dominated; New South Wales; Australia; natural inlet behaviour","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","","","-33.463999, 151.434996"
"uuid:3134a851-2b40-4af1-896a-41a6ce7a5816","http://resolver.tudelft.nl/uuid:3134a851-2b40-4af1-896a-41a6ce7a5816","Phishing as a Service: Designing an ethical way of mimicking targeted phishing attacks to train employees","Meijdam, K.C.","Pieters, W. (mentor)","2015","Organisations want to protect themselves against targeted phishing attacks, and mimicking those in order to train their employees in an important measure to do so. However, previous experiments have shown that this is ethically sensitive and might lead to feelings of deception and harm at the side of the employees. In this paper, the design of an ethical to way to implement mimicked targeted phishing attacks to train employees is described. The design process is based on Value Sensitive Design and Values at Play, and use the notion of Value Hierarchies to make the design process explicit and discussable.","Value Sensitive Design; targeted phishing; security awareness; phishing training","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering Systems and Services","","Information and Communication Technology","",""
"uuid:267beaa5-8b62-4e58-b1ad-ff94355c68f9","http://resolver.tudelft.nl/uuid:267beaa5-8b62-4e58-b1ad-ff94355c68f9","Design and Analysis of an Installed Pusher Propeller with Boundary Layer Inflow","Van Arnhem, N.","Lv, P. (mentor); Veldhuis, L.L.M. (mentor)","2015","Boundary Layer Ingestion is an integrated propulsion concept in which a propulsor operates in boundary layer flow instead of the free streamflow with the goal to reduce the fuel flow for a given operating condition. The objective of this thesis is to obtain a better understanding of the power benefit of an installed pusher propeller at the aft fuselage by designing the aerodynamic shape of the propeller and validate the design by means of CFD simulations. A propeller analysis tool for uniform inflow (UI) and non-uniform inflow (NUI) named N-XROTOR is developed using the lifting line code XROTOR in combination with XFOIL to calculate airfoil properties. The tool is validated using experimental results and results from CFD simulations of uniform inflow propellers. N-XROTOR shows good agreement of the trend of the CT-J and CP-Jcurves but a constant over prediction of both thrust and power with respect to experimental data is observed and several deviations are explained. A series of CFD simulations in ANSYS Fluent using a reduced wedge shaped domain of one blade of the N250 propeller are performed for several advance ratios including a grid refinement study. Minor deviations between a transient and a steady simulation are found and the steady method is chosen based on computational cost. The trends of N-XROTOR in terms of CT-J and CP-J compare well with the CFD simulation with a constant over prediction of the performance quantities by N-XROTOR. These over predictions are also noticeable in the radial distributions of thrust and torque with slight over predictions in the high loaded region on the blade. For a moderate advance ratio of J = 0.79 the thrust and power are over predicted by 5.25% and 3.67% respectively. A comparison with the standard kapppa-omega SST turbulence model and the SST model with low Reynolds number correction is made. The radial flow on the propeller blade is shown to be quite significant and varies along the blade and shows good agreement with the distribution of bound circulation and the resulting trailing vorticity. A design procedure is developed in which the propeller shape is optimised using shape functions to describe the pitch and chord distribution and a NACA four series airfoil is used to limit the number of design variables for a gradient based optimisation algorithm in Matlab environment. The interaction effects are assumed to be determined a-priori and a tapered aft fuselage and the pressure field induced by the fuselage are neglected. Input quantities for the design routine include an inflow field from CFD analysis, the design advance ratio and a thrust requirement. The design objective of all optimisations is minimum power. For the reference design case an axisymmetric body from ESDU is subject to CFD simulations to obtain the inflow profile and fuselage drag for the isolated and installed configuration. Interference effects are approximated using an Actuator Disk (AD) model at the predefined location of the propeller with a pressure jump equal to the defect in total pressure in the boundary layer based on findings from previous research. An 11% increase of drag is found for the equilibrium condition which is primarily due to increased pressure drag. Larger pressure jumps show only a marginal increase in drag. In a comparison study, the number of blades is set to four, an advance ratio of J = 1.50 is chosen and in combination with a radius equal to 99% of the total gage pressure of the undisturbed air yields a tip Mach number of around 0.50. The optimisation results show that the NUI propeller requires 6.93% less power compared with the UI propeller despite the 11% higher thrust. The thrust distribution of the NUI propeller shows a significant increase in thrust in the lowaxial velocity region towards the root and the maximum thrust is shifted inboard. The ratio of thrust to power dT/(dQ Omega)­ along the propeller blade shows a constant distribution for the UI propeller, while the NUI propeller has a smooth increasing distribution towards the root. This distribution shows that thrust requires a relatively low power when the local axial velocity is relatively low. It is found that this is the main benefit of positioning a propeller in the boundary layer. The bound circulation distribution shows a shift towards the root compared with the distribution of the uniform inflow propeller which is the result of the optimised propeller shape which benefits from the favourable thrust to power ratio in the inner radii. The NUI propeller has a significant increased chord compared with the optimal UI and also a higher lift coefficient distribution. The local efficiency defined as eta = dTVa/(dQ Omega) with Va as the local inflow velocity. Optimal UI propellers have a constant efficiency distribution, but the NUI propeller shows a decreasing trend towards the root which is also found in literature. The trend of lower local efficiency is also found when an optimisation for minimum power is performed using a radially varying actuator disk with the same inflow and thrust requirement as for the full blade propeller. Additional analysis on the NUI propeller include a comparison of off design conditions and additional optimisations are performed to quantify the effect of the number of blades, radius and advance ratio. The optimised NUI propeller in the installed configuration is simulated using CFD. N-XROTOR over predicts the thrust and power by 4.15% and 4.71% respectively compared with the CFD simulation, which are deviations of the same order as the N250 simulation. The thrust to power distribution shows good correspondence. In the root region this ratio is under predicted by N-XROTOR which is expected to be the result of a large pressure and velocity gradient at the junction of the spinner and propeller surface resulting in a region of recirculation. Also the blockage effect of the tapered spinner results in larger angles of attack in the root region. The outer region shows trailing edge stall which is found to be primarily due to the coarse mesh in that region. Improved results are obtained when N-XROTOR uses airfoil data obtained from two-dimensional CFD analysis of a particular airfoil section. The kappa-omega SST model with low Reynolds number correction shows almost exact agreement with XFOIL. The standard turbulence model shows a decambering effect and an earlier stall behaviour. The remaining deviations between N-XROTOR with approximated CFD airfoil properties are expected to originate from the radial flow on the blade and the variation in circulation in chordwise directions which are not simulated in N-XROTOR. Both the externally induced radial flow by the tapered aft fuselage and the self induced radial flow are expected to result in a decambering of the airfoil due to the influence on boundary layer growth as well as a reduced chordwise velocity resulting in a locally lower dynamic pressure experienced by the airfoil contour. The interference effects of the propeller onto the fuselage are compared with the Actuator Disk (AD) approximation. An over prediction of 0.74% of the drag by the AD model of the fuselage excluding spinner is observed. Downstream of the full blade simulation the pressure is rapidly decreased to a low finite value at the aft end of the spinner. This is the result of the finite bound circulation at the propeller root which releases a strong trailing vortex from each blade. These vortices combine into a strong axial vortex which induces a strong tangential velocity and therefore in a low pressure acting on the spinner. A slipstream analysis is performed of circumferentially averaged flow quantities in radial direction at a plane behind the propeller and the axial development of several averaged flow quantities is shown. Several recommendations for future work are formulated to improve the propeller design, improve the design procedure, reduce the interference effects and increase the power benefit of the non-uniform inflow propeller.","Boundary Layer Ingestion; BLI; pusher propeller; CFD; interference effects; design optimisation; integrated propulsion","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy, Flight Performance and Propulsion","","Flight Performance and Propulsion","",""
"uuid:b5a0009e-0687-4a20-9e9e-4c6ecd99561d","http://resolver.tudelft.nl/uuid:b5a0009e-0687-4a20-9e9e-4c6ecd99561d","Investigation of the coarse grained heat-affected zone microstructure and hardness of multipass welded X65 linepipe steel.","Zuidema, W.","Kestens, L.A.I. (mentor)","2015","In this thesis the effect of single and multipass welding on the microstructure and hardness of X65 linepipe steel of four different chemical compositions was investigated. The welding was simulated for three different heat inputs and two second pass peak temperatures. The thermal simulation has been achieved by simulating the CGHAZ with a Gleeble? thermal-mechanical simulator, peak temperatures used for this simulation were derived from dilatometer experiments. Significant overheating of 50 ?C above Ac1 was observed at high heating rates, comparable heating rates of welding. The peak temperature of the first weld cycle was 1350 ?C and the second cycle either 810 ?C or 860 ?C. The overall hardness of the samples was measured as was the micro hardness of M-A constituents. The microhardness of M-A constituents was found to be significantly lower than in previous research, 310 Hv versus 800 Hv. The hardness of the single pass CGHAZ was parametrized for the cooling rate and the chemical composition with an R2 value 0.98. The parametrization shows increased hardenability due to increased Ni content. The CGHAZ microstructures were examined by optical and scanning electron microscopy as well as EBSD, revealing microstructures consisting of martensite, lower bainite, upper bainite, granular bainite, acicular ferrite and M-A constituents. A number of samples were quantitatively described by use of the IQ distribution from EBSD measurements and the M-A constituents were quantified by Lepera’s etchant for all multipass welded samples. A fraction of M-A constituents was found to be higher than 1.6 %all reheated CGHAZ samples and the fraction is found to be dependent on the heat input and hardenability of the steel. It was found that Lepera’s etchant reveals a higher fraction of M-A constituents than what is observed by EBSD measurements. The M-A constituents as revealed by Lepera’s etchant were found to not completely consist martensite and austenite in the steels considered in this research. Therefore the M-A constituents are suspected to be of less importance for the fracture toughness than discussed in previous research.","X65; M-A; Multipass; Welding; Linepipe; EBSD","en","master thesis","","","","","","","","2016-09-07","Mechanical, Maritime and Materials Engineering","Materials Science & Engineering","","","",""
"uuid:ef7f1a11-aaef-4bba-be20-4689b522d3f5","http://resolver.tudelft.nl/uuid:ef7f1a11-aaef-4bba-be20-4689b522d3f5","Brand Strategy and service design for Joylent","Van Leeuwaarden, N.B.","Calabretta, G. (mentor); Roscam Abbing, E. (mentor)","2015","Joylent is a Dutch startup producing powdered food. In order to survive the fierce competition in the industry Joylent should exploit their brand, and have an innovation strategy and process. The innovation process will be infused with information from their customers via their own community in which they can easily invite customers to join in their research or create their own ideas.","Joylent; powdered food; community","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","PIM","","","",""
"uuid:0ed6d2a8-1886-4028-9702-64d518f5761d","http://resolver.tudelft.nl/uuid:0ed6d2a8-1886-4028-9702-64d518f5761d","Modelling route choice of inland shipping vessels in the port of Rotterdam","Gerritse, E.J.","Zheng, H. (mentor); Negenborn, R.R. (mentor); Lodewijks, G. (mentor)","2015","","route choice; inland shipping; multinomial logit","en","master thesis","","","","","","","","2015-09-17","Mechanical, Maritime and Materials Engineering","Transportation Engineering and Logistics","","","",""
"uuid:5948cba4-57ed-4635-9e6f-607a006285b3","http://resolver.tudelft.nl/uuid:5948cba4-57ed-4635-9e6f-607a006285b3","PDM-CAD/CAM interfacing in shipbuilding","Schiltkamp, S.H.","Coenen, J.M.G. (mentor)","2015","The aim of this thesis is to make clear what the possibilities are towards the implementation of Product Data Management (PDM) in combination with CAD/CAM software in the shipbuilding industry. Even though in many industries there is already an extensive PDM integration in combination with CAD/CAM software, the shipbuilding industry is in its initial phase of integrating PDM in its standard way of working. The main focus of this research is to Investigate the use of PDM together with CAD/CAM software and design a concept solution that meets the requirements of an optimal integration collaboration between PDM and CAD/CAM software. In order to make clear what these possibilities are, a literature study has been done. From literature the theory of PDM has been described and the UML modelling language has been chosen for a description of the PDM possibilities in shipbuilding. Before description of these possibilities a generic engineering process of shipbuilding is described to function as a basis to identify PDM possibilities. A virtual block of a not existing ship has been used to delimit this engineering process to a surveyable size. Within the described engineering process, activities are identified that not only have a connection to PDM but may also benefit from PDM. Using the described engineering process and PDM theory, the PDM functions have been described. These functions have been directed to assist the activities that are linked to PDM and also benefit from PDM. The PDM functions are not only described for their functionality but they are also placed in a system architecture to give a representation of how the PDM functions are executed when used by the described engineering process. Based on the description of the engineering process and the PDM functionality, a qualitative measurement has been performed to give a clear indication of the benefits that the PDM solution has on the described engineering process. In order to show that the described engineering process can also handle the engineering of a complete ship, the process has been expanded to describe the engineering of two existing ships. Finally the functionality of the PDM solutions has been tested for its feasibility on the NUPAS-CADMATIC hull and outfitting packages. The implementation of PDM consists of two main aspects. The main perspective is taken into consideration from a system point of view where PDM functionality is executed by a PDM system and makes re-use of engineering data possible. Moreover, the data storage needs to be redesigned to a central data vault that facilitates this re-use and other benefits. The other perspective is from a managerial point of view, where configuration management and a different decision making is implemented in the way of working on the work floor of the engineers. An important part of this research are the changes to the system regarding the NUPAS-CADMATIC hull and outfitting packages. At this moment, the proposed PDM solution is not feasible with the packages. This means that should the packages decide to adhere to the proposed solution, changes need to be made to the systems for them to have a shared central data vault and therefore benefit from proper re-use of engineering data as described by PDM.","PDM","en","master thesis","","","","","","","","2020-08-29","Mechanical, Maritime and Materials Engineering","Marine & Transport Technology","","DPO","",""
"uuid:fdef1d52-52de-435c-a875-e8461e1cd973","http://resolver.tudelft.nl/uuid:fdef1d52-52de-435c-a875-e8461e1cd973","Sedawgyi Water Resources/Irrigation system simulations","Quirijns, S.","Kerssens, P. (contributor)","2015","","Mandalay; RIBASIM; Water magement; Myanmar Internship","en","student report","","","","","","","","","Civil Engineering and Geosciences","Water Management","","","",""
"uuid:4f81d8f8-fbc9-4352-ba4e-d05031483886","http://resolver.tudelft.nl/uuid:4f81d8f8-fbc9-4352-ba4e-d05031483886","BIM’s Horizon: BIM and its Envisioned Use in Engineering Infrastructure","Ter Maaten, J.","Hertogh, M.J.C.M. (mentor); Van Nederveen, G.A. (mentor); Lukosch, S.G. (mentor)","2015","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Integral Design and Management / Construction Management and Engineering","",""
"uuid:29edf411-cf49-4ab3-a704-117d7459dd83","http://resolver.tudelft.nl/uuid:29edf411-cf49-4ab3-a704-117d7459dd83","Modelling the effect of oil on foam with the wave curve method","Heins, R.","Rossen, W.R. (mentor)","2015","Enhanced oil recovery by gas injection can show excellent displacement efficiency but may suffer from poor sweep efficiency. A promising method to overcome poor sweep efficiency in gas-injection enhanced oil recovery processes is foam injection. The effect of oil saturation on foam stability is complicated and not fully understood. Complexity in foam models give rise to numerical problems in three-phase simulations with foam. The objective of this thesis is to study foam displacement with fractional-flow theory instead of simulation. Results are expected to be unaffected by numerical problems. Fractional-flow theory is a strong analytical tool which can greatly enhance the understanding of displacement of fluids initially in the reservoir by injected fluid. In this study we use fractional-flow theory to describe foam displacement in the presence of oil in a 1-D reservoir model. Fractional-flow theory for displacement with three immiscible phases has increased complexity compared to fractional-flow theory for displacement with two phases. The wave curve method solves fractional-flow problems with three immiscible phases based on shock waves, rarefaction waves and areas of constant state. RPN, developed at the Instituto Nacional de Matemática Pura e Aplicada, develops solutions to three-phase fractional-flow problems based on the wave curve method. The software package is used to enhance understanding of foam displacement. In this study we succesfully couple STARSTM interpolation parameters for foam displacement to RPN. Unlike simulators, RPN is an analytical tool which should be insensitive to numerical artifacts. Practical results obtained from the software include a catalog of displacement cases with initial and injection conditions either inside or outside the foam region. Next, a challenging scenario of Namdar Zanganeh (2011), where simulation yields unsatisfactory results, is addressed with RPN. An approximation of this scenario showed similar numerical artifacts in simulation, RPN provides well-behaved solutions. Thirdly, an analysis is done on the formation of an oil bank ahead of the foam front with an oil saturation that could possible lead to foam collapse. This study shows that the oil saturation in the oil bank never exceeds the saturation where oil leads to foam collapse.","enhanced oil recovery; reservoir engineering; foam; modelling","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum engineering","",""
"uuid:9f313e13-4570-48f8-8aff-3eef35bbad99","http://resolver.tudelft.nl/uuid:9f313e13-4570-48f8-8aff-3eef35bbad99","Infield Cable Topology Optimization of Offshore Wind Farms","Katsouris, George (TU Delft Applied Sciences)","Zaaijer, M B (mentor); Rodrigues, S. (mentor); van Bussel, G.J.W. (graduation committee); Bauer, P. (graduation committee); Delft University of Technology (degree granting institution)","2015","As part of the effort to reduce the cost of offshore wind energy, this thesis addresses the problem of the design of the infield cable topology for offshore wind farms. The final outcome of the project is a tool that can be implemented in a more general optimization platform for offshore wind farm design. Therefore, the main objective is to approximate the optimal inter-array cable connections in affordable computation times. A review of the state of the art collection system designs indicates the radial and branched designs as the designs with the highest potential among the conceptual designs. The key target of the optimization procedure is the minimization of cable cost. Regarding the design constraints, cable capacities are respected and inter-array cable crossings are strictly avoided. The literature review of the related research reveals the complexity of the problem and pinpoints the use of heuristic methods. Planar Open Savings (POS) [1] and Esau-Williams (EW) [2] heuristics are chosen to treat the single cable radial and branched designs respectively. Both algorithms are saving heuristic methods, starting from a star design. At each iteration, the merging of two routes is considered that could yield to the maximum cost saving. After the implementation of the algorithms is validated, a methodology is proposed to allow the possibility for multiple cable types. The behaviour of the heuristics is evaluated both cost and time-wise in a wide range of instances. The results show that the parameters that differentiate their behaviour are the use of single or multiple cable types and the position of the substation, which can be located either centrally or outside the area of the farm. Moreover, a hybrid approach between POS and EW is developed that improves the performance of EW for multiple cable types. Finally, specific recommendations are made regarding the use of the best algorithm for each case. The practicality of the developed tool is enhanced by including the possibility to choose the switchgear configuration and by eliminating the crossings between inter-array cables and transmission lines. Last, modifications allow the minimization of crossings with pipelines/cables that are possibly laid on the seabed. Throughout the report, the comparison of results provided by the tool with the actually installed layouts shows the prospects of inter-array cable cost reductions.","Windenergy","en","master thesis","","","","","","","","","","","","Electrical Engineering | Sustainable Energy Technology","",""
"uuid:8af405cc-bdd1-46c0-a790-a66471eadb3f","http://resolver.tudelft.nl/uuid:8af405cc-bdd1-46c0-a790-a66471eadb3f","Solving the mTSP for fresh food delivery","Huizing, D.","Van Essen, J.T. (mentor)","2015","A starting company intends to deliver fresh baby food at home addresses in the area of Zwolle, the Netherlands. The limited time windows of delivery encourage the notion of dividing the addresses over several shifts. Dividing optimal routes over these shifts is shown to be an instance of the multiple traveling salesman problem (mTSP). Three problems are posed, all relating in one way or another to solving the mTSP: 1. Solve the mTSP for the specified delivery area; 2. Find an efficient method for solving an mTSP in which certain nodes may only be visited by certain salesmen; 3. Estimate the largest amount of customers that may be serviced per four hour shift. A number of methods to solve the mTSP are presented: namely branch and bound, greedy search, simulated annealing and neural networks. The first three of these methods are also modified to handle customer availabilities on specific shifts. The methods are tested and compared within the context of the three different problems. The results of these tests are discussed and solutions to the three problems are suggested.","mTSP; multiple; traveling; salesman; problem; branch and bound; greedy search; simulated annealing; neural networks","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Applied mathematics","","Bachelor of Science","","52.5, 6.0833333"
"uuid:d9f03d28-03ff-4fb7-992c-053908f87243","http://resolver.tudelft.nl/uuid:d9f03d28-03ff-4fb7-992c-053908f87243","The stability region of the tubing performance relation curve","Gromotka, Z.J.G.","Dubbeldam, J.L.A. (mentor); Egberts, P.J.P. (mentor)","2015","The tubing performance relation curve is a measure of well performance in gas well engineering. It describes the two-phase flow inside a well and as such is modeled as a two-phase one dimensional pipe flow. Convention claims that production points on the TPR curve to the right of its minimum are stable. There also exist claims about the region slightly to the left of the minimum of the TPR curve being stable. To find the stability criteria the time behaviour of a small perturbation from the steady state conditions is modeled and studied. In the end the first claim was indeed verified but further research is suggested to determine if the stable region might be slightly larger.","two-phase flow; pipe flow; nonlinear; stability; gas well; liquid loading","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Applied mathematics","","Mathematical Physics","",""
"uuid:3974d142-645b-4a1e-9a9a-08298697b240","http://resolver.tudelft.nl/uuid:3974d142-645b-4a1e-9a9a-08298697b240","Impact of ICT on the Productivity-Compensation Gap: A Study on Technology, Works, and the Role of Capital","Pramudita, D.A.","Naastepad, C.W.M. (mentor)","2015","Researchers have observed that the growth of hourly compensation in the U.S. has lagged behind the labour productivity growth since 1970 (Fleck, Glaser, & Sprague, 2011). The disparity between productivity and compensation, which is known as the productivity-compensation gap (Mishel & Gee, 2011), suggests that workers do not benefit directly from the increase in the productivity growth. The dynamism between labour productivity and compensation may be influenced by the use of Information and Communication Technology (ICT). By raising labour productivity, ICT saves (labour) costs - in this thesis, the money that is saved due to productivity growth, or freed by productivity growth, is called ‘freed capital’. In principle, the freed capital can in turn be used to expand the production and create new jobs for those whose work is made redundant by ICT. But is this the best possible societal option for the use of the freed capital? While increasing labour productivity, ICT may also reduce work and/or alter work relationships, implying significant shifts in responsibility and hence in remuneration. Following this, two main research questions are formulated: First, what is the impact of ICT on the productivity-compensation gap? Second, what principle that should guide the use of freed capital? In particular, could it be used to counter the impact of ICT on the productivity-compensation gap? Does a theoretical principle exist that would support such use of freed capital? In the end, this thesis finds that the impact of ICT on the productivity-compensation gap is hard to be measured, mainly because the impact of ICT on the hours worked is still inconclusive. Nonetheless, this thesis provides two insights that shows ICT still have influences on works: first, ICT contributes to around $ 1.06 trillion (in constant 2005 US$) of the U.S. value added in period 1970-2005. This means that while, apparently, ICT does not have significant impact on the total hours worked, it still increases labour productivity (output per hour). Secondly, ICT is shown to have a positive correlation with the increase of both high-skill and low-skill works, while it has an opposite effect on the medium-skill works. This means that while ICT is assumed to open up new work opportunities, it may also cause a job polarization, in which the medium-skill workers might end up competing for a lower-skill works. Following this result, this thesis reviews various economic theories to review principles that may guide the freed capital towards activities that may counter the impact of ICT on the productivity-compensation gap. This thesis then argues that the answer for such question will depend on which value that is important from the economic perspective. If utility maximization is accepted as the value that organize economic activities, the freed capital can find its place in the expansion of production, as what neoclassical and Schumpeterian perspective prescribe. However, if human beings are perceived to have a higher capacity than merely satisfaction of utility, then the freed capital should also be used for the development of human capacities and character. Just like the investment in innovation that need a great commitment, the investment in the development of human capacities and character will also need one, even greater.","ICT; technological revolution; works; capital; productivity; inequality; economics theory","en","master thesis","","","","","","","","","Technology, Policy and Management","Economics of Technology and Innovation","","Management of Technology","",""
"uuid:26cd1922-1ecc-4f7e-b8bc-46c5c00f8846","http://resolver.tudelft.nl/uuid:26cd1922-1ecc-4f7e-b8bc-46c5c00f8846","A 1-Mega Pixels HDR and UV Sensitive Image Sensor With Interleaved 14-bit 64Ms/s SAR ADC","Zhang, R.","Theuwissen, A.J.P. (mentor)","2015","This thesis presents a 1-Mega pixels high-dynamic range and UV sensitive image sensor in 0.18 µm technology with 14-bit interleaved 64Ms/s SAR ADC. It can achieve 64 fps and 101 dB dynamic range. The pixel array contains three kinds of pixel: UV pixel, visible pixel and low blue pixel. Those three kinds of pixel form a special 4×4 kernel to meet the technology etching requirement and an optimized modulation transfer function (MTF). The extended floating diffusion method is introduced to achieve the high-dynamic range. An odd/even time interleaved analog readout method is used to increase the read out speed. To read out column outputs, the row driver block, column loads, S/H circuits, readout buffers and digital control block are all implemented in this design. A 14-bit interleaved 64Ms/s SAR ADC is used to digitize the analog signals. The ADC block contains 16 14-bit 4Ms/s SAR ADCs. To achieve a 4Ms/s 2V peak-to-peak 14-bit ADC, a high speed comparator with offset calibration is implemented. Two 14-bit serializers are used to serialize the ADC output in this design.","UV sensitive; interleaved SAR ADC; comparator; offset calibration; high dynamic range","en","master thesis","","","","","","","","2015-09-14","Electrical Engineering, Mathematics and Computer Science","Microelectronics & Computer Engineering","","Microelectronics","",""
"uuid:b0856bac-e5bd-49a1-898e-476768bb3d2e","http://resolver.tudelft.nl/uuid:b0856bac-e5bd-49a1-898e-476768bb3d2e","Design of a Hand-Held Spasticity Assessment Instrument for children","Katsouris, D.K.","Van der Helm, A.J.C. (mentor)","2015","Development of a spasticity assessment equipment to be placed on children during examination.","spasticity; orthotics; compliant; grip","en","master thesis","","","","","","","Campus only","2017-09-11","Industrial Design Engineering","Industrial Design (AED)","","Integrated Product Design","",""
"uuid:59acb86c-167e-4851-b82d-efe0e3377fd9","http://resolver.tudelft.nl/uuid:59acb86c-167e-4851-b82d-efe0e3377fd9","Epidemic Mitigation via Awareness Propagation in Multi-layer Network","Chen, C.","Wang, H. (mentor); Qu, B. (mentor)","2015","The pervasiveness of the Internet and smartphones enables individuals to participate in online social networks such Twitter and Facebook, besides the classic physical contact network. Such multi-layer network allows for feedback between networks / layers. For instance, the spread of a biological disease such as Ebola in a physical contact network may trigger the spreading of the information related to this disease in online social networks. The information propagated online may increase the alertness of some individuals resulting in avoidance of the physical contact with infected members in the physical contact network, which possibly protects the population from the infection. In this thesis, we propose two models for studying not only epidemic spreading and information propagation, but also the interactions between the epidemic and information. We explore two key factors that may influence the performance of such epidemic mitigation via awareness propagation: (i) the time scale of the epidemic information propagation in online social network relative to that of the epidemic spreading in physical contact network, or equivalently, the information update frequency in the social network, and (ii) the structure of the multi-layer networks. Contrary to our intuition, we find that very frequent information updates in an online social network sometimes reduce the mitigation effect when using awareness information. Such mitigation tends to perform better when the physical contact network and the online social network overlap more. We explain these findings analytically, with the help of our original Individual-Based Mean Field Approximation IBMFA. Moreover, we derive the analytical approach Microscopic Markov Chain Approach MMCA according to our models. We show that IBMFA is a better approximation than the MMCA in some scenarios, especially around the epidemic threshold. Our results indicate that the optimum effect of epidemic mitigation does not require very frequent information updates in an online social network which dilute the alertness information and therefore reduce the effect of mitigation, whereas encouraging individuals to keep in touch with their physical contacts as well online is beneficial for the mitigation.","Epidemic Mitigation; Awareness Propagation; Interactions; Multi-layer Networks; Time scale; Overlap; Individual-Based Mean Field Approximation (IBMFA); Microscopic Markov Chain Approach (MMCA)","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Intelligent Systems","","Telecommunications","",""
"uuid:5cd532f3-afa5-45ad-812c-a49eb6449876","http://resolver.tudelft.nl/uuid:5cd532f3-afa5-45ad-812c-a49eb6449876","Mesh, shared products at your service","Postma, L.","Giaccardi, E. (mentor); Kleinsmann, M.S. (mentor); Schouwenaar, M. (mentor)","2015","In a world where resources are wasted as products are unused, it is time for a new way of sharing products. What if you don’t own a product but still have access to it? It’d be an ownerless product. This graduation project emphasises the best user experience of such a product and reimagines a new product-service system from the ground up, by examining the collaborative consumption context, exploring opportunities for ownerless products, and experimenting with designs for a new service to give access to products. All this forms the foundation for a set of guidelines on how to design an ownerless product. Building on the blueprint that’s been crafted in this report, Mesh is the new, sustainable, product service.","product sharing; ownerless products; user experience design; product-service systems","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Master of Science Design for Interaction","",""
"uuid:a336e1e0-dfe1-47e4-9c2b-c6194e4a5124","http://resolver.tudelft.nl/uuid:a336e1e0-dfe1-47e4-9c2b-c6194e4a5124","Designers in the Alliance: A study into the role, position and added value of external designers in Dutch infrastructure alliances","Kroes, M.E.F.","Hertogh, M.J.C.M. (mentor); Koolwijk, J.S.J. (mentor); Vrancken, J.L.M. (mentor)","2015","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Construction Management and Engineering","",""
"uuid:4f2f3d38-0f30-4d17-9cab-ddac98f03520","http://resolver.tudelft.nl/uuid:4f2f3d38-0f30-4d17-9cab-ddac98f03520","Maatschappelijk verantwoord aanbesteden - De ontwikkeling van maatschappelijke belangen in aanbestedingen","Yancheshmeh, S.R.S.","Chao-Duivis, M.A.B. (mentor); Van der Veen, J. (mentor); Hombergen, L.P.I.M. (mentor)","2015","","","nl","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Construction Management and Engineering","",""
"uuid:4a5cacb7-b4dd-4665-b697-c53427b3bed6","http://resolver.tudelft.nl/uuid:4a5cacb7-b4dd-4665-b697-c53427b3bed6","Design, Implementation and Verification of the Attitude Determination and Control Algorithms for the DelFFi Satellites","Haghayegh, M.","Guo, J. (mentor)","2015","","ADCS Satellite","en","master thesis","","","","","","","","","Aerospace Engineering","Spaceflight","","Space Systems Engineering","",""
"uuid:1c1fe286-d07f-46d0-a5b5-d0fbd86b03d2","http://resolver.tudelft.nl/uuid:1c1fe286-d07f-46d0-a5b5-d0fbd86b03d2","Empowering seniors: Design of a tool to support elderly patients and their care network","Alberts, J.W.","Albayrak, A. (mentor)","2015","Research showed how important it is for seniors to feel independent. During the ageing process, seniors go through several events that have great impact on both their physical wellbeing but also on their mental and subjective wellbeing. Leading the senior to feel insecure and sometimes even feel down. One of those events are falls. Falls can lead to hospitalization which concurrently lead to a decreased quality of life. Fall prevention interventions have shown to decrease the senior’s fall risk. Current interventions developed lack a user centered approach. Seniors are only perceived as patients, leading to a lack of empowerment. Therefore a definition for empowerment is developed: Enable self-awareness of subjective and objective well-being resulting in an increased self-confidence and self-efficacy throughout the ageing process. Bloei enables seniors to feel empowered. Through positive reflection on the seniors important moments in their daily lives, while assessing their abilities through health assessment the seniors are supported in the development of their self-efficacy. With a tailored advice and inspirational activities, the senior will become aware of his/her possibilities. A nurse practitioner will play a supportive role in order to increase the seniors confidence.","seniors; empowerment; personal informatics; awareness; self-care","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Master of Science Design for Interaction & Master of Science Integrated Product Design","",""
"uuid:ddee3708-bc9d-475c-b18f-196811c9777c","http://resolver.tudelft.nl/uuid:ddee3708-bc9d-475c-b18f-196811c9777c","More than just a new lettuce packaging","Kong, K.","Mugge, R. (mentor); van den Hende, E.A. (mentor)","2015","For this graduation project Rijk Zwaan has developed a new lettuce product that utilizes both an indoor vertically growing system and a packaging for leafy vegetables suitable for both growing and marketing (end package). The product of Rijk Zwaan deals with both growers and consumers. To growers the product must be feasible and appealing at a technical level. At the same time it must be marketing wise appealing to consumers as well. Currently the packaging is almost feasible at a technical level for growing. However, the packaging still needs a certain appearance that is appealing to consumers. In order to develop a successful packaged vegetable product, it must incorporate the needs and preferences of the targeted consumers as well (Brown & Eisenhardt, 1995; Cooper & Kleinschmidt, 1987). Moreover, elaborated analyses in the market of the consumers and the packaging are performed. In short, the meaning, form and function of the packaging still needs to be determined. To successfully launch the innovative vegetable product on the market it is important that the consumers are willing to purchase the product. The appearance of the product is the most fundamental determinant of sales success (Cooper and Kleinschmidt, 1987). To ensure the innovative product triggers consumers’ attention, it is essential that the product communicates a desired certain meaning (Chang & Wu, 2007) as people do not buy products but meanings (Verganti, 2009). Rijk Zwaan should therefore look beyong features, functions, and performances and understand the real meanings users give to the new lettuce product. Thus, the scopes of this project is (1) to analyse the market of the lettuce product and (2) in order to develop design guidelines to create the packaging design of the new product with the the desired meaning and a fitting appearance.","","en","master thesis","","","","","","","Campus only","2017-09-10","Industrial Design Engineering","Industrial Design","","Master of Science Strategic Product Design","",""
"uuid:0ee880fb-7063-42cd-aa82-b67f7c84d1e9","http://resolver.tudelft.nl/uuid:0ee880fb-7063-42cd-aa82-b67f7c84d1e9","Development of a low-cost 3D foot scanner","Hoeksema, J.","Molenbroek, J.F.M. (mentor); Song, Y. (mentor)","2015","As 3D-design and production tools are becoming increasingly important in the development of new products, the urge to 3D-digitize complex objects rises. A solution is found in a reverse engineering tool that allows real life objects to be converted into a digital representation. Those so called 3D-scanners are the subject of this graduation thesis, more specifically in the field of feet related products. At the moment of writing, 3D-scanners are generally considered expensive and do not always form a good substitute for traditional methods. This is mainly caused by specific properties required to make 3D-scanning a useful tool for the intended application. As current 3D-scanning solutions lack the ability to adjust to specific applications, the number of suitable applications remains low. This in turn has a negative effect on the costprice. Furthermore does 3D-scanning require knowledge on digital design and manifucturing tools. This knowledge was found to form a barrier in the implementation of 3D-scanning. However, in the production of feet related products, 3D-scanning has the potential to positively influence production efficiency as manual labor can be replaced by digital modelling. The complex shape of the foot is traditionally measured by hand, cast with plaster or estimated by eye. Those methods often involve a messy and laybor intensive production processes. Applications that involve the production of feet related products appeared to be very versatile. Where one application demands for a specific property in order to make the tool functional, another application might not benefit at all from this property. It was therefore found that a tool is required that has the simplicity to be applied without the need for extensive training, and at the same time has the flexibility to adjust to application specific demands. Since the integration of all those demans into a single device would be seemingly impossible, a more modular approach was used. Rather than using an optical non-contact tool to gather the required data, this design approach uses mechanical contact pins that ‘feel’ the shape of the object. Although this might seem like an obsolete technique, the system is designed to minimize the technical complexity and integrates application specific knowledge to achieve a useful result without demanding excessive data. Furthermore, this mechanical digitizing technique makes the process more insightful for the operator. The pins can be adjusted manually without requiring any knowledge on digital modelling. Due to the simplicity of the data output, the digital model can be previewed life during the digitization process. To illustrate this technique a showcase application was selected that involves the automatic generation of a shoe last for the production of mass customized shoes. A design is presented that incorporates knowledge on shoe fit to limit the number of required data points while constraining the measuring error. The limitation of data points keeps the complexity low and makes the mechanical construction possible. In order to give meaning to the data from the mechanical digitizing device, a parametric model was designed and tested. This model automatically creates a digital approximation of the foot that can be used as a basis for a shoe last design. As this system is designed to output a shoelast rather than the shape of the foot, the production workflow does no longer include the digital conversion from foot to shoe last.","3D; scan; parametric; foot; mechanical digitizer; foot model","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Applied Ergonomics & Design","","Integrated Product Design (IPD)","",""
"uuid:774cf3e6-dedc-462e-a70d-1ed7908cf6ff","http://resolver.tudelft.nl/uuid:774cf3e6-dedc-462e-a70d-1ed7908cf6ff","Arbitrage-free approaches for pricing interest rate derivatives under the SABR model","De Groot, M.R.","Oosterlee, C.W. (mentor); Hoorens, B. (mentor); Malafaia, V. (mentor)","2015","This thesis is about pricing swaptions under the SABR model or a variant thereof. In the interest market a stochastic local volatility is often used by practitioners to describe the volatility curve in the strike dimension of swaptions. It is a fast approach to inter- and extrapolate market quotes. It is however well-known that this approach is not arbitrage-free. This led to the investigation of approaches that are arbitrage-free. Several approaches have been proposed in the literature to resolve the arbitrage. Computationally rapid approaches that are arbitrage-free are desired as an alternative for Hagan's formulas. These approaches can be used to describe the volatility curve in the strike dimension. The focus will be on an approach that is analytically exact under the SABR model, an approach that reduces the dimensionality in the dynamics of the SABR process and the stochastic collocation method (SCM). The SCM will be used to remove the arbitrage in the stochastic local volatility approach. All approaches will be calibrated to swaption volatility curves and the impact on the inter- and extrapolation of the market quotes is investigated. This is done by investigating the extrapolation of the volatilities, the sensitivities and pricing constant maturity swap (CMS) derivatives using a convexity adjustment method dependent on the complete volatility curve.","SABR; volatility; swap; swaption; constant maturity swap; arbitrage; stochastic collocation method; convexity adjustment","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Numerical Analysis group","","","",""
"uuid:b722da02-089f-42a8-a3ea-fb3f5900bcdd","http://resolver.tudelft.nl/uuid:b722da02-089f-42a8-a3ea-fb3f5900bcdd","Persistent self-supervised learning principle: Study and demonstration on flying robots","Van Hecke, K.G.","De Croon, G.C.H.E. (mentor); Van der Maaten, L.J.P. (mentor); Izzo, D. (mentor); Hennes, D. (mentor)","2015","We introduce, study and demonstrate Persistent Self-Supervised Learning (PSSL), a machine learning method for usage onboard robotic platforms. The PSSL model leverages a standard supervised learning method to simplify the learning problem, but acquires training data in an unsupervised and autonomous manner. Using two platforms, a small multicopter on earth and the space based test bed SPHERES inside the International Space Station , we demonstrate the PSSL principle on a proof of concept problem: learning monocular depth estimation using stereo vision. The robot operates first in a ground truth mode based on the distance perceived by the stereo system, while persistently learning the environment using monocular cues. After the performance of the estimator transcends a ROC quality measure, the robot switches to operation based on the monocular depth estimates. Our results show the viability of the PSSL method, by being able to navigate a room on the basis of learned monocular vision, without collecting any training data beforehand. We identify a major challenge in PSSL caused by a training bias due to behavioral differences in the estimator and the ground truth based operation; however, this is a known problem also for related learning methods such as reinforcement learning. PSSL helps solve this problem by 1) clearly separating the learning problem from the behavior and 2) the possibility to keep learning during estimator behavior.","persistent self-supervised learning; MAV","en","master thesis","","","","","","","","2016-01-01","Electrical Engineering, Mathematics and Computer Science","Embedded Systems","","Pattern recognition","",""
"uuid:8694f9f4-14b4-483f-811e-54d12d5f782e","http://resolver.tudelft.nl/uuid:8694f9f4-14b4-483f-811e-54d12d5f782e","Tool transport within a high volume fibre metal laminate panel production environment","Rinsma, B.R.E.","Veeke, H.P.M. (mentor)","2015","This report describes the research into transport within the fibre metal laminate (FML) panel production process. The process analysis concludes what the possibilities are, the requirements and the boundaries for a tool transport system. The concept selection study shows that an AGV between the stations and an in-frame wheeled transport system within the station suits best. The logistical analysis show that this is possible with at least 5 AGVs, but that further study on planning might even reduce the number of AGVs required.","","en","master thesis","","","","","","","","2025-01-01","Mechanical, Maritime and Materials Engineering","Marine & Transport Technology","","Transport Engineering and Logistics","",""
"uuid:a354865a-d633-4263-8337-ca3f3809ba49","http://resolver.tudelft.nl/uuid:a354865a-d633-4263-8337-ca3f3809ba49","Predicting scour around offshore wind turbines using soft computing techniques","Centen, I.","Metrikine, A. (mentor); Raaijmakers, T.C. (mentor)","2015","In this report a prediction method is developed for scour around monopiles. A soft computing technique called genetic programming (GP) is used to create a scour prediction formula that can compute scour in all offshore conditions, meaning current-induced, wave-induced and combined current- and wave-induced scour. The GP was trained with an extensive database of laboratory scour measurements from multiple sources, to ensure that a wide range of conditions was represented. Furthermore, only dimensionless parameters were used to create a formula that is also applicable for field tests. The formulas where analyzed both on their mathematical and physical behavior and it was concluded that they could accurately predict scour in all conditions. The new scour prediction method was compared to various existing scour prediction methods and it was seen that the formula created in this study predicted more accurate scour depths, especially for test with larger scour depths. The study was finalized with a comparison to a second soft computing method: the neural network. It was found that the GP is less successful in predicting the scour depth compared to the NN. However, the high accuracy of the NN could not have been achieved without the knowledge of the parameter behavior obtained by the GP.","scour; offshore wind; monopile; Genetic Programming; GP; Neural Network; NN","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Offshore Engineering (as part of EWEM)","",""
"uuid:bbd67752-bb52-43c6-87a0-26b45d24c439","http://resolver.tudelft.nl/uuid:bbd67752-bb52-43c6-87a0-26b45d24c439","Work Deck Evaluation","Reijerse, F.J.","Hopman, J.J. (mentor); Van Oers, B.J. (mentor)","2015","To provide security at sea and to provide security from the sea the Royal Netherlands Navy (RNLN) has a fleet consisting of different kinds of vessels in several classes. One class of vessels is the Alkmaarklasse Mijnenbestrijdingsvaartuigen (AMBV), the Alkmaar class mine countermeasure vessels. These vessels are starting to show their age and the Defence Materiel Organisation (DMO) is developing concepts for the ‘new MCMV’ (Mine CounterMeasure Vessel). These concepts are assessed on their technical and financial feasibility, to see if the required performance is achieved and to see if they fit in the available budget. This thesis focuses on the work deck design of the concept designs of mine countermeasure vessels. Only summary available because of confidentiality.","design assessment; selection problem; launch and recovery; layout; MCMV; ship design; naval architecture","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Marine Technology","","Design, Production and Operation; Ship Design","",""
"uuid:83955e69-3fd1-4e96-bb32-9772d5a6c207","http://resolver.tudelft.nl/uuid:83955e69-3fd1-4e96-bb32-9772d5a6c207","Optical frequency shifting using (electro-optic) rotating wave-plates and its application in heterodyne displacement interferometry","Van der Niet, R.T.","Meskers, A.J.H. (mentor)","2015","The generation of a split-frequency is inherent to heterodyne displacement interferometry. Currently this split-frequency is predominantly generated by either acousto-optic modulators (AOM), Zeeman-lasers (ZL), or a combination. Both techniques have their drawbacks, e.g. AOM have a limited bandwidth and are optically inefficient due to a loss of optical power in various unused diffraction orders. Whereas the optical output power of a Zeeman-laser decreases rapidly as the split frequency is increased. This publication presents a third method to generate split-frequencies for heterodyne displacement interferometry, making use of an electro-optic rotating quarter wave-plate. Using this technique, the split-frequency is tunable, optical power is preserved, and the technique has potential to generate split-frequencies of several MHz. Moreover, this publication contains a complete review of frequency-modulation using rotating mechanical- and electro-optic wave-plates. The rotating electro-optic quarter wave-plate is implemented in a heterodyne displacement interferometer and compared to state-of-the-art split-frequency generation techniques in terms of periodic non-linearity (PNL). Experimental results showed a first order PNL of 2.7nm obtained with the presented electro-optic split-frequency generator, in contrast to 1.6nm and 0.18nm for the ZL and AOM respectively.","heterodyne displacement interferometry; periodic non-linearity (PNL); rotating wave-plate; optical frequency shifting; lithium niobate; electro-optic effect; single-sideband suppressed carrier","en","master thesis","","","","","","","","2017-08-28","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering","","Mechatronic System Design","",""
"uuid:35996aca-8365-4927-ada3-f9859e55d5fa","http://resolver.tudelft.nl/uuid:35996aca-8365-4927-ada3-f9859e55d5fa","Controlling and reducing case picking in the supply chain: A case study of Unilever & Kuehne + Nagel","Wammes, M.","Verbraeck, A. (mentor); Van Duin, J.H.R. (mentor); Duinkerken, M.B. (mentor); Loonstra, H. (mentor)","2015","","Case picking; Unilever; Kuehne + Nagel; Supply chain; Manual pick; Automatic Layer Picker; BBD; Best Before Date; Incomplete inbound; Customer behaviour","en","master thesis","","","","","","","","","Technology, Policy and Management","Policy Analysis","","Transport Infrastructure and Logistics","",""
"uuid:a4b80df7-4aef-4515-8d9c-831348db45bf","http://resolver.tudelft.nl/uuid:a4b80df7-4aef-4515-8d9c-831348db45bf","On the development of a surgebin","De Boer, A.A.","Van Rhee, C. (mentor); Talmon, A.M. (mentor)","2015","Research on the design of surgebins, used in the wet mining cycle. A 1.5D advection diffusion model is created and validated with experimental tests.","1.5D; dynamic; model; surgebin","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Offshore & Dredging engineering","","ODE","",""
"uuid:0e642228-c300-4db9-8946-0e9adcb63431","http://resolver.tudelft.nl/uuid:0e642228-c300-4db9-8946-0e9adcb63431","Monitoring of the Sittaung River","Van Rest, M.H.","Rutten, M.M. (mentor)","2015","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","","",""
"uuid:389f9950-ad0c-41f1-ad31-c19f9f5f55f0","http://resolver.tudelft.nl/uuid:389f9950-ad0c-41f1-ad31-c19f9f5f55f0","Fidelity of a photon stream during transmission","Van Marlen, P.","Blaauboer, M. (mentor); Dubbeldam, J.L.A. (mentor)","2015","","","en","bachelor thesis","","","","","","","","","Applied Sciences","Quantum Physics","","","",""
"uuid:41f5555e-2173-487d-b517-f11b03ee20fe","http://resolver.tudelft.nl/uuid:41f5555e-2173-487d-b517-f11b03ee20fe","Saving energy in care homes through behaviour change","The, L.E.","Romero Herrera, N.A. (mentor); Sonneveld, M.H. (mentor); Guerra Santin, O. (mentor)","2015","The thesis details the graduation project about saving energy in care homes through behaviour change. The project is part of a bigger project between TU Delft and Eneco, a Dutch energy company. Working towards their vision to provide their customers with services that help them save energy, Eneco invested in corporate customers to review and renovate their buildings to become more sustainable and energy efficient. Users of a building often have a measurable influence on the energy consumption of a building. In order to meet the desired energy performance, the occupants will need to change their energy use behaviour. Eneco is interested in helping their customers save more energy by supporting such a behaviour change amongst occupants. Excessive energy consumption is a problem for the Dutch healthcare sector, as they need to draw away money from their own care budget to cover their energy costs. This results in less care per client due to the lowered budget. Amstelring, a care organisation and one of Eneco’s corporate customers, recognises this budgeting problem. Amstelring’s care homes formed a case study for this project to find out how energy can be saved in care homes through behaviour change. In health care institutions like care homes, the main driver is providing good care to clients. As such, the wellbeing of clients is of great importance. Their wellbeing is influenced by the indoor climate, mainly temperature and fresh air. The indoor climate of a building plays a big role in its energy consumption, especially since the heat loss of a building depends on the difference between outdoor and indoor temperature. Meaning that the clients’ comfort needs are a crucial factor for how far energy saving practices can go; a tension exists between energy saving on the one hand and ensuring comfort on the other hand. Contextual research found that caregivers are a promising target group to design for, as caregivers have the ability to regulate comfort in all client rooms and are closely related to the clients’ needs. Currently, the only ways to maintain the comfort in client rooms is by manual actions such as turning on the heater or opening a window. This leads to fluctuations in the temperature, resulting in energy loss. When, in the future, the temperature is maintained along with the client’s preference, no extra energy will be lost. Clients are often (partly) incapable of performing the actions for maintaining comfort. Therefore, communication with caregivers is crucial to maintain the comfort effectively. However, there is a communication gap between caregiver and client that makes it difficult to maintain the client’s comfort effectively; caregivers feel a lot of time pressure which makes them more focused on providing the primary care for more obvious client needs; clients, in turn, sometimes find it difficult to express how they feel and are sometimes hesitant to call a caregiver for help when they feel uncomfortable. The final design, Fruitful, tackles the communication gap by bringing caregiver and client closer together to establish two-sided communication about comfort. Fruitful is a small table plant that reacts to its environmental climate by measuring the temperature, humidity and carbon dioxide of the surrounding environment. By reacting to these parameters, Fruitful offers a quick way to check whether something is wrong, without obstructing the caregiver’s workflow. The design is a metaphor to a real plant, which both caregiver and client understand. That way, the design facilitates caregivers and clients in a fun way to talk about comfort and to discuss solutions when needed. Even if the client does not express his discomfort, Fruitful’s indication provides enough motivation for the caregiver to ask. The indications from Fruitful leave room for discussion between caregivers and client and provide the opportunity to set preferences. As such, Fruitful is rather a reflection tool that helps caregivers and clients discuss heating and ventilation regularly. As a result, the comfort is maintained well and gas is used effectively. Fruitful is just the beginning of a future vision. A smart thermostat for each individual client room is the ideal future scenario, where caregivers are supported to maintain comfort efficiently and in a carefree way. Fruitful’s current design makes sure that caregivers and clients learn to communicate about comfort on a routine basis, which improves the effectivity of future solutions.","design; energy saving; behaviour change; care homes","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Master of Science Design for Interaction","",""
"uuid:cf175f30-5cea-430a-a252-0e581203abd1","http://resolver.tudelft.nl/uuid:cf175f30-5cea-430a-a252-0e581203abd1","The Value of Data for Philips Hospital to Home Solutions: A strategic design approach for creating shared value through integrated Chronic Care data solutions","Canales Durón, R.","Kleinsmann, M.S. (mentor); Simonse, L.W.L. (mentor)","2015","This project analyzed the topic of what is the value of data for Hospital to Home (H2H) telehealth solutions focusing on chronic care in the Netherlands in order to deliver strategic design advice on what are interesting opportunities to explore in this field. The project focused on an analysis of the strategic situation of the company that explored the external context of integrated care and a shift towards digital healthcare, as well as the internal context of the company towards integrated solutions and the consolidation of a HealthTech company. In this context, H2H has the potential of becoming a driver that integrates both the internal and external trends into meaningful solutions that connect patients and care providers via telehealth. Due to the rapid advancements of information and communication technology (ICT), many healthcare needs can now be potentially provided outside of the hospital through different digital platforms, also known as eHealth or telehelath. There are many of these solutions available in the market, and even within Philips, but are not yet connected to each other. Therefore the objective was set on developing an integration strategy and future opportunities for H2H, and also on exploring business models for such strategy in the Dutch context. The current landscape of chronic care in the Netherlands was therefore explored in order to assess how far are these developments in practice and what needs are there yet to be addressed. Interviews with different actors were used to create a map of the current use of data and information systems in the Netherlands for chronic care. This research concluded in a current scenario of difficult communication due to ill-suited technologies and organizational structures, where several organizational and people barriers resist change such as conflicting views on value in healthcare and resistance to change from people in the organizations. It was therefore concluded that it was necessary not to only look into future opportunities in terms of new products or services, but also on organizational aspects in implementation, such as payment schemes, stakeholders, roles and change management. Based on these findings and the objectives established, a future scenario for integration was created and different opportunities were explored that resulted in an organizational model and a service blueprint to translate this vision into a framework for innovation. The future scenario considers using the HealthSuite Digital Platform (HSDP) as an integrator of digital services and the eCareCoordinator and eCareCompanion tools as platforms for patients and care providers to fulfill their health and healthcare related needs. The envisioned organizational model aims to make H2H more attractive for and create value in care practices in the Netherlands where there is a clear shift of chronic care towards primary care settings, opposed to the current strategy of focusing on large hospitals. In this model the alternative to use legal entity organizations known as ‘care groups’ is suggested as it makes value-based payments possible in the Netherlands in a similar manner to Accountable Care Organizations (ACOs) in the US, which H2H has experience with. The service blueprint maps different potential digital, data and information management solutions throughout a customer journey from implementation up to a self-care stage as a potential future program for H2H. This model recommends using H2H to create programs for different levels of disease and with a focus on facilitating a transition to self-care for patients. Finally a strategy roadmap visualizes the transition from the current context into the future scenario described in this report at different levels that impact innovation. The key recommendations discussed in this report are the integration strategy and key role of H2H in it, a more holistic approach to innovation that also considers organizational and people aspects, and thus a stronger collaboration between the internal innovation and business groups.","chronic care; eHealth; telehealth; strategic design; data","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","Master of Science Strategic Product Design","",""
"uuid:b3c23f27-259a-44a4-9e16-598740232b3e","http://resolver.tudelft.nl/uuid:b3c23f27-259a-44a4-9e16-598740232b3e","Offshore stabilization pontoon for a heavy lift vessel: Concept design and workability","Ten Klooster, A.M.","Huijsmans, R.H.M. (mentor); Den Haan, J. (mentor); Hekkenberg, R.G. (mentor); Van der Heijden, K. (mentor)","2015","","workability; stability; pontoon","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Offshore and Dredging Engineering","","","",""
"uuid:6ff67b1f-adf7-4435-9600-7d29ce0c5147","http://resolver.tudelft.nl/uuid:6ff67b1f-adf7-4435-9600-7d29ce0c5147","Operability study of floating bulk transhipment operation","Wang, Y.","Huijsmans, R.H.M. (mentor); Van Deyzen, A. (mentor); Pauw, W. (mentor); Naaijen, P. (mentor); Godjevac, M. (mentor)","2015","Floating Bulk Transhipment Operation (FBTO), as a link in the whole bulk logistic chain, used to be performed only in well-sheltered water. Current operability assessment of FBTO is mainly based on experience and rules of thumbs, only taking into account environmental conditions such as significant wave height and wind speed. Corresponding to rocketing development of FBTO in more challenging environment, it becomes more and more important to develop a dedicated and reliable methodology which can assess the operability of FBTO properly. Matching the features of FBTO, long duration and multiple operational phases, persistency analysis is introduced in the operability study. However, persistency analysis has still not been thoroughly studied and well-supported by literature. Moreover, among all existing operability techniques, persistency analysis as one of them has its capability and limitation. To be better adapted to FBTO and to further extend the methodology for other offshore operations, it is thus beneficial to decompose the operability assessment procedure and then categorize the available techniques, as well as to understand conditions of each. This thesis report starts with a benchmark study of worldwide FBTO project. Among various FBTO configurations, the most representative scenario is chosen, which consists of 1 capesize vessel, 1 floating crane and1 feeder vessel. Operation procedure and criteria are described based on interviewing different floating bulk transhipment operator companies. After that, this thesis proposes an operability assessment methodology for FBTO and has the versatility in assessing any other offshore operation, if the three components of this methodology, operability assessment table, mechanism and switches, are used properly. The methodology concludes with an operability assessment scheme, in which useful operability study techniques, such as persistency analysis, scatter analysis, frequency-domain analysis etc., are categorized. Then, an in-depth study about persistency analysis is performed. Persistency analysis approaches more reality than scatter analysis because it accounts duration and chronological sequence of the operation, as well as change in weather conditions. Finer persistency data quality and proper selection of sampling interval will both lead to more accurate operability assessment. Last but not least, the case study illustrates how the proposed methodology works. The first case compares persistency analysis to scatter analysis, while the second one studies the influence due to persistency data of different resolution on operability assessment. The last case demonstrates using this methodology to predict expected duration of FBTO including possible suspension.","operability study; floating bulk transhipment operation; persistency analysis; operability assessment scheme","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Offshore engineering","","Offshore and dredging engineering","","52.001111, 4.371990"
"uuid:d6a94507-05c5-483f-8a15-2c5ab3d266b0","http://resolver.tudelft.nl/uuid:d6a94507-05c5-483f-8a15-2c5ab3d266b0","Semi-nonlinear analysis of the effect of deepening on tidal asymmetry","Van der Leer, N.A.","Winterwerp, J.C. (mentor); Van Prooijen, B.C. (mentor); Wang, Z.B. (mentor); Dijkstra, Y.M. (mentor)","2015","In the last century, various tidal rivers in Europe have seen a large tidal amplication, which is believed to be caused by the deepening and channelization of these rivers. Additionally, large increases in suspended sediment concentrations induced by flood dominance are thought to have decreased the effective hydraulic drag, enhancing further tidal amplication. The tidal amplication causes problems with respect to flood defenses and navigational safety. Furthermore, the increased concentration of suspended sediment causes ecological problems. In the meanwhile, ports along these rivers consider further deepening to accompany ever-larger ships. This research aims at a better understanding of the consequences of deepening and/or a reduced hydraulic drag for tidal asymmetry. Special attention is given to the non-linear behavior of tidal propagation into rivers. Much research has already been done on the implications of deepening tidal rivers. These studies have been done by using either linearized models or fully non-linear ones to solve the Saint Venant equations. This research uses a perturbation model to relate those linear analyses to non-linear behavior. The model uses linear solutions for the water levels and flow velocities (referred to as the leading order solutions) to generate estimates of the effect of the non-linearities, allowing to calculate them separately. By comparing the solutions to those of the fully non-linear SOBEK model we show that this perturbation model provides good estimates of the magnitude and the behavior of the residual and the M4 tide for values of epsilon up to at least 0.5. This research also shows that a linearized friction parameter r can be just as sensitive to boundary conditions or geometrical parameters, as it is to quadratic friction parameters such as the Chézy coefficient. This is done by consistently fitting the leading order solutions of the perturbation model to the M2 frequency solutions of SOBEK with a constant Chézy coefficient. Especially for variations of the imposed tidal wave amplitude, we found a strong sensitivity of the linear friction parameter r. This notion makes the interpretation of changes in r complex and reduces the utility of linearized models. Concerning the role of non-linearities with respect to tidal asymmetry we show that for all realistic depths in the Upper Sea Scheldt, the internally generated M4 tide and residual flow is of the same order of magnitude or larger than the external M4 tide and river flow. Moreover, in case of deepening, the relation between the importance of the non-linearities or overtides and the non-linearity parameter epsilon is not trivial. We depicted a range of epsilon which no clear relation to the overtides exists, denoted as the arbitrary range. The main reason for the existence of this range is the complex response of the leading order water level and flow velocity to deepening. The largest non-linearities depend on both these leading order solutions. Additionally, all non-linearities have a different direct dependency on the average water depth h. It is encouraged to continue work with the perturbation model as it shows great analysis potential. The use of a more accurate description of the friction term is recommended.","tidal asymmetry; non-linearity; perturbation approach; Upper Sea Scheldt","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Environmental Fluid Mechanics","",""
"uuid:6b98ee93-f020-4afa-a5a1-d21d5c1b4717","http://resolver.tudelft.nl/uuid:6b98ee93-f020-4afa-a5a1-d21d5c1b4717","Planning visualization during rail possessions","Schouten, L.","Verbraeck, A. (mentor); Vrancken, J.L.M. (mentor); Van Nederveen, G.A. (mentor); Van Riel, B. (mentor)","2015","OV SAAL Zuidtak Oost is a large construction site where BAM Rail expands rail capacity from two to four railway tracks over 8 kilometre. In order to have a safe work environment, BAM Rail is allowed to possess the railway tracks for variable durations. For OV SAAL Zuidtak Oost a 9 day-rail possession is scheduled for the beginning of August 2016. For these nine days, at four different connected construction sites rail related construction works are executed. A strict and complex planning schedule is needed to guarantee all construction works can be completed before the deadline. In preparation for rail possessions, Gantt-charts and time-distance diagrams are used to create the planning schedules. Gantt-charts consist of a chain of linked activities representing the different tasks the construction site managers have to execute. Problem statement The planning methods that are currently used provide insufficient overview over complex TVPs (‘TreinVrije Perioden’). Only very experienced planners have the knowledge to make and interpret these planning schedules. Users identify three missing aspects resulting in failure costs and inefficiencies:  The currently used planning methods show no spatial overview of the state of the complete line system at each moment of time.  No clashes between different operational activities can be detected.  Construction logistics (working trains, heavy machinery) cannot be represented with the current methods. BAM Rail experiences failure costs and expensive operational inefficiencies by the application of Gantt-charts and time-distance diagrams for general rail projects. Many users indicate that a spatial representation of a Gantt-chart would be useful in eliminating risks and inefficiencies. Therefore, the objective of this research is described as follows: To design a planning visualization method which assists BAM Rail in enhancing insight into the state of a linear construction site over time. Apart from the design of a planning visualization method, this research presents several additional recommendations related to observed processes during rail related construction works. Current situation User interviews and site visits were conducted to perform an in-depth analysis of the current situation. After categorizing the users’ responses, six main causes of risks and inefficiencies were identified: 1. Absence of clash detection: some clashing activities and invalid state sequences are left unidentified in the planning phase, resulting in failure costs during the construction phase. 2. High project complexity: Gantt-charts become too complex to comprehend, resulting in experts making mistakes when creating planning schedules. 3. Missing project overview: no basic site overview is available, resulting in non-expert decision makers making wrong decisions. 4. The impossibility to perform a scenario analysis: this results in failure costs as no control measures and fall back scenarios can be quantitatively analysed. 5. The impossibility to show construction logistics real-time: train paths and machinery locations are unknown, resulting in failure costs when making operational decisions. 6. The impossibility to determine delay propagations real-time: measures to settle delayed activities cannot be quantitatively analysed real-time, resulting in failure costs when making operational decisions. A requirements analysis presents a set of requirements for a visualization method. The complete set of requirements spans a solution space for a high-level design. High-level design Stakeholder interviews and test runs resulted in insights in the desired model output. The desired model output has been used to create a platform-independent high-level design. The high-level design consists of a model architecture, software architecture, pseudo code and system specification for use in the implementation in a software package. An important aspect of the high-level design is clash detection. Five different types of clashes are identified. For all five an automatic clash detection module is required. Activities moving in time are included in the high-level design as well. Implementation The suitability for implementation of the high-level design has been evaluated for two software packages, Navisworks and ArcGIS. The four main categories of evaluation criteria are defined as functional criteria, project management criteria, user interface criteria and configuration criteria. Preliminary tests with both software packages formed the basis for the evaluation. ArcGIS outperformed Navisworks in three out of four main criteria. After a sensitivity analysis, it was decided to create a prototype of the high-level design in ArcGIS. During the process of generating and validating the desired user output in ArcGIS, several shortcomings of ArcGIS were detected. These shortcomings were sorted based on the main functional aspects of the visualization model. Software developer Esri Nederland BV has been consulted to confirm the identified shortcomings and asked for strategies to overcome these limitations. Several recommended adjustments for ArcGIS are presented to Esri before a fully functional model can be implemented. Esri estimated that an investment of --------- and -------- are necessary to deliver a fully functional visualization model, satisfying all user requirements. Research conclusions The conclusions of this research answer the main research question of this research: How can BAM Rail get insight into the state of the construction works at a line construction site at each time step? Firstly, it can be concluded that failure costs are associated with the identified risks and inefficiencies. Providing an adequate insight into the state of the construction site system at each time step reduces the planning related risks and thus the failure costs. It can be concluded that the state of the construction works can be represented best by several aspects: 1. For each planned activity the location must be presented in a geospatial overview. Both static and dynamic activities must be included. 2. For each user-defined track section the state of that section must be presented in the geospatial overview. 3. Time and infrastructure constraints must be included in the geospatial overview. It is concluded that a planning visualization method is necessary to get insights into the state of the construction works of a line system for each time step. A module to automatically detect clashes is necessary to assist users in detecting the identified types of clashes. The high-level design included how the model architecture and software architecture of a planning visualization model should look like. By implementing a software package with these characteristics and structure, insights into the state of a construction site system can be provided. Esri’s ArcGIS is found to be most suitable for the implementation. The possibility to present activities and states as polygons matches with the desired user output. Furthermore, CAD drawings and aerial photos facilitate an overview on scale. Finally, intuitive navigation through ArcGIS is considered as user-friendly. Concluding, creating a planning visualization model in ArcGIS results in satisfying the user requirements– from planner to site manager – and thus solve the six main causes of risks and inefficiencies. Recommendations BAM Rail is advised to request an official proposal from Esri for a visualization method in ArcGIS. In addition, a BAM project team leading the implementation in ArcGIS should be formed. This team is responsible for monitoring the progress and deadlines, organizing test runs and defining milestones and showstoppers. Additional recommendations are presented. First, two recommendations for BAM Rail are presented. Then, recommendations for further research are presented. It is observed that the communication between the coordinating construction site manager (BCU) and construction site managers (‘uitvoerder’) during the construction works is done only by phone, once every 2-4 hours. Users consider these phone calls as inefficient for regular operations. Therefore it is recommended to investigate the implementation of an integrated mobile system for site managers to report the progress using an application on a mobile phone or tablet. Furthermore, it is recommended that a more thorough workplace inspection is executed before a TVP starts. Currently, it often occurs that physical obstacles or deviating situations are only discovered during the construction phase, resulting in a wrong initial state. When these deviations are discovered in an early phase, failure costs can be prevented. This research presents a visualization algorithm rather than an optimization algorithm. It is recommended to investigate the possibility of implementing an optimization algorithm to not only visualize planners’ decisions, but also making them. Two aspects must be changed when designing an optimization algorithm. First, the addition of a probabilistic activity duration instead of a deterministic activity duration must be added. Second, expressing the starting time of each activity in completion time of prior activities must be added. Finally, it is recommended to research the possibility of implementing resources in the model. By considering available working crew, machinery or costs, the use of these materials can be viewed at each moment in time. Also for the extra resources constraints can be defined. Resources can be implemented in the clash detection model as well.","planning visualization; contractor; Gantt-chart; spatial planning; rail possession","en","master thesis","","","","","","","","2015-08-23","Technology, Policy and Management","Transport & Planning","","Transport, Infrastructure and Logistics","",""
"uuid:7ff550cd-2c04-4bf2-8ddc-d62b0043a492","http://resolver.tudelft.nl/uuid:7ff550cd-2c04-4bf2-8ddc-d62b0043a492","The Effect of Context Content and Immersion in Product Placements","Van de Wildenberg, I.M.A.","Van den Hende, E.A. (mentor); Saaksjarvi, M.C. (mentor)","2015","This graduation project is carried out for Greenhouse Group (GHG), a company that is active in the field of online marketing. Because the online marketing world is a competitive field, looking into innovative technologies like AR and VR is important. In this way they can find new ways to enhance their marketing position. AR is about supplementing the physical, real world environment with elements that are generated with a computer, while VR is about completely secluding a person’s view from the outside world by a head-mounted display, making the consumer feel like being in a virtual environment. These technologies can be used as innovative channels for product placements, making a product or brand appear in a virtual or augmented environment. Such technologies offer the option to easily adjust the context of the product placement and to make the consumer immerse into the environment of the placed product. These features might change the overall effectiveness of the product placement. Process The balance model, proposed by Russell and Stern (2006) was used as the basis of this research. The connections in the model were adapted to product placement situations, using holographic technologies (like AR and VR) as their channel. Like the initial model, the adapted one was based on Heider’s balance theory (1946). To find out whether this model is applicable to predict the effect of such product placements, an independent 2x2 research was carried out with 170 participants. Results The results of this research showed that the adapted balance model is applicable to predict the effect of product placement situations using AR or VR as their channel, but only when the consumer-context connection was evaluated as positive. The evaluation of this connection can significantly be influenced by the content of the context. Manipulating the level of immersion had a significant direct effect on the consumer’s attitude towards the placed RNP. When a content full context was shown, the immersion manipulation also affected the vividness and imaginability of the product. By knowing these findings, the effectiveness of product placements can be influenced. Conclusions Based on the results, recommendations for GHG and product innovation management could be made. The most important ones are: 1. Combining the benefits of AR and VR with other skills the company has can improve the effectiveness of marketing campaigns. Such skills include using open data to find the consumer’s preferences, to realize the most effective context situation. 2. Add product placements into the increasingly popular immersive games. Compared to traditional product placements, this way seems less persuasive and results in a higher day after recall, and thus more effective product placements. 3. By changing a product placement’s context according to the consumer’s preferences, the product placement can be made more effective.","Product placement; Immersion; Virtual reality; Augmented reality","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","Graduation project","",""
"uuid:eab22a8e-bc93-4c68-a4df-02c3aa586dea","http://resolver.tudelft.nl/uuid:eab22a8e-bc93-4c68-a4df-02c3aa586dea","Water level monitoring in the Sittuang river basin","De Vilder, L.H.","Rutten, M.M. (mentor)","2015","","Monitoring; Sittaung; Myanmar","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","","",""
"uuid:9074805a-194c-4a35-aa03-595ccc7ffeab","http://resolver.tudelft.nl/uuid:9074805a-194c-4a35-aa03-595ccc7ffeab","Aircraft maintenance cost modelling considering the influence of design parameters","Urdu, M.","Zhao, X. (mentor)","2015","About 70-85% of commercial aircraft Life Cycle Cost (LCC) is determined during the design stage of the aircraft. Aircraft maintenance is a vital process within aircraft operation and it takes around 15% of LCC. The present day aviation market drives the airlines to reduce the maintenance cost as much as possible. The current maintenance cost estimation methods used by the airlines mainly emphasize the cost contribution due to operation parameters. However, in order to analyze the maintenance cost of an aircraft and its components, influences from both the aircraft operation parameters and aircraft design parameters should be investigated and incorporated in the maintenance cost estimation model. Therefore, the research question of the project is: How to develop a maintenance cost estimation model by emphasizing essential operation and design parameters? To answer the research question, a methodology is developed to estimate the maintenance cost of an aircraft and its components as a function of operation and design parameters. Two maintenance cost estimation methods are developed: Aging Involved Method (AIM) and Interval-Based Method (IBM). The AIM method estimates the Scheduled and Unscheduled Maintenance Labor Cost (SMLC and UNMLC), Scheduled and Unscheduled Maintenance Material Cost (SMMC and UNMMC) and maintenance overhead cost by taking the aging factor into the account. On the other hand, the Interval-Based Method (IBM) conducts detailed inspection of average maintenance labor cost by categorization. The categorization can be made in terms of materials, aircraft part items (spar, skin, rib, and etc.) and maintenance tasks. Afterwards, a bottom-to-top maintenance labor cost estimation is performed, based on the maintenance intervals and man-hours, which are extracted from the Maintenance Planning Document (MPD). In order to obtain a direct link between design and operation parameters, and maintenance intervals, a method starting from component load conditions, stress analysis, damage types, till maintenance interval determination has been developed. The developed methods are applied on the composite rudder of an A330-200 aircraft as a case study to obtain cost outputs. During the AIM analysis, the SMC is approximated as a constant while the UNMC increases as a function of increasing aircraft age. From the sensitivity analysis on the AIM method, the main cost drivers for operation and design parameters are obtained: vertical tail surface area, flight cycles, flight hours and rudder surface area. From the IBM analyses follows that, maintenance labor cost of composite rudder components are less than non-composites materials.","aircraft maintenance; cost modeling; composite","en","master thesis","","","","","","","","2016-01-23","Aerospace Engineering","Aerospace Design, Integration & Operations","","","",""
"uuid:da489993-b134-4a1d-b817-c984afc3b0fa","http://resolver.tudelft.nl/uuid:da489993-b134-4a1d-b817-c984afc3b0fa","Multiaxial fatigue criteria for offshore mooring chains subjected to out-of-plane bending","Calf, I.J.","Romeijn, A. (mentor)","2015","The out-of-plane bending of offshore mooring chains leads to a rapid deterioration of the chain, ultimately leading to failure in a period that is a fraction of the design-life. The deformation of the contact surface of the chainlinks leads to the interlocking of the link. This in turn causes the chain to behave like a bending beam. The combination of the chains behaving in this manner and the vessel movement leads to a bending moment in the chain link, especially in the top section, where the tensions are high. Due to the alternating nature of the loading, multiaxial fatigue effects cause chain failure. In this thesis, a literature study is performed, forming an introduction to the general concept of fatigue and the exploration of available methods. For multiaxial loadings, so-called critical plane methods are widely accepted as the most effective approach. This critical plane method search for the material plane experiencing most damage, according to the damage criterion that is assessed. Furthermore, a number of methods for the cycle decomposition are present. For uniaxial methods, the rainflow method is most used. Applying this method to multiaxial fatigue problems can lead to problems, which leads to the formation of alternative approaches. The other approaches being discussed are the Wang & Brown method and the Modified Wang & Brown method. The Bannantine & Socie method is discussed too, but could be classified as a critical plane search algorithm, instead of a novel cycle counting method. After the exploration of the problem, a finite element model is created. This model consists of multiple chain links, that are proofloaded while plastic deformation is allowed. This leads to the aforementioned deformation of the contact surfaces on the links. After the proofloading step is completed, three different cases are performed with an elastic material formulation. These cases comprise of varying angle ranges; Case 1 only induces an out-of-plane bending in the analysed link, Case 2 induces an out-of-plane and in-plane load, while Case 3 is similar to Case 2, apart from an added phase difference between the two angles. The stress and strain results from the finite element analyses are input in the Pragtic fatigue software. For each case, a number of fatigue criteria are calculated. These criteria are, named after the author, the following: McDiarmid, Dang Van, Matake, Liu & Mahadevan, Carpinteri & Spagnoli, Findley, Wang & Brown & Miller, Smith & Watson & Topper and Socie. Different critical plane search algorithms and/or cycle counting methods have been explored. Some of these criteria lead to a Fatigue Index, while others lead to an Accumulated Damage formulation. From the results it is clear that the cases combining two angles lead to higher damages, leading to the believe that the multiaxial behaviour should be taken into account when assessing fatigue damage of mooring chains. Furthermore, it seems that for the different cases, the SWT criterion gives either the highest or the lowest damage. As this is originally a multiaxial parameter, it shows that the use of traditional uniaxial methods could either over- or underestimate the damage, depending on the specific loading. Furthermore, results show that the criterion that is suggested by Bureau Veritas' Guidance Note on OPB, namely the Dang Van criterion, often gives the lowest fatigue index. This suggests that this criterion might not be conservative enough. This statement needs to be validated with the help of experiments. Furthermore, the method specifically aimed at multiaxial problems, the Wang & Brown method (including the multiaxial cycle counting method), leads to highest accumulated damage. As this method was aimed for multiaxial and non-proportional loading, it can be reasoned that this method makes a conservative, proper prediction. As for the critical plane orientation, it seems that the most damaging plane, likely the location of crack growth, is often close to the shear plane. This fuels the believe that the shear stress is the factor driving the OPB failure mode. However, since this problem inherently involves a mean stress (the tension on the mooring chain), the critical plane does not need to be the shear plane. When exploring this with the globe plane search algorithm, it seems that the critical plane is often a plane oriented at some angle in between the shear and normal plane. The results and statements made in this report are at this point mathematical. As the OPB concept is relatively newly discovered, more experiments are needed to be able to back up any statements that can be made. Furthermore, the research presented here could benefit from a larger group of cases, and added detail on the loading part of the analyses. Furthermore, as material fatigue parameters are limited, a sensitivity anayses on these parameters could shed some light on the influence of parameters used. The parameters used in this report are taken from similar materials and should at least give a very reasonable approximation of the material's fatigue behaviour.","fatigue; OPB; mooring; out-of-plane; chain; offshore","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Department Maritime & Transport Technology","","Section Ship and Offshore Structures","",""
"uuid:baabe051-0c11-449e-8aca-565f419603f8","http://resolver.tudelft.nl/uuid:baabe051-0c11-449e-8aca-565f419603f8","The wasted Disposables in Dutch Hospital’s health care pathways","Oomen, V.A.M.","Ludema, M.W. (mentor); Beelaerts van Blokland, W.W.A. (mentor); Tavasszy, L. (mentor)","2015","The Dutch health care system is a large drain on the Dutch national budget. With the Dutch hospitals costing 20 billion euros yearly. Six billion euros, of the total 20 billion euros are spend on goods and materials. If a part of these goods are wasted within the health care system, the costs of this system could be reduced by reducing this waste. This report has researched the possible waste reduction possible in health care pathways by researching three health care pathways in Dutch hospitals. This report answers the research question: How can the possible wasted disposables be reduced in health care pathways in Dutch hospitals . This question is answered with the creation of a framework based on multiple case studies created from research done in the three health care pathways, Percutaneous Coronary Intervention, Cataract and Hip Fractures. These health care pathways are visualised using IDEF0 diagrams, and the disposable flows are shown in matching tables. To provide constructive solutions to possible wasted disposables within health care pathways in Dutch hospital in general, a framework is constructed. This framework generalizes health care pathways to four basic steps, diagnostics, pre-operation, operation and post operation. The main solutions to reduce the chance of wasted disposables within health care pathways, is to improve the amount of standardisation of disposables used and needed, and improve the communication and collaboration between parties within the health care pathways. These solutions are based on the case studies and the literature research on the Dutch health care system in general, Dutch hospitals and the goods logistics within the Dutch health care system. This also provides an answer to the research question, on how to reduce disposables being wasted in health care pathways in Dutch hospitals. This thesis research is conducted on three care pathways are researched in multiple hospitals, but not all Dutch hospitals. Different results could be reached when researching other health care pathways or different hospitals, but the framework provided in this report could be valid for these different situations. Both the validity of this research and the constructed framework has to be researched in subsequent research studies. These subsequent research studies have to verify if the conclusions derived in this research is valid for other health care pathways and if the framework is applicable and if it has achieved its objective.","Disposables; Duth Health care system; logistics; supply chain","en","master thesis","","","","","","","","2015-09-04","Technology, Policy and Management","Transport Infrastructure and Logistics","","","",""
"uuid:a7b46938-8cce-4ddf-b68e-1159293eedc7","http://resolver.tudelft.nl/uuid:a7b46938-8cce-4ddf-b68e-1159293eedc7","Towards a Service-Oriented Company within Healthcare","Gedde, T.","Hultink, E.J. (mentor); Roscam Abbing, E. (mentor); Lynghaug, E.A. (mentor)","2015","This graduation project has been executed in collaboration with the Norwegian company Redcord AS. Redcord designs and develops equipment for sling exercise training as well as treatment of musculoskeletal disorders (MSDs). Neurac, an evidence-based treatment approach, is at the core of the business. The active treatment method utilizes the Redcord equipment and one’s own body weight to restore proper neuromuscular control and functional stability. Redcord provides a course program in Neurac treatment developed for physiotherapists. Course participants who complete all steps qualify for the title “Certified Neurac Provider” (CNP). Greater patient involvement, few number of treatment sessions and less relapse make Neurac and the CNP therapists unique. The company aims to build up an international network of CNPs in order to spread the unique treatment concept, and help even more people worldwide. However, in 2014 there was a decrease in number of enrollments and an increase in the drop-out rates of participants. The organization seeks guidance on how to ensure engagement throughout the entire course program. The purpose of this project was to identify strategies on how to optimize the journey towards becoming a CNP. A second goal of this thesis was to prepare the company for a shift towards becoming more service-oriented. Both aspects are covered in the final design solution. The main focus areas in the customer research were to map out personal experiences, barriers and perceptions of successful and unsuccessful areas in the current course program. This was important in order to define requirements for a new and improved process. Six aspects causing low customer satisfaction were identified, hence (1) inaccessibe courses, (2) inadequate follow-up of participants, (3) lack of preparations in prior to a course, (4) the need of a clear goal, (5) confusing terminology, and (6) desire to share experiences. The main finding of the customer journey mapping proved to be that the individual training period in-between the offline courses is critical. Six guiding principles on how to safeguard and motivate the participants were made. The proposed design solution is an online training center that guides the participants through their individual training in a fun and engaging way. “Neurac EduCenter” inspires course participants and the organization to walk hand-in-hand throughout the program creating a holistic user experience. This thesis presents a service-led competitive strategy coupled to existing products to enhance customer relationships. Neurac EduCenter provides Redcord with an opportunity to sell monthly subscriptions to strengthen the company’s revenue stream. The concept serves as a foundation for Redcord upon a shift towards becoming more service-oriented.","Service Design; Branding; User Experience; Servitization; Healthcare; Education","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","Master of Science Strategic Product Design","",""
"uuid:10e1cc51-7af5-416b-9c3d-99f43b61b3ce","http://resolver.tudelft.nl/uuid:10e1cc51-7af5-416b-9c3d-99f43b61b3ce","TRADR Tactical Display System","De Graaf, M.C.; Sparreboom, J.","De Greeff, J. (mentor)","2015","This Bachelor Thesis took place as part of the Bachelor Computer Science at Delft University of Technology. This report describes a project issued by the Long-Term Human-Robot Teaming for Robot-Assisted Disaster Response (TRADR) consortium. During this project, the team worked on creating a user interface for use in human-robot teams during urban search and rescue missions and situation assessment. New innovations for use in search and rescue operations are being created to keep rescue workers safe during these operations. It also allows them to work more efficient, possibly saving more lives. One of these innovations is the use of robots to assess the situation at disaster areas. To work efficiently in robot-human teams, a clear overview of the data gathered should be available to the rescue workers. This project is aimed at creating a graphical user interface to give the rescue workers this overview. The Tactical Display System (TDS) is a user interface created using Python, RQT and QT. It consists of an overhead map of the disaster location using the virtual globe application Marble. This overhead map shows gathered and shared information in an efficient manner using a Point of Interest (POI) pinned on the map. Beside the overview map the user interface consists of several tools and informational toolbars to allow further elaboration on the shown points of interest. The development of the TDS was done in three phases. The first phase is defined as the research phase in which the system was designed and research has been done to make sure the provided solution is the best solution. Here it has been found that the user interface should be not distracting which is solved by the option to hide information and giving non-invasive messages. The second phase is defined as the development phase, where the program was created using the insights gathered from the research phase. The application is build up as set of plugins for modularity. The application, which is a plugin itself, has been built in a model-view-presenter architectural pattern. It has been made easy to add a new plugin for new developers in the project. The last phase is the concluding phase, in which the product created in the development phase was tested and documentation was created. All 'must have' requirements have been implemented. The code was tested by SIG, a company which performs static code analysis. The result is that the code is above average maintainable (a four out of five stars on SIG's star system), but there are not enough tests present. The result is a modular graphical user interface for use in urban search and rescue missions, which allows rescue workers to keep a clear overview, allowing them to work more efficiently in robot-human teams.","gui; search and rescue; tradr; tds","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Computer Science","","","",""
"uuid:ad52adbd-3856-406c-93d4-10fda3fc1b7b","http://resolver.tudelft.nl/uuid:ad52adbd-3856-406c-93d4-10fda3fc1b7b","Experience-enablers: A research on how different types of acquisitions can contribute to people’s subjective well-being","Claus, J.J.H.","Pohlmeyer, A.E. (mentor); Casais, M. (mentor)","2015","This thesis consists of two main parts: Part 1- A qualitative user study of ten interviews, referred to as Study 1, that is addressed in ‘Violins, ovens and hiking shoes: How experience enablers affect people’s subjective well-being in comparison to material- and experiential acquisitions’ Part 2- A quantitative user study of 209 participants who filled in an online survey, referred to as Study 2, is addressed in ‘Nuancing the experience recommendation: how do experiential, experience-enabling, material- and symbolic purchases differ in relation to well-being’. While Study 1 generated rich user data and aimed at increasing the understanding on the material-experiential spectrum, Study 2 quantified the hypothesis that resulted from Study 1 concerning the acquisition types and their relation to one’s happiness.","Experience-Recommendation; Design for Subjective Well-Being; experience-enablers; life experiences; material acquisitions; symbolic products","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","","",""
"uuid:bdb4e885-91d9-431f-9657-d1c51cb270fb","http://resolver.tudelft.nl/uuid:bdb4e885-91d9-431f-9657-d1c51cb270fb","Pricing options on gas under a regime-switching model","Van Tol, L.J.M.","Oosterlee, C.W. (mentor); Vuik, C. (mentor); Fokkink, R.J. (mentor); Van Elderen, E.M. (mentor)","2015","This thesis deals with pricing options on natural gas under a regime-switching model. First of all, a regime-switching model for natural gas is considered. Hereafter, historical gasdata are examined to find a model which fits the data. Next, a system of PDE's is derived in order to price an option under the regime-switching model. Finally, option prices are determined through Monte Carlo simulation. The influence of various parameters on the option price is also investigated.","option pricing; regime-switching; natural gas","nl","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Applied mathematics","","","",""
"uuid:e6571b30-0af9-491a-a24f-f7cac21744ba","http://resolver.tudelft.nl/uuid:e6571b30-0af9-491a-a24f-f7cac21744ba","A Novel Variable Stiffness Mechanism for Steerable Percutaneous Instruments","Oude Vrielink, T.J.C.","","2015","Percutaneous instruments are used in medical interventions to reach specific locations in the body by means of needle-puncture of the skin. The accurate positioning of these instruments in the body is challenging, influencing the efficacy of diagnoses and therapies. The positioning error can be reduced by the development of a steerable mechanism that can alter its stiffness. In a previous literature study, a steerable variable stiffness mechanism was selected. The aim of this research is to examine the stiffness this mechanism can achieve for the application of percutaneous instruments. Based on an iterative design and research process, small-scale prototypes as well as experiments for testing these prototypes have been developed to examine the different aspects influencing the stiffness of the manipulator. The results show that for an initial deflection the force exerted on the prototype must exceed a certain threshold. Also, this initial force, as well as the stiffness, can be controlled by the tension exerted on the wires that run through the mechanism. In comparison with a friction-locking mechanism, the developed prototype has lower threshold forces, but does not lose its stiffness after this force is exceeded. This indicates that the presently studied mechanism may be better suited for certain applications than the friction-locking mechanism. Furthermore, the insights gained in this study show that the mechanism can be optimized to increase the threshold force. Together with further literature, the present study gives reason to believe that the rigidity the current prototype can attain is sufficient for some specific percutaneous applications. The reduction of slip and the optimization for specific percutaneous instruments are recommended as further steps of development to expand the range of applications it can be used for.","minimal invasive surgery; percutaneous interventions; variable stiffness mechanism; steerable mechanisms","en","master thesis","","","","","","","","2017-09-02","Mechanical, Maritime and Materials Engineering","BioMechanical Engineering","","Bio","",""
"uuid:6e541dec-2d4a-4514-b521-a01ab709065f","http://resolver.tudelft.nl/uuid:6e541dec-2d4a-4514-b521-a01ab709065f","Deposition of high density tailings on beaches","Van de Ree, T.H.B.","Van Rhee, C. (mentor); Talmon, A.M. (mentor); Van Kesteren, W.G.M. (mentor); Chassagne, C. (mentor)","2015","One of the biggest uncertainties affecting the operations of mine and land reclamation activities is related to tailings / slurry management, for which the processes of beach deposition play a critical role. Deposited tailings produce both sheet and channelised flow, causing varying velocity profiles which lead to differential settlement. This is impacting slurry management operations (e.g. fines capture, capacity of a storage facility) as well as the mine closure and/or reclamation planning (e.g. strength of the deposit, differential consolidation, total settlement and reclamation topography). Even though critical, little is known about the physics of non-Newtonian slurry flow over beaches. In order to improve this knowledge, Deltares initiated a research program. This program starts with large scale experiments, aiming to mimic and understand the flow behavior observed in practice. This study is based on the large scale experiments at Deltares and consists of a literature survey, experiments and numerical modeling. In the experiments a mixture of clay, silt, sand and salty water is discharged over a 2% slope in which the composition is varying during the experiments. In the subsequent experiments water is added to the initial mixture causing a decrease in strength. Along with decreasing strength, the flow behavior changes from robust sheet flow to more dynamic channel flow. The change in flow behavior corresponds to a change in observed surface shear-profile. During (slow) sheet-flow, perpendicular and abrubt shear planes are observed wheras smooth and parabolic shear planes were observed during (fast) channel flow. Accompanying the change of strength, flow behaviour and shear mechanism, a shift from non-segregating to segregating slurry is observed. A hypothesis formulated in this thesis states that the observed surface shear profile is similar to the vertical shear profile, this could explain why the slurry is segregating or not. Rheology measurements were conducted with mixture used in the flume experiments. The measurements reveal rheopectic behavior with cumulative shear, after a certain time (added shear) the rheology reaches an equilibrium state (i.e. constant flowcurve). From these measurements an empirical function was developed relating the flow-curve to the mixture composition. Shear stress calculations based on these rheology measurements in combination with observations show a great agreement with the equilibrium bottom shear stress during the last part of the experiments. However during the first part the dynamic shear stress was exceeding the equilibrium bottom shear stress of the inclined flow, this is most likely caused by the developing rheology which was not yet at the equilibrium flow curve in the beginning of the experiments. After channel formation, cores were taken in both flowing and stagnant (or very slow flowing) parts. These cores were analysed on water content and particle size distribution. In addition vane (strength) measurements were conducted at nearby locations. A comparison between flowing and stagnant parts reveal no significant difference in water content, particle size distribution and strength. Though an equivalent variation (increasing strength and particle size, decreasing water content) was found (from top to bottom) over the height of the mud layer due to settling and consolidation of solids. From bathymetry measurements it was observed that the overall slope of the mud-line was steeper at the location of the flowing part than at the stagnant part. In addition the equilibrium slope was decreasing during the experiments caused by a decreasing strength. In previous research on flowing tailings, an analytical model(1DV) was developed aiming to predict the deposition of sand. The ingenuity of this model is a rheology-based viscosity profile which lead to a settling-velocity distribution over the height of the flowing layer. The 1DV model developed in this study assumes a similar (and constant) settling-velocity profile, however the problem is solved numerically. Hence it is possible to include more aspects like hindered settling, a multi-sized particle size distribution and morphological development. The input for the model is generated by flume-measurements in combination with the developed rheology function. The model predicts the average sedimentation measured during the flume experiments reasonably well. The experimental data was also compared with the original model (without hindered settling and with a mono-sized particle size distribution (d=2d50), it was found that the developed model performs slightly better than the original model.","tailings; non-Newtonian; segregation; rheology; experimental","en","master thesis","","","","","","","","2017-09-01","Mechanical, Maritime and Materials Engineering","Maritime and Transportation Technology","","Dredging Engineering","",""
"uuid:1f7f4393-59c7-4eb9-85d1-be30e8e71421","http://resolver.tudelft.nl/uuid:1f7f4393-59c7-4eb9-85d1-be30e8e71421","XPRS: The development of a fashion brand for hip-hop dancers","Smits, C.","Mugge, R. (mentor); Calabretta, G. (mentor)","2015","In this graduation project a brand was designed from scratch. The goal of the project was to develop a fashion brand for male hip-hop dancers that addresses values that currently are not being addressed by the men’s fashion industry. The focus of the designed brand is on helping hip-hop dancers being free in their expression, thinking and movement and on bringing them together.","brand design; entrepreneurship; fashion; hiphop","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","Master of Science Strategic Product Design","",""
"uuid:296239b2-a85e-4124-894e-009a7d3252c5","http://resolver.tudelft.nl/uuid:296239b2-a85e-4124-894e-009a7d3252c5","Reliability modelling in transmission networks: An exploratory study for further EHV underground cabling in the Netherlands","Kandalepa, N.","Van der Meijden, M.A.M.M. (mentor); Rueda Torres, J.L. (mentor); Tuinema, B.W. (mentor); Kuik, G.R. (mentor)","2015","The worldwide demand for electricity, which is steadily increasing, leads to a continuous need for developing and extending the transmission networks. However, the installation of new overhead lines faces many challenges due to social, environmental and technical reasons. One solution that gains widespread public support is the installation of extra high voltage AC XLPE underground cable systems. Although this development is quite encouraging from a social perspective, new problems and challenges might arise, both from a technical and economical perspective. One significant area of concern is the reliability of underground cable systems. The current experience with EHV cables shows that although the cable failure frequency is very close to that of the overhead line, the additional components of the underground cable system (joints and terminations) reduce the reliability of the whole system. In addition, compared to overhead lines the EHV cables appear to have significantly longer repair times. The main goal of this thesis is to develop a reliability assessment approach in order to examine how the installation of EHV underground cables in transmission networks impacts the overall reliability level. First, an extensive literature review on basic reliability concepts and on main differences between OHL and UGC is conducted. In order to enrich the insight gained from this literature review, unstructured interviews took place with experts from the Dutch transmission system operator. By using the findings from both sources, a qualitative generic approach is proposed. This approach can be used as a framework for the decision making process that a planner should follow when he/she examines the possibility of installing EHV UGC in a transmission network from a reliability point of view. Different steps are included, from what input data are necessary until how to process and evaluate the results (reliability indicators) from the reliability assessment. The approach is used for the case study of the EHV Dutch transmission grid. A contingency analysis regarding failures of 380kV transmission overhead lines and underground cables is performed by using two methods: State Enumeration and Monte Carlo simulation. If a contingency leads to overloaded connections, remedial actions (such as generator re-dispatch within the Netherlands, cross-border re-dispatch and load shedding) are applied in order to relieve the overload. Then, several reliability indicators are calculated and the reliability level of the network is determined. A sensitivity analysis in the repair time and the failure frequency of EHV cables is conducted and the simulations are performed for different cable lengths and varying number of cable sections as well.","EHV cables; reliability; remedial actions; key performance indicators; repair time; contingency","en","master thesis","","","","","","","","2015-09-30","Electrical Engineering, Mathematics and Computer Science","Electrical Power Engineering","","MSc Electrical Power Engineering","",""
"uuid:8aae8131-bb83-42a5-ad2b-2781f70c1d36","http://resolver.tudelft.nl/uuid:8aae8131-bb83-42a5-ad2b-2781f70c1d36","Bali Beach Project - an evaluation of the coastal structures in Bali, Indonesia","Arntz, B.J.M.; Kusters, J.F.; Knoppe, K.I.; Huisman, R.R.; Ringoir, V.H.","","2015","","Erosion; Bali; Beach; Candidasa; Indonesia","en","student report","TU Delft, sections Construction Management & Engineering, Offshore and Dredging Engineering","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","","","-8.409518, 115.188919"
"uuid:ccb56154-0b70-4a41-8223-24b0f8d145c5","http://resolver.tudelft.nl/uuid:ccb56154-0b70-4a41-8223-24b0f8d145c5","An Investigation of the Non-Linear 3D Flow Effects Relevant for Leading Edge Inflatable Kites","Deaves, M.E.","Schmehl, R. (mentor); Gaunaa, M. (mentor); Gillebaart, T. (mentor)","2015","The kite power group at TU Delft is currently researching the use of leading edge inflatable (LEI) kites for use in power generation. A thorough understanding of the aeroelastics of these kites is paramount to the development of system simulation models and optimum kite and system designs. The current lack of understanding is therefore seen as a roadblock to the development of a commercially viable kite power system. The aeroelastics of LEI kites are complicated by three main challenges.  There is a high degree of coupling between the flexible kite and the aerodynamic loading. This means that a fluid-structure interaction approach is typically needed to produce accurate simulation results.  The low aspect ratio and large anhedral of the kite means that 3D effects are significant.  During normal power production it is desirable to fly the kite at high angles of attack where significant non-linear viscous phenomena (e.g. flow seperation) are known to occur. In order to model correctly the 3D viscous aerodynamic phenomena present in LEI kite flight a computational approach utilizing a steady-state Reynolds-Average-Navier-Stokes (RANS) solver has been suggested. This work presents a review of relevant literature, outlines the computational approach taken, and discusses the limitations and computational costs of the approach. It was found that the RANS approach is able to model the kite’s flow environment up to angles of attack of 24?. At angles larger than this significant flow separation from the suction surface of the kite precludes the use of a steady-state solver. At angles as low as 18? significant non-linear effects begin to take effect, decreasing lift and increasing drag. It was also found that at lower angles of attack separation from behind the leading edge tube serves to decrease effective camber and therefore lift. The computational cost of the approach is heavily influenced by the quality of the mesh generated, in particular the presence of non-orthogonal cells. It is concluded that the RANS approach is capable of quantifying well the non-linear flow effects of LEI kites at moderately high angles of attack. The challenge of this method in the future will be to decrease it’s significant computational costs so that it may be used in the context of systems modeling, optimization, or fluid-structure interaction.","Windenergy; kite power; aerodynamics; wind energy; RANS; CFD","en","master thesis","","","","","","","","","Aerospace Engineering","Wind Energy","","European Wind Energy Master","",""
"uuid:5d45bce9-ef29-459c-a659-417342a875e9","http://resolver.tudelft.nl/uuid:5d45bce9-ef29-459c-a659-417342a875e9","Incident, infragravity and very low frequency wave motions on an atoll reef platform","Gawehn, M.A.","Stive, M.J.F. (mentor); Van Dongeren, A.R. (mentor); Van Rooijen, A.A. (mentor); Storlazzi, C.D. (mentor); Zijlema, M. (mentor)","2015","Coral reefs are hard structures that front many coasts in tropical and subtropical climates and protect them against wave attack and erosion. Despite reducing incoming wave energy by up to 98%, coral reefs are not a guarantee that mainland or island coasts are safe from being flooded. This was demonstrated by a series of wave-driven flooding events in 2008, that caused widespread damage to infrastructure and freshwater resources at islands in the western Pacific Ocean (e.g. Micronesia, the Marshall Islands, Kiribati, Papua New Guinea and the Solomon Islands). In particular atoll islands were exposed as they are only 2-5m above sea level. Atolls are typically ring shaped and enclose a lagoon in the center. The actual landmass is small and therefore offers little space for rainwater to accumulate under the subsurface and form a fresh water lens. Coastal managers are concerned that future overwash events may get more frequent due to climate change related sea levels rise and thereby cause permanent salinification of the natural fresh water sources. The United States Geological Survey (USGS), the National Oceanographic and Atmospheric Administration (NOAA) and the University of Hawaii jointly initiated an investigation on the processes that are involved in the flooding of atoll islands. Eventually, research aims at the prediction of hazardous flooding events by utilizing computer models. Plenty of nearshore processes have been described for reef systems in general, however, few studies have been devoted to the understanding of wave run-up. For sandy beaches the latter is typically a combination of wave induced set-up, incident short wave and incident infragravity swash. In case of coral reefs the physics behind wave run-up can get far more complex. On reefs with a steep reef face and long shallow flats, strong amplification or even resonance of low frequency harmonics have been observed, both of which are likely to increase surf beat. During a field experiment at the Kwajalein atoll between 3-Nov-2013 and 13-Apr-2014, hydrodynamic data were collected by the USGS to study run-up at a typical atoll reef with a steep fore reef (1:18) and a long horizontal reef flat. In previous research these data have been used to force one dimensional XBeach models and to subsequently validate the model performance with respect to set-up, short and long wave heights and wave run-up on the beach. The models performed well with exception of the reproduced wave run-up that was systematically underestimated. The current study continued on this subject, i.e. investigating wave run-up at Kwajalein atoll. The report was divided into two parts. For the first part, subharmonic wave motions were analyzed using the available field data. For the second part wave run-up was reexamined with one dimensional (non-hydrostatic and surfbeat) XBeach models. More specifically, the purpose of the data analysis was to get a detailed description of the (long) wave hydrodynamics and to find out how major run-up events distinguish themselves from the ordinary situation. Focal points were: Generation mechanism associated with subharmonic motions across the reef site and the amplification of very low frequencies (VLF) on the reef flat. Subsequently, 1D-XBeach modelling plugged in on the open question whether run-up would eventually be predictable by XBeach. Roi-Namur was shown to be sensitive to wave climates with long peak periods, that generally induced strong VLF amplification across the reef flat. Concurrently, fundamental resonant periods were strikingly similar to the most energetic VLFs observed on the inner reef flat. Tidal differences also impacted the combined low frequency (LF) energy inshore as the response was larger during high tide. Instead of bound long wave release, evidence was found that free long waves were generated by a moving breakpoint. This was also confirmed by high values of the relative bedslope parameter. Further investigation of the breakpoint mechanism moreover suggested that the relative effectiveness of generating infragravity (IG) waves varied for different wave conditions, which was explained by changes in the length of the breakpoint excursion. It was found that non-hydrostatic models used in the first Kwajalein study underrated wave induced set-up because the mean sea level (MSL) was imposed 0.5 m too low on the model boundary. Taking this water level offset into account, the performance of XBeach-Surfbeat (XB-SB) was compared to XBeach-Non-hydrostatic (XB-NH). The former captured enough physics to compete with the non-hydrostatic XBeach mode in terms of the representation of wave induced set-up, short wave heights and long wave heights. However, XB-SB fell short on the prediction of run-up, which is a crucial weakness since the models are ultimately meant to estimate just that. Furthermore, two distinct ways of boundary forcing were tried, i.e. with measured spectra or idealized JONSWAP spectra. Bulk LF energy was best reproduced by models that were forced with measured spectra. Onset of idealized JONSWAP spectra introduced erroneously high LF amplitudes at the fore reef. This problem could be improved upon by changing the value of the peak enhancement factor. Run-up heights were well reproduced by non-hydrostatic simulations, in contrast to simulations with XB-SB that underestimated them. It was reasoned that incident short wave swash in XB-NH was key to better run-up predictions. Combining the findings of the data and model analyses it could be concluded that both LF waves as well as nonlinear solitary waves significantly contribute to wave run-up on the beach.","infragravity; very low frequency; run-up; atoll; reef; XBeach; model; low frequency; swash; coral; Kwajalein; cross correlation; surf beat","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Hydraulic Engineering","","9.1898235, 167.4242972"
