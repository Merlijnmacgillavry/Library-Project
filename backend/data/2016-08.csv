"uuid","repository link","title","author","contributor","publication year","abstract","subject topic","language","publication type","publisher","isbn","issn","patent","patent status","bibliographic note","access restriction","embargo date","faculty","department","research group","programme","project","coordinates"
"uuid:67fa3d90-a158-43b2-806e-2101a7d9a2c4","http://resolver.tudelft.nl/uuid:67fa3d90-a158-43b2-806e-2101a7d9a2c4","Applying component models in participatory modelling: a trade-off between model ownership and process efficiency: Assessment of a process designed to explore innovations","Dankers, F.","Klijn, F. (mentor)","2016","Scientific article: Participation and model ownership in group model building processes – an exploratory study based on expert interviews. Abstract: Participatory modelling is an approach that is widely applied in policy making. Participation enhances the development of ownership of the model and thus commitment to the process outcomes, but also demands more time for the process. To organize participation effectively, it is relevant to know what the contribution of participation is to the process purposes. Where other studies focus on classification of participation, this research explored the effects of participation. This was done by combining literature research with consultation of various experts on the expected effects of different forms of participant involvement. The results show what degree of model ownership can be expected to develop under different forms of involvement. Furthermore, the broader relevance of participation and model ownership to participatory modelling processes is discussed. It is recommended that future research focuses on empirical validation of this research.","participatory modelling; group model building; participation; model ownership; expert interviews","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi Actor Systems","","Systems Engineering, Policy Analysis and Management","",""
"uuid:e5f90dec-7e32-411c-a228-7210c6be2ea6","http://resolver.tudelft.nl/uuid:e5f90dec-7e32-411c-a228-7210c6be2ea6","Market risk calculations in stock- and bond prices: a garch-copula approach","Pries, H.","Cirillo, P. (mentor)","2016","The financial crisis of $2008$-$2009$ has led to more strict regulatory supervisory on banks and insurance companies, focusing on better (market) risk models. The linear correlation models did not foresee the extreme losses in asset values, because they were not able to forecast high volatile markets in which the dependence between financial assets seemed to increase. Research on copula theory, a tool to model more advanced dependence structures, predominately analyses the effect of the copula on highly dependent stock indexes, often using one underlying simulation model. This study compares two slightly dependent equity and bond prices for four different combinations of univariate simulation models, including Black-Scholes, Hull-White, GARCH and different residual models, three different copula models (Gaussian, Student $t$ and flipped Gumbel) and two calibration methods. The goal is to measure the effect of different copula dependence models in economic scenario generators in combination with different simulation models and compare results in terms of accuracy, stability, resilience and complexity. The main result is that due to the little dependence the impact of the copula model is limited and dependence in the copulas is small compared to the estimations based on stressed markets. Hence, for these low dependent portfolios the copulas do not have much added value. Remarkable is that the dependence implied by the copula can strongly depend on the underlying model. The estimated dependence parameters of the copulas are lower for models using a GARCH volatility model. This can be explained by the non independent identically distributed residuals in the model without GARCH, i.e. the high volatile market periods lead to volatility clustering in the residuals. If this volatility clustering is not captured by the model, it can lead to amplification of the dependence due to misfitting of the univariate simulation models. The differences in risk estimations are mainly caused by the choice of simulation models. The GARCH volatility model leads to an increase in the calculated market risk at a horizon of one year. The choice of residual model has large impact on the risk calculations in one day and (depending on the strength of the dependence) can have both negative and positive impact on the risk calculations in one year and the choice of simulation models should therefore be chosen with care.","expected shortfall; copula; garch; market risk; value at risk","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Mathematics","","","",""
"uuid:1fc1e0b9-743e-42f2-81ab-ca3ed10f4d76","http://resolver.tudelft.nl/uuid:1fc1e0b9-743e-42f2-81ab-ca3ed10f4d76","Seismic depth imaging of sea-level controlled depositional sequences at the New Jersey shelf","Asshoff, K.","Reiche, S. (mentor); Buske, S. (mentor)","2016","This master thesis is part of a joint project carried out at TU Bergakademie Freiberg and RWTH Aachen University, aspiring to simulate numerically the fresh water emplacement and recharge system at the New Jersey shelf. Analyses of data, acquired during the most recent seismic drilling campaign IODP Leg 313 at the New Jersey shelf, revealed complex salinity variations with depth (Lo et al., 2013). Additionally, fresh water reservoirs with a salinity lower than 3 g/L have been discovered down to 400 m depth below the sea floor and up to 130 km far-off the New Jersey coast (Mountain et al., 2010; Post et al., 2013). The origin of the freshwater distribution so far-off the New Jersey coast is still unknown, leading to an ongoing debate over two conflicting hypotheses. It is the main objective of the joint research, to achieve a better understanding of these vertical salinity variations and the controlling factors of groundwater circulation at the NJ shelf. Re-processing and depth imaging of the available seismic data are necessary for deriving a reliable 3D subsurface model to be further applied for numerical flow and transport simulations. During this master thesis, the seismic profile line Oc270_029 was primarily re-processed in the time domain. Following the steps of pre-processing, a Kirchhoff pre-stack time migration approach was executed, resulting in an enhanced time-migrated image. On the base of this time-migrated section, horizon interpretation was performed to proceed with the depth imaging of the seismic data. As a first step, an interval velocity model was derived as a function of depth by standard semblance analysis of the seismic data. This velocity model was used in a Kirchhoff pre-stack depth migration, resulting in a first subsurface model. Subsequently, the velocity model was refined by horizon- and grid based tomography approaches and an updated Kirchhoff prestack depth migration result was calculated using this new velocity models. Interpretation of previously identied sequences by Miller et al. (2013a) on the depth-migrated stack of Oc270_029 is concluding this thesis.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","IDEA League Joint Master's in Applied Geophysics","",""
"uuid:31f783bc-ca78-4fb2-bf45-18452dd8139d","http://resolver.tudelft.nl/uuid:31f783bc-ca78-4fb2-bf45-18452dd8139d","Hysteresis Compensation in Piezo-Actuator","Sinha, R.K.","Hossein Nia Kani, S.H. (mentor)","2016","To meet the ever increasing demands for the linearization of the nonlinear plant various techniques have been proposed. Out of the possible approaches disturbance observer based techniques is very popular in industrial application due to its simple implementation and high accuracy. However, the linear disturbance observer based technique is limited upto a certain frequency which in-turn is defined by the fundamental limitation of the linear control techniques. In this work a novel generalization of second order reset element is proposed with application in improving the performance of the disturbance observer (DOB) based compensation scheme. To this end, two new configurations are proposed for the improvement of the linear DOB. In the first configuration a new nonlinear low-passing filter has been proposed with similar gain as a linear filter but with much less phase lag which results in improved performance of the linear DOB. In the second configuration a new implementation of the combination of the linear DOB with the reset elements is proposed which results in minimization of sensitivity of the complete closed loop. Eventually, the effectiveness of the proposed architectures is proven on a practical setup of a PiezoActuator, which is well known to have a dominant hysteretic nonlinearity in voltage to displacement relation. The disturbance observer based compensation scheme has wide domain of applications extending from input/output disturbance rejection, eliminating unmodelled dynamics to the compensation of bounded nonlinearities. So, with the proposed improvement to the linear DOB the performance of a number of application where DOBs are currently being used, can be improved and at the same time new application areas for DOB also opens up.","","en","master thesis","","","","","","","","2017-08-31","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering (PME)","","","",""
"uuid:a0293a3b-141c-4ae7-a53a-3f079b30f202","http://resolver.tudelft.nl/uuid:a0293a3b-141c-4ae7-a53a-3f079b30f202","Studying the Effect of Flooding Water Salinity on Two-phase Flow in Porous Media through Pore-scale Modelling","Zheng, L.","Hiller, T. (mentor)","2016","The wetting behaviour of fl uids in porous media has a great impact on oil recovery and its understanding is of vital importance in reservoir modelling. This work adopts a finite volume direct simulation approach to investigate how the change of ooding water salinity, by means of altering the contact angle, affects the wettability hence oil recovery. Two-phase fl ow simulations are performed on a Ketton carbonate for primary drainage followed by water flooding at various contact angles. The results show that as the contact angle is increased from 0º to 47º, water layers in the rock crevices trigger fewer oil snap-offs and hence there is less trapping. When the contact angle is set to change over time, a reduction in contact angle/salinity during reduces the degree of trapping. This low salinity effect becomes more prominent as the contact angle difference between the fl ooding and connate water grows. The results are compared with network model predictions and experimental data. In terms of the practical constraints, the coarse mesh grid setting for speeding up the simulations may jeopardise the quality of the predictions of direct simulations but performing direct simulations on a fine mesh over a volume that guarantees macroscopic representativeness is found to be impracticable. Network models, on the other hand, can be run on larger samples, but their accuracy may suffer from poor representation of the pore geometry. To overcome this dilemma, a two-stage simulation approach is proposed where the pore-scale accurate snap-off pressures are obtained from high-resolution direct simulations. By tuning the pore-network model accordingly, its accuracy can be improved and has the potential to be used as an effective upscaling tool to predict macroscopic flow properties.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","","",""
"uuid:bbc39c2e-7c99-4e1f-8481-dc43d1b61719","http://resolver.tudelft.nl/uuid:bbc39c2e-7c99-4e1f-8481-dc43d1b61719","Stimulating elderly in care homes to move using Tovertafel games","van Adrichem, N.","van der Cammen, T.J.M. (mentor); Goossens, R.H.M. (mentor)","2016","As the pressure on elderly care is strong, preventive policy for health care is therefore essential. The goal of prevention for elderly is to keep older adults healthy, autonomous and independent as long as possible, even when they already need daily care. A preventive measure which is increasingly gaining popularity over the past few years is physical exercising. Physical activity can help reduce progression of diseases, delays emerging disabilities, has a positive influence on cognitive functioning, increases emotional well-being, and reduces the risk of loss of independence. This results in an overall better quality of life in nursing homes. Unfortunately, as the required level of care needed for elderly people in nursing homes is relatively high compared to those who could still live at home, preventive exercising is not always seen as essential in the residential setting yet. Moreover, due to factors as pain, changing attitude, fear of falling, lack of availability and care staff taking over activities, inactivity problem in nursing homes is increasing. Different types of preventive physical activities, motivators, barriers and role of stakeholders are explored during the context research phase of the project. Exergames – interactive computer games which require physical activity in order to play - were found to be effective means to stimulate elderly people in nursing home to become physically active. Well-designed games can increase intrinsic motivation as they stimulate discovery, enable the player to intuitively able operate tasks, fit interests of the player, enable interaction with others, and provide an appropriate level of challenge. Related to motivation of playing (computer) games and to exercise adherence, well-designed exergames could get the users into a state of flow. Furthermore, context research revealed that older adults are motivated by social interactions and group play, so multi-player exergames can be positively influencing the play experience and adherence. Although several existing exergames are specifically for the elderly target group, opportunities were found to design an exergame based on Tovertafel technology. From the observations and interviews during the context research, it became clear that one of the most valuable things for elderly people in terms of physical well-being is to maintain as much functional mobility as possible. Functional mobility refers to a person’s ability to carry his own weight while standing, make transfers and move independently in order to perform personal care and Activities of Daily Living. Based on context research insights, an exergame focusing on lower extremity activity was designed with the following interaction qualities in mind: immersive, experience-focused, social, skill-independent, pleasurable. The ideation phase was focused on generating and testing inspiring game ideas which could be further developed by going through a few quick iterations. The final result of the ideation phase was a single evaluated game concept of a music game, which had most potential to stimulate players to maintain functional mobility in an enjoyable way, while meeting the design goal, interaction vision and requirements. In collaboration with physio therapists, programmers, users and music-therapist the exergame concept has been refined in the conceptualisation phase of the project. Parts of this concept have been developed into interactive prototypes for user testing purposes. Although game design should be seen as a coherent whole, the serious game design framework of (Mitgutsch and Alvarado, 2012) offered a basis to structure all game elements (framing, narrative, content, aesthetics, mechanics) in relation to the game purpose of stimulating elderly people to become physically active in order to maintain mobility. This framework was used to iterate on all game elements and to guide the play-tests in the evaluation phase. The final concept can be described as a rhythm game in which players need to stamp on the glowing cues, in order to turn on an instrument tracks of the song. The players cooperate to hear all instruments, thus when all play well, the complete song is the ultimate result. The difficulty of the glowing patterns(which are following the beat) is dynamically adjusted based on the performance of the individual player. This challenges all players at their own level of physical and cognitive abilities. The evaluation of the design goal and interaction vision showed that the game stimulates elderly people to move their legs, while all interaction qualities are found in the play experience.","movement; motivation; elderly; nursing homes; lower extremity; inactivity; Tovertafel; Active cues; physical; activity","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","","",""
"uuid:4026488c-ac10-48d0-853f-a5da1698fd5e","http://resolver.tudelft.nl/uuid:4026488c-ac10-48d0-853f-a5da1698fd5e","Flexible Platform as a Service adoption in business: a capability framework for supporting interoperable and portable cloud solutions","Vlas, T.E.","Cunningham, S. (mentor); Janssen, M.F.W.H.A. (mentor); Werker, C. (mentor); Noltes, J. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi Actor Systems","","Management of Technology","",""
"uuid:1d4fcebb-6a0f-4d47-924c-52213fc97ce4","http://resolver.tudelft.nl/uuid:1d4fcebb-6a0f-4d47-924c-52213fc97ce4","Joint inversion of shear-wave velocity azimuthal anisotropy from surface and body waves","Kühnert, J.B.","Boiero, D. (mentor); Bagaini, C. (mentor); Manukyan, E. (mentor)","2016","The phenomenon of shear-wave splitting (SWS), in which the shear-wave (S-wave) splits into a fast traveling and a slow traveling component of different polarization, is the key indicator for shear-wave velocity anisotropy. Despite conveying valuable subsurface information, the phenomenon potentially degrades the seismic image quality. For these reasons, after being investigated predominantly in fields of global seismology, the analysis and compensation of SWS forms now a routine step for S-wave processing in refection seismology. When a S-wave source is not available, the orientation of the fast and the slow axis of the anisotropic medium and the magnitude of the velocity variation can be determined by analyzing the azimuthal variations of refection events from compressional- to shear-wave (PS-) converted waves. However, PS-waves can not be consulted for anisotropy information in the near-surface due to low coverage and contamination from coherent noise. Here, surface waves can provide the missing information by analyzing the azimuthally varying phase velocities. As the body waves and the surface waves are sensitive to the same anisotropic properties, their data can elegantly be combined and inverted. The performance of the inversion with combined data in determining the subsurface anisotropic properties is superior to a sole body wave inversion due to the complementarity of the to wave types. This is highlighted when using the inversion results for the compensation of the effect of SWS. Through the incorporation of the surface waves, the near surface can be compensated correctly which consequently leads to an enhanced compensation beneath.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","IDEA League Joint Masters in Applied Geophysics","","51.1576094, -0.1520318"
"uuid:b9283dd5-5aa7-4db0-89b4-5e0087a85f3a","http://resolver.tudelft.nl/uuid:b9283dd5-5aa7-4db0-89b4-5e0087a85f3a","Pioneering start-ups' first mover advantages and success factors in B2B service markets","Van den Brink, W.","Van Geenhuizen, M.S. (mentor); Roosenboom-Kwee, Z. (mentor); Enserink, B. (mentor)","2016","Less attention has been given to first mover or pioneer advantages (FMA) for services than for physical products. Likewise, less attention has been given to FMA for business-to-business (B2B) markets than for business-to-customer markets. This research investigates FMA for B2B services using a grounded theory approach in existing academic knowledge, and by empirical analysis of university start-ups. Empirical analysis is performed through statistical analysis and Rough Set Analysis: a machine learning approach that induces decision rules for classification. Several patterns are evident from the data and a tool is introduced that can aid interested actors in assessing a start-up's performance based on several of its characteristics.","B2B; first mover; FMA; pioneering; start-up; spin-off; Rough Set Analysis","en","master thesis","","","","","","","","","Technology, Policy and Management","Values Technology and Innovation","","Management of Technology","",""
"uuid:16006009-4d26-47a4-9886-2162b517e3e8","http://resolver.tudelft.nl/uuid:16006009-4d26-47a4-9886-2162b517e3e8","Frequency-dependent reflection response of a laterally varying fracture, represented as a nonwelded interface","Osukuku, G.A.","Minato, S. (mentor); Ghose, R. (mentor)","2016","Displacement across the interface is not continuous in the case of a nonwelded interface. The stiffness of a nonwelded interface is a function of fracture properties, e.g., roughness, asperities, aperture and fracture infill. In this research, the fracture compliance has been estimated by AVO inversion of P-P reflected waves from a nonwelded interface. Laboratory experiments were carried out in order to measure the reflection response of dry and wet fractures. CMP data were acquired for homogeneous and heterogeneous fluid-fillled fractures. The objectives of the research are: 1) to capture in the laboratory and study the reflection response of a laterally varying heterogeneous fracture, 2) to investigate the effectiveness of linear slip theory in predicting the AVO response of a heterogeneous fracture and estimate the fracture properties, 3) to observe in laboratory and utilize in characterization the scattered seismic waves from a laterally varying heterogeneous fracture, and 4) to utilize the time-lapse seismic reflection response for monitoring a fluid-filled laterally heterogeneous fracture. AVO inversion was carried out to obtain the fracture compliances. In order to illuminate a fracture which is significantly thinner than the predominant seismic wavelength, one promising approach is to incorporate the linear slip theory in the AVO inversion. This has been successfully implemented in this research. As a result, we could characterize a fracture which is nearly 65 times thinner than the seismic wavelength that we availed in our laboratory experiments. Fracture compliance values obtained from AVO inversion coupled with the linear slip theory can distinguish between wet and dry surfaces within a fracture. The obtained values of fracture compliance show the reliability of the AVO inversion in characterizing heterogeneous fractures. The obtained P-P reflection coefficients for each CMP gather closely match the expected values calculated from the linear slip theory. The value of the average compliance for the fluid-filled fracture in our experiment, estimated from AVO inversion, is 7:4x10^-14m/Pa. This value is slightly higher than the theoretical value of 6:4x10^-14m/Pa. This difference can be explained by the presence of minute quantity of air bubbles that got trapped in the infill fluid in the fracture during our experiment. We calculated that the volume of trapped air that can cause this deviation is 0.001%, which agrees with earlier observations. The AVO inversion of a fluid-filled nonwelded interface can be successfully used to monitor the fractured reservoir properties, using the dry fracture response as a reference. The scattered seismic waves from a heterogeneous fracture, observed for the first time in laboratory, show that these events affect the primary reflections, and therefore, there is a need for inversion that takes into consideration both specular and nonspecular reflection events.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","IDEA League Applied Geophysics","",""
"uuid:d2ad3627-3b6e-4707-95e5-962353e124b6","http://resolver.tudelft.nl/uuid:d2ad3627-3b6e-4707-95e5-962353e124b6","Space-filling Curves Heuristics for the 4D Travelling Salesman Problem in Chip Manufacturing Machines","Swennenhuis, C.M.F.","Aardal, K.I. (mentor)","2016","","","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","","",""
"uuid:5ea21702-d6fb-484c-8fbf-15c5b8563ff1","http://resolver.tudelft.nl/uuid:5ea21702-d6fb-484c-8fbf-15c5b8563ff1","Characterization of CMOS Image Sensor","Jain, U.","Theuwissen, A.J.P. (mentor); Uwaerts, D. (mentor)","2016","CMOS image sensors comprise of two processes: designing and measurement/testing. They are designed with certain characteristic performance and it is important to measure these characteristics accurately. CMOS image sensor converts light information into digital information which can be reproduced in the form of an image. Conventional 4T pixel with a pinned photodiode is a popular choice for designing image sensors; with certain modification in the pixel architecture better characteristic performance can be achieved with trade-offs. Quantum efficiency, linearity, full-well capacity, conversion gain, noise, non-uniformity, dark current, modulation transfer function and image lag are the main characterises of a CMOS image sensor. The quantum efficiency defines the photon-electron conversion and collection efficiency of the image sensor which ideally should be 100 percent i.e. 1 electron-hole pair for every photon incident with linear photo response. Higher full-well capacity means more number of electrons that can be stored. Conversion gain tells the image sensors ability to convert the electrons generated into a voltage, higher the better. Noise sets the dynamic range of the image sensor by defining the lower limit of signal level, continuous advances have been made to reduce the noise and some image sensors can achieve noise level of 1e-. Modulation transfer function defines the spatial resolution and contrast relation of the image sensor, which is also the measure of crosstalk of the image sensor. Another characteristic which is more of an advantage over CCD image sensors is the image lag which is also known as memory effect; defines the image sensors ability to completely transfer the charge from photodiode to floating diffusion. These characteristic parameters define the CMOS image sensor and having a standard measurement procedure for computing this characteristic is necessary. This report presents the standard measurement procedure to characterize CMOS image sensors. This project is an internal project for Caeleste CVBA, Mechelen, Belgium, hence the measurement procedures are more specific to Caeleste requirement and follows EMVA 1288 standards. Measurement procedure includes all the details to evaluate the characteristic parameter: measurement background, block diagram, procedure and data processing are some of the key elements of the procedures. At Caeleste different methods are performed to compute a characteristic parameter accurately and precisely. Also, the software library and hardware tools were updated for improving the measurement accuracy and speed.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Microelectronics","","","",""
"uuid:3b14e941-f1b3-40a0-95d4-35e7fae5b578","http://resolver.tudelft.nl/uuid:3b14e941-f1b3-40a0-95d4-35e7fae5b578","Scaled agile maturity model","Chandrasekaran, R.M.","Janssen, M.F.W.H.A. (mentor); Ubacht, J. (mentor); Warnier, M.E. (mentor)","2016","In today’s world agile software development has been embraced more and more in software service industry. Though the agile practices have gained widespread popularity in the recent years, there are quite a number of concerns in scaling the agile practices from team level to the entire enterprise. Few frameworks such as Scaled Agile Framework (SAFe, Disciplined Agile Delivery (DAD) and Large Scale Scrum (LeSS) have been developed to address these concerns in scaling agile practices. Although these frameworks provide a template in scaling agile in large enterprises, currently there is a lack of a holistic method which would help them in implementing scaled agile practices or adapting to scaled agile software development. Before or after adopting such a framework, organizations require a structured model for assessing the level of completeness of adoption or find areas of improvements in the scaled agile practices, which would also help them in developing a roadmap for further progress and initiatives. This research offer guidance for the IT organizations towards scaled agile software development by providing a maturity model. This maturity model is composed of six stages as rows and five scaled agile principles in columns, in which each stage and column forms a matrix of scaled agile practices. Each of the practices consists of indicators which help in assessing the level of adoption of the practices. Once having identified the lacking criteria in the adoption of scaled agile practices, organizations can start focusing on the lacking criteria and other areas of improvement. The research also strongly suggests the collaborative spirit in adaptation of scaled agile practices, by using this model as a discussion tool in the team, program and portfolio levels of scaled agile environment. Future research in this arena would aim at researching on the dynamics of emergence on scaled agile practices and the notion of such an emergence on multitude of stakeholders involved in a scaled agile process.","scaled agile framework; agile software development; maturity model; scaled agile practices; ambidexterity","en","master thesis","","","","","","","","2017-08-17","Technology, Policy and Management","System engineering policy analysis and management","","","",""
"uuid:e86a17c5-9598-46ae-9e98-d4901c8f6d86","http://resolver.tudelft.nl/uuid:e86a17c5-9598-46ae-9e98-d4901c8f6d86","Core orientation and seismic anisotropy investigations around the COSC-1 borehole","Bienstein, L.M.","Jublin, C. (mentor); Schmelzbach, C. (mentor)","2016","The COSC-1 drilling project in central Sweden resulted in a 2.5 km deep cored borehole with a core recovery of almost 100 % and was primarily supported by the International Continental Scientic Drilling Program (ICDP) and the Swedish Research Council. With the goal to understand the mountain building processes in the Scandinavian Caledonides a wide range of geophysical and geological investigations have been and are still being performed within this project. This study focuses on core orientation utilising acoustic televiewer data. Subsequent seismic anisotropy investigations are based on Finite Difference modelling. The geological background model is built according to the lithological drill core description and modelled seismic properties. The main objectives were utilizing small-scale seismic measurements conducted on drill core samples and comparing the resulting synthetically modelled velocity profiles to large-scale field measurements.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","IDEA League program in applied geophysics","",""
"uuid:d068455d-f8c1-4767-935e-940684ec94f6","http://resolver.tudelft.nl/uuid:d068455d-f8c1-4767-935e-940684ec94f6","Data-driven 3D Deghosting Using Multisensor Marine Measurements","Koene, E.F.M.","Kamil Amin, Y.I. (mentor); Caprioli, P. (mentor); Vassallo, M. (mentor); Van Manen, D.J. (mentor)","2016","Marine seismic acquisition tows submerged streamers to record pressure waves from the subsurface. The recording, however, contains both the desired upgoing wavefield and its (immediate) reflection off the sea-surface, causing a downgoing wavefield known as the seismic ghost. Interference between the up- and downgoing waves causes periodic low signal to noise ratio (S/N) ‘ghost notches’ in the recorded spectrum. To restore the broadband upgoing signal, we must remove the ghost (‘deghosting’). Deghosting using solely pressure measurements fails to restore the signal in the low S/N notches of the data. Current acquisition techniques acquire signals with different ghost notches, such that their proper combination recovers the broadband signal. This thesis uses multisensor acquisition: measurements of the pressure and particle velocity vector. The ghost notches on the pressure and vertical particle velocity are offset by half a period, such that their combination may provide good S/N at all frequencies. Current multisensor deghosting techniques make deterministic assumptions on the data and ghost model (such as a known streamer depth, or assuming energy propagating only along the streamer). If the assumptions do not correspond to the data, the deghosting fails to restore the true broadband signal. We propose two novel data-driven deghosting techniques, which estimate an adequate deghosting filter based on the data itself. The first method estimates the 3D propagation of energy using measurements of the pressure and crossline particle velocity along a single streamer. The 3D incidence angle is used to sum the pressure wave with vertical particle velocity such that only the upgoing wave is recovered. The second method estimates the filter parameters that explain the recorded ghosted data by minimizing a multisensor least-squares deghosting cost function. The cost function is analytically shown to outperform similar single sensor adaptive deghosting techniques in terms of sensitivity to the true ghost model. The obtained filter parameters may then be used to construct an inverse filter that restores the upgoing wavefield. We found that both methods produce encouraging results on real data, outperforming the existing deterministic multisensor deghosting methods.","deghosting; adaptive deghosting; marine seismic","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Applied Geophysics","",""
"uuid:798229ba-0a13-4207-a4ab-5177eb2e5515","http://resolver.tudelft.nl/uuid:798229ba-0a13-4207-a4ab-5177eb2e5515","Cooperative taxfree articles solution","Rakhshandehroo, S.","van Heur, R.J.H.G. (mentor); Bergema, C.P.A.M. (mentor); Ottens, R. (mentor); Dijkstra, A. (mentor)","2016","PASSME is a project funded by the European Union’s 2020 Horizon research and innovation program and it aims for fast airports and stress-free passenger journey. In order to achieve these goals, it has established ten work packages, each focusing on an element of the whole system. Work package 3 is concerning hand luggage and is managed by KLM Royal Dutch Airlines. This work package consists of three types of luggage: 1) check-in luggage, 2) hand luggage, and 3) taxfree articles. This project is focusing on the taxfree articles luggage and has to contribute to the main PASSME goals of reducing unwanted airport time and improving passenger’s experience.","Omnichannel; taxfree articles; Amsterdam Airport Schiphol; KLM; Passenger Services; Networked Innovation","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovatie Management (PIM)","","Strategic Product Design","",""
"uuid:6ddb9d31-413e-4e71-9b9c-da0f020c9c98","http://resolver.tudelft.nl/uuid:6ddb9d31-413e-4e71-9b9c-da0f020c9c98","The use of Dutch governmental reference architecture in the public/private healthcare domain","Greve, R.J.","de Reuver, G.A. (mentor)","2016","In the last decade, there has been an increase in connectivity between private and public healthcare organisations. This increase is facilitated by technology developments, new legislations in healthcare, decentralization of healthcare and new market developments. Solutions are designed in the area of healthcare ICT. However, there is a problem with the design of these solutions; there is a lack of knowledge on how reference architecture can be used to resolve tensions between public and private healthcare organisations. The main research question was therefore: ""How effective is the Dutch governmental reference architecture (NORA) for private organisations that want to connect to the public domain?” To determine the effectiveness of the NORA two methods were used. The first method was to find out what tensions often arrive between public and private organisations when they collaborate and second if all parts of the NORA are actually usable for private organisations. To do this the following research objective was devised: “Design and evaluate a private domain solution architecture for a matchmaking platform based on governmental reference architecture to investigate NORA’s effectiveness in private architecture”. There are differences between public and private architecture because of the difference in legislation, guidelines and standards. Connecting public and private organisations is therefore inherently difficult. A solution was investigated through the use of reference architecture because It was assumed that when different organisations follow the same reference architecture, the tensions can be solved. A single design case study was used to investigate this assumption. A private matchmaking platform is being developed to connect private and public healthcare organisations in the Netherlands for the benefit of the aging population. The public reference architecture used in the Dutch government is the NORA and was used during the design of a project start architecture which was further developed to a solution architecture for an online matchmaking platform that supports elderly to live longer independently. Using literature in the field of design theory, project start architecture, reference architecture, governmental architecture and solution architecture, this thesis will expand the literature on the inherent tensions between public and private organisations by applying Dutch governmental reference architecture in a solution architecture that facilitates information exchange within the public/private healthcare domain. The results generated from the design, interviews and a questionnaire showed that the NORA might be useful as a basic framework but the NORA is not ready to use by private organisations. Many parts are not applicable for private organisations and the resulting architecture needs to be enhanced with domain architecture. The tensions found were split in two domains. The healthcare tensions were healthcare standards, design requirements in healthcare, laws which need to be complied with, privacy concerns and a high level security. As well as the tensions between the public and private domain; attention must be paid to the different guidelines and regulations used by both, the effect of politics on the goals of each organisations, the availability of usable data for both, a different timeline and different concerns steered by society or the market. The first showcase of the usability of public reference architecture in private organisations makes a unique contribution to the area of public/private architecture. However, it can be concluded that the NORA is not ready to be used as a reference architecture for private organisations and as such does not solve all tensions in public/private collaborations. However, the NORA can provide a common language between public and private architects, which makes collaboration easier. The NORA consists of principles, standards and building blocks of which 6 principles cannot be used, the standards cannot be used except the open standards and the building blocks cannot be used since they are designed for public only organisations. As of now the NORA is too abstract, not adjusted and takes too much time to inspire and attract private organisations. This study can be used as a first case in a multiple case study in this area of research. Future research should be conducted on the use of NORA within the Dutch government and the possible uses and adaption of the NORA for private organisations. This can then be combined into a new reference architecture or adjusted NORA for public/private collaborations.","reference architecture; solution architecture; public/private healthcare collaboration; NORA","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","Management of Technology","",""
"uuid:a3dac484-0849-401a-acbe-230fcbac3003","http://resolver.tudelft.nl/uuid:a3dac484-0849-401a-acbe-230fcbac3003","Full waveform inversion of the African crust and mantle: strategies for accelerating model convergence","van Herwaarden, D-P.","Fichtner, A. (mentor); Afanasiev, M. (mentor)","2016","Master of Science in Applied Geophysics at Delft University of Technology, ETH Zürich and RWTH Aachen University. - The continent of Africa is one of the most geophysically interesting regions on the planet. More specifically, Africa contains the Afar Depression, which is the only place on Earth where incipient sea-floor spreading is sub-aerially exposed, along with other anomalous features such as the unexplained topography in the south. Despite its geophysical significance, relatively few tomographic images exist of Africa. This stems mainly from the sparse distribution of seismic monitoring stations on the continent, which itself is a result of the political instability and general geographical remoteness. As a result, the debate on the geophysical origins of Africa's anomalies is rich and ongoing. In this project a tomographic image is produced using the technique of elastic Full Waveform Inversion (FWI). To our knowledge this is the first attempt at performing a continental-scale FWI of the region. Data recorded from 100 earthquakes has been used as input for the inversion. The adjoint method was used to iteratively update the initial model, which was extracted from the Collaborative Seismic Earth Model. Forward and adjoint modelling were performed in the regional version of SPECFEM3D GLOBE, a global wave propagation solver based on the spectral element method. Over the course of ten iterations, the time-frequency phase misfit decreased satisfactorily, and additional details were added to the starting model. The final model was validated by evaluating the change in misfit for ten earthquakes not previously used in the inversion. All of these events showed a decrease in misfit.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","IDEA League Joint Master's in Applied Geophysics","",""
"uuid:585a8d14-1ff6-4e45-ace4-fbc3d7b23002","http://resolver.tudelft.nl/uuid:585a8d14-1ff6-4e45-ace4-fbc3d7b23002","SparkJNI: A Reference Design for a Heterogeneous Apache Spark Framework","Voicu, T.A.","Al-Ars, Z. (mentor)","2016","The digital era's requirements pose many challenges related to deployment, implementation and efficient resource utilization in modern hybrid computing infrastructures. In light of the recent improvements in computing units, the defacto structure of a high-performance computing cluster, ordinarily consisted of CPUs only, is superseeded by heterogeneous architectures (comprised of GPUs, FPGAs and DSPs) which offer higher performance and lower power consumption. Big Data, as a younger field but with a much aggressive development pace starts to exhibit the characteristic needs of its archetype and the development community is targeting the integration of specialized processors here, as well. The benefits do not come for granted and could be easily overshadowed by challenges in implementation and deployment when considering development time and cost. In this research, we analyze the state-of-the-art developments in the field of heterogeneous-accelerated Spark, the current Big Data standard, and we provide a reference design and implementation for a JNI-accelerated Spark framework. The design is validated by a set of benchmarked micro-kernels. The JNI-induced overhead is as low as 12% in access times and bandwidth, with speedups up to 12x for compute-intensive algorithms, in comparison to pure Java Spark implementations. Based on the promising results of the benchmarks, the SparkJNI framework is implemented as an easy interface to native libraries and specialized accelerators. A cutting-edge DNA analysis algorithm (PairHMM) is integrated, targeting cluster deployments, with benchmark results for the DNA pipeline stage showing an overall speedup of ~2.7 over state-of-the-art developments. The result of the presented work, along with the SparkJNI framework are publicly available for open-source usage and development, with our aim being a contribution to current and future Big Data Spark shift drivers.","Apache Spark; JNI; Java; PairHMM; Big Data; FPGA; Heterogeneous","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Quantum Engineering","","Computer Engineering","",""
"uuid:0f0ce3d0-088f-4306-b884-12054c39d5da","http://resolver.tudelft.nl/uuid:0f0ce3d0-088f-4306-b884-12054c39d5da","Rescaling of incorrect source strength using Marchenko Redatuming","Brackenhoff, J.A.","Van der Neut, J.R. (mentor); Wapenaar, C.P.A. (mentor)","2016","The iterative Marchenko scheme is a recent development in the field of Geophysics and a way to retrieve Green's functions at any point in the subsurface, called a focal point. To achieve this, only the reflection response measured at the Earth's surface and an estimation of the first arrival at the focal point are required. If the amplitude of the reflection response is scaled incorrectly, the method will suffer from artifacts in the estimation of the Green's function. The amplitudes of the reflection response are unknown when the source strength of the recording is unknown. To correct for source strength, a correction factor is used. The correction factor can be retrieved by using a function that has its minimum at the required correction factor, a so-called cost function. Additionally a scaling factor can be determined, which is used to ensure that the final Green's function has the correct amplitudes. Three cost functions are proposed. The first cost function minimizes the upgoing Green's function and only works if no reflectors are present below the focal point. The second cost function minimizes the reflection of a truncated medium that has no reflectors above the focal point. The second cost function can handle a focal point with reflectors below it. However, it is very computationally expensive, especially in 2D and 3D. Therefore the third cost function is introduced. The third cost function is more efficient than the second one and is based on the minimization of the upgoing Green's function, with a source and receiver at the focal point. It is less accurate, retrieving only very close approximations of the required correction factor in case reflectors are present below the focal point. The minimization of the cost functions fails if there is an overlap in time of physical events and artifacts. In case physical events overlap with each other, none of the cost functions works perfectly.","thesis; msc; Marchenko; scaling; source strenght; correction factor; scaling factor; cost function","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","","",""
"uuid:0be41536-dae1-444c-94c6-5b47047a6cc6","http://resolver.tudelft.nl/uuid:0be41536-dae1-444c-94c6-5b47047a6cc6","Investigating the robustness of Green’s function retrieval via Marchenko focusing and Seismic Interferometry","Thomsen, H.R.","Broggini, F. (mentor); Van Manen, D.-J. (mentor); Mildner, C. (mentor); Ravasi, M. (mentor); Kritski, A. (mentor)","2016","Seismic interferometry and Marchenko focusing are alternative techniques to retrieve the Green’s function between a virtual source in the subsurface and receivers at the surface. Seismic interferometry requires the presence of a receiver in the subsurface at the position of the virtual source, while Marchenko focusing utilizes only the reflection measurements at the Earth’s surface and an estimate of the direct wave from the virtual source to the acquisition level. I find that, for both methodologies, limited recording aperture of the acquisition array is a strong limiting factor when trying to retrieve events caused by interactions with curved scattering objects in the subsurface. It is also established that, given the dense sampling of the wave field, applying the Marchenko focusing scheme provides a more accurate retrieval of the Green’s functions in such scenarios. However, if the source and receiver arrays are subsampled, Marchenko focusing provides less robust retrieval of the accurate subsurface fields. Marchenko focusing also has more severe requirements on the reflection response. When an erroneous scaling of the amplitudes and/or a constant phase-shift is introduced the method fails to retrieve accurate subsurface wave fields without artifacts caused by internal multiples. I propose a workflow to calibrate the reflection response, prior to Green’s function retrieval via Marchenko focusing, using additional information in the form of a VSP dataset. First a virtual VSP dataset is estimated via Marchenko focusing, to subsequently compare its upgoing component to the upgoing part of the recorded VSP. Thereby, making it possible to correct for a constant phase shift applied to the reflection data. By identifying the minimum residual energy between the virtual VSP and the recorded VSP wave fields, an erroneous scaling of the reflection response can also be corrected. This workflow leads to a more robust Marchenko focusing approach where the reflection response can be redatumed to a target zone in the subsurface: the resulting ghost-free gathers, and ultimately images of the subsurface, show more illumination and improved resolution, leading to better delineation of thin stratigraphy as well as faulted structures.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Master in Applied Geophysics - IDEA League","",""
"uuid:ebd30e85-10c7-40e4-bde2-d84a3ccf8c7a","http://resolver.tudelft.nl/uuid:ebd30e85-10c7-40e4-bde2-d84a3ccf8c7a","First steps in DC homegrid power management","Gravemaker, M.L.; Janssen, S.","Bauer, P. (mentor); Ramirez Elizondo, L.M. (mentor); Vega Garita, V.E. (mentor)","2016","A demonstration setup of the Tesla Powerwall has been installed by Eneco at the department of DCE&S at the TU Delft. This setup is used to investigate the functionalities of the Tesla Powerwall. As research is being done about transforming AC homegrids towards DC homegrids, it would be interesting to investigate how the Tesla Powerwall would function in these surroundings. A lot of research has already been done about DC-microgrids, however a design containing the Tesla Powerwall is missing. In this thesis the first steps for the design of a DC homegrid model containing loads, generation and storage have been made. A configuration of buck and boost converters are made for transformations between the different voltage levels. Simulations in Simulink have shown that the chosen topology produce the desired voltage levels. The model can be used for further analysis of the DC homegrid","DC grid; Tesla Powerwall; Energy storage; homegrid","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","DC Systems, Energy Conversion & Storage","",""
"uuid:46a0d7b7-5f30-481b-a48c-de32513f69d3","http://resolver.tudelft.nl/uuid:46a0d7b7-5f30-481b-a48c-de32513f69d3","8uW to 1mW Input Power Management IC Design for RF Energy Harvester","Jiang, Y.","Serdijn, W.A. (mentor); Dolmans, G. (mentor); Dijkhuis, J. (mentor)","2016","This master thesis project aims to design a power management block which is suitable for RF energy harvesting application with input power range from 8uW to 1mW. RF energy harvesting applications are low power Wireless Sensor Network or Transceiver with working duty cycle and average power consumption is from tens of uW to hundreds of uW. Based on the transceiver working condition, general power management block system can be designed in a two stages architecture including the Harvest Interface Stage and Load regulation Stage with storage element in between. The purpose of Harvest Interface is to regulated input voltage according to varying input power condition and Load regulation in system is to regulated output voltage for the target application. The Storage Element in between can store energy coming from Harvest Interface and discharge it according to the load. In terms of low input power situation (8uW in this project), the total power management system should be designed to be ultra low power loss. This project focused on the design of Harvest Interface Stage with the full bridge MOSFET rectifier to be the input of Harvest Interface and output connected to the Storage Element. Power loss estimation was done to evaluate topologies of Harvest Interface Stage. Then, inductive DC/DC boost converter was chosen to be the topology. Next, the switches control principle was analysed and low power switches control design was implemented to regulated input voltage of DC/DC boost converter. In addition, in order to track the varying input power, a power detector has been made which consists of 9bits ADC, DAC, RC integrator and Maximum Power Point Tracking [MPPT] digital design to continuously track the maximum power point by comparing new cycle power value with previous cycle power value. The output of power detector will give a input voltage reference for DC/DC boost converter to regulate its input voltage. The saturated input voltage reference indicates the maximum PCE from rectifier to DC/DC boost converter. Finally, Schematic and Layout were done in this project and simulations were implemented for both schematic and layout. Maximum Power Point Tracker [MPPT] can track 8uW power source with only 1 percent power loss inaccuracy and for 1mW power source, 7.5 percent inac- curacy power loss will occur. DC/DC converter [Without MPPT] power conversion efficiency post layout simulation(RC Extraction) for 8uW input power is 70 percent and 89 percent for 1mW input power source.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Microelectronics","","","",""
"uuid:ec25e33c-577c-42e9-8a81-e8a80a48890f","http://resolver.tudelft.nl/uuid:ec25e33c-577c-42e9-8a81-e8a80a48890f","A wave-equation based AVO inversion of VSP and surface seismic data","Steenhuisen, V.","Haffinger, P. (mentor); Wever, A.K.T. (mentor); Wapenaar, C.P.A. (mentor)","2016","","seismic exploration; inversion; wave-equation","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience and Engineering","","Applied Earth Sciences","",""
"uuid:aec7e9a2-68f4-474b-8afa-74ada021c10a","http://resolver.tudelft.nl/uuid:aec7e9a2-68f4-474b-8afa-74ada021c10a","Plate tectonics of New Zealand: insights from 3D thermomechanical modelling","Boonma, K.","Gerya, T. (mentor); Liao, J. (mentor)","2016","New Zealand sits on an oblique convergent plate boundary between the Australian plate in the east, and the Pacific plate in the west. The tectonic setting comprises two opposing subduction zones, Hikurangi (ca. 25 Ma) in the north and Puysegur (ca. 20-6 Ma) in the south, and a transpressional continental transform fault, the Alpine Fault (ca. 23-22 Ma), linking the two subduction zones. Despite New Zealand being well-studied through several geological and geophysical methods, the dynamic evolution of the processes involved in the subduction zones and the continental transform are still not clear. An exploratory investigation, which employs 3D thermomechanical numerical modelling, is conducted in order to gain insights into the dynamic tectonic evolution of New Zealand. The finite-differences code (‘I3ELVIS’) iteratively solves the conservation laws (mass, momentum, and energy) in a staggered grid. The study focuses on three aspects of the dynamic evolution: (i) the 3D geometric and structural development; (ii) the topographic development; and (iii) the faults/stress development. Four sets of models are constructed to study the influence of prescribed weak-zones’ geometry on New Zealand’s tectonic evolution. The modelling results indicate that the continental transform system does not appear to be attaining a steady state. The results exhibit key tectonic features such as an oblique continental transform and the migration of subduction zones into the transform, all of which are consistent with natural observations. The interpretations of modelling results reflect (3D) structural development of the transform which are comparable to the interpretation of seismic survey transects. The observed key structural features are the steeply-dipping Alpine Fault, the thickening of the lower continental crust, and the presence of thrusting fault-blocks in the upper continental crust. An observed set of splay faults in the modelling results reflect the similarities, in faults and stress development, to the the Marlborough Fault System.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Applied Geophysics (IDEA League) program","",""
"uuid:3d74c4f7-f2d0-4539-9a39-e1c3a03f2c37","http://resolver.tudelft.nl/uuid:3d74c4f7-f2d0-4539-9a39-e1c3a03f2c37","Seamless transitions of multiple micro-grids between the backbone interconnected and the islanded operational modes","Bilidis, N.","Bauer, P. (mentor); Ramirez Elizondo, L.M. (mentor)","2016","The traditional power system has been undergoing fundamental changes during the latest years. The developments in the field of renewable energy sources, as well as in the power electronics industry, acted as drivers aiding the establishment of micro-grids. Nowadays, power production is becoming more and more decentralized, therefore loads and consumers are powered via local generation avoiding the losses resulting from the transmission system. However, there is still a lot of research needed in the field of micro-grids, before they are mature enough to penetrate the conventional system at a large extent. This thesis aims to contribute to the further development of the field of micro-grids. The thesis focuses on a grid comprised of various micro-grids working in parallel. The micro-grids are interconnected through a low voltage backbone. Each micro-grid can operate either in islanded mode or connected to the backbone and working in parallel with other micro-grids. The main restriction that has to be respected is the lack of ICT equipment. Only local measurements are allowed and each of the interconnected micro-grids cannot acquire information for the rest of the micro-grids. Thus, the frequency of the system is the only communication signal and the main control variable. The frequency closely reflects the state of the system. The main component of each micro-grid is the converter of the battery energy storage system. Apart from that, each micro-grid consists of a PV system and a load, as well as the connection to the backbone. The key aspect is the lack of connection to the main grid. The backbone interconnected system can be perceived to be working in stand-alone mode. Within this regime, the question arises as to how to achieve seamless transitions of such a micro-grid between the islanded and the backbone interconnected operational modes. The literature shows many examples of such transitions of a micro-grid and the main grid. This thesis addresses the seamless mode switching issue in an extended grid comprised of parallel micro-grids and under the absence of a main conventional grid. The main aim of this thesis was to develop adequate controllers which would ensure seamless transitions of the previously described micro-grid between the islanded and the backbone interconnected operation. Nevertheless, in order to implement and test the efficiency of the controllers, the development of a base-model regulating the operation of the micro-grid both in islanded and in backbone interconnected mode was necessary. The respective controllers were developed in the PowerFactory simulation software and their efficiency was tested under different operational conditions, especially under extreme cases. The results were evaluated and conclusions were finally drawn.","microgrids; transitions; seamless; backbone; intentional islanding; unintentional islanding; frequency; synchronization; CSGriP","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","","",""
"uuid:cee5e97b-d023-4e27-8cb6-75522528e62d","http://resolver.tudelft.nl/uuid:cee5e97b-d023-4e27-8cb6-75522528e62d","Towards a fault tolerant RISC-V softcore","Heida, W.F.","Wong, J.S.S.M. (mentor)","2016","As predicted by Gordon E. Moore in 1975, the number of transistors has doubled every two years over the last decades. This technology scaling led to a higher performance of Integrated Circuits (ICs) like processors, but have also made these devices more susceptible to Single Event Effects (SEEs). SEEs are caused by transistors changing state unintended due to particles disposing significant amounts of energy when they hit an IC. In this thesis, the design, implementation, and verification of a fault tolerant RISC-V processor, which can detect two errors and correct one error, is presented. The RISC-V Instruction Set Architecture (ISA) is an open-source architecture which defines all interactions between the software and hardware. Two designs, the ECC-based and Hybrid design, are presented which both use Error Correction Codes (ECC) and N-Modular Redundancy (NMR) for adding fault tolerance to the softcore. Hamming Single Error Correction, Double Error Detection (SECDED) codes were selected as the ECC coding scheme. The hybrid design, which uses NMR in the pipeline and ECC for memory elements, is chosen for implementation, because it has a lower complexity, less Single Points of Failure (SPOFs) and a higher estimated clock frequency. The design's fault tolerance is verified with fault injection through saboteurs, which are placed at predefined locations in the design. The test architecture for the design consists of a verification suite, a simulation-based environment using ModelSim and an on-board test environment using a Spartan-6 Field Programmable Gate Array (FPGA). The verification tests showed that the hybrid design can mitigate all injected single and double faults, without having a single failure occurring. The design did not meet the clock frequency and resource utilization targets set by Technolution B.V. because of long delay paths and large decoders. The design, however, allows for improvements like the insertion of extra pipeline stages and parallel decoding and data processing","RISC-V softcore; Fault Tolerance; ECC; NMR","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Computer Engineering","","","",""
"uuid:ccfb7cd2-f8ca-4380-ad03-3b9dcc78f0b3","http://resolver.tudelft.nl/uuid:ccfb7cd2-f8ca-4380-ad03-3b9dcc78f0b3","Coupled modeling of dynamic ice-structure interaction on offshore wind turbines","Willems, T.","Metrikine, A.V. (mentor)","2016","The offshore wind industry is growing at high pace and is already one of the major renewable energy sources in Europe. To support this growth, costs must be further decreased, amongst others by efficiently designing the support structure of the offshore wind turbines (OWT). This requires accurate prediction of environmental loading such as dynamic ice-structure interaction. From industry practice it is known, that current phenomenological models show results that are highly sensitive to the used input parameters. It is investigated if the methodology used in industry practice, can be improved by coupled phenomenological modeling of dynamic ice-structure interaction. The phenomenological model proposed by Hendrikse and Metrikine (2015) is implemented and extended with the creep and buckling failure modes. The input parameters of the model are derived from reference data that is obtained from full-scale measurements. A verification study is performed to demonstrate that the reference data is matched. Furthermore, a qualitative verification is performed on the failure modes and ice-induced vibration regimes. The phenomenological model is applied to a structural model of an OWT and investigated in two case studies. For the implementation of the structural model, firstly the phenomenological model for ice-structure interaction is adjusted for cylindrical structures. Aerodynamic damping and thrust are included to enable simulations of the ice-structure interaction during the production of electrical energy. The coupled model is applied in two cases. The first case considers an offshore wind turbine model with ice loading as the only subjected load to the structure. The second case considers the combination of ice and wind loading. A comparison is made between coupled and uncoupled models for ice-structure interaction. The uncoupled models use an external ice load series to represent the ice-structure interaction and are therefore based on methodology used in industry practice. In case of ice-only loading, it is concluded that the uncoupled model is not capable of capturing intermittent crushing and frequency lock-in behavior due to inconsistency between the load signal and the response. In case of combined ice and wind loading, it is concluded that the uncoupled model is incapable of capturing intermittent crushing behavior and ice-induced damping. The same inconsistency is used as an explanation.","offshore wind turbines; phenomenological modeling; ice-induced vibrations; ice-structure interaction; arctic engineering","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Offshore and dredging engineering","","","",""
"uuid:5a014950-67f4-469f-b763-7ae944326af9","http://resolver.tudelft.nl/uuid:5a014950-67f4-469f-b763-7ae944326af9","Improving passenger experience in airports through redesign of the jet bridge environment","de Bruin, B.F.L.","Hiemstra-van Mastrigt, S. (mentor); Kuipers, H. (mentor)","2016","This report is the documentation of the project ‘Improving passenger experience in airports through redesign of the jet bridge environment” assigned by the TUDelft as a part of ‘PASSME’. It describes the design of an environment to replace the current jet bridge.","jet bridge environment; airports","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Design for Interaction","",""
"uuid:29c2d241-1b55-4f88-8ee7-5f28970b0a00","http://resolver.tudelft.nl/uuid:29c2d241-1b55-4f88-8ee7-5f28970b0a00","Calibration and Inversion of Large-scale Electromagnetic Induction Measurements of Agricultural Fields and Comparison with Leaf Area Index and Sun-induced Fluorescence Measurements","Verweij, E.","van der Kruk, J. (mentor)","2016","Recently, paleochannel structures were detected using electromagnetic induction (EMI) measurements and satellite based leaf area index (LAI) patterns on agricultural fields near Selhausen (Germany). This paleochannel system influences the amount of water that is available for crops. For two fields, the LAI patterns were correlated to the EMI measurements. In this thesis, additional fields were investigated with EMI for increased depths and high-resolution airborne sun-induced fluorescence (SIF) and LAI measurements were performed to estimate plant activity and available leaf area, respectively. The crop patterns present in the SIF and LAI were compared with the EMI apparent electrical conductivity (ECa) patterns. The EMI measurements were performed with two multi-receiver sensors, allowing measurements over nine depths of investigations (DOI). Three fields were measured, one field (F12), located in the Lower Terrace, showed relatively high ECa values. The second field (F13), located in the Upper Terrace, showed generally low ECa values. The third field (F10) was for two-third located in the Lower Terrace and one-third in the Upper Terrace and showed an abrupt change in ECa values. The filtered ECa values were interpolated to a 1-by-1 m grid. The different measurement directions were still visible after interpolation of F13, indicating that different ECa values were measured in perpendicular directions. The differences could only be observed for the northern part of the field, that was recently ploughed by the farmer. Further investigations showed that the ECa values were higher in the ploughing direction, with differences up to 2 mS/m. The SIF and LAI were measured using the HyPlant imaging spectrometer. The high spectral resolution of HyPlant allows the retrieval of fluorescence using the Fraunhofer Line Depth Principle. A simple ratio of the fluorescence, retrieved at 680 and 760 nm, clearly showed the within field variability and was compared with ECa values using a linear regression analysis. High SIF values were observed above the Lower Terrace, whereas lower values were observed above the Upper Terrace. Small-scale patterns of higher SIF values in the Upper Terrace originate from the paleochannel system. Regression analysis of the interpolated ECa maps of F13 resulted in coefficients of determination (R2) ranging from 0.32 to 0.64 for SIF and 0.25 to 0.55 for LAI. Both regressions showed increasing correlation with larger DOI, indicating that the deeper part of the soil plays an important role in the crop performance. To obtain quantitative ECa values and being able to perform a large-scale EMI inversion, a calibration was performed based on predicted ECa values from ERT and VES measurements and inversions. A regression analysis between measured and predicted ECa values resulted in good, but different regressions. The corresponding R2 values were not decisive for finding the optimal calibration parameters. The ERT-based calibration resulted in high scale factors for the CMD-SpecialEdition. The resulting calibrated ECa maps contained negative values for the low conductive part of the field. Comparison of a two, three, and four-layer VES inversion, showed that a three-layer VES inversion could be used to calibrate the ECa values. The calibrated ECa maps are inverted for a multi-layer earth using a parallelized version of the shuffled complex evolution (SCE) approach. The inverted EMI data for a three-layer model contained a negative middle layer, indicating that the used calibration was not successful, probably due to shallow electrical conductivity changes that were not captured by the direct current measurements. The calibration procedure needs to be re-evaluated and different direct current measurements and setups should be considered.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Applied Earth Science","",""
"uuid:757c8076-8a48-48e5-9ccc-ce85ac1efa93","http://resolver.tudelft.nl/uuid:757c8076-8a48-48e5-9ccc-ce85ac1efa93","Cost and Benefit Assessment of SWIM enabled applications and services: A value engineering approach to assess real life use cases","Eyselbergs, T.F.N.","Curran, R. (mentor)","2016","The purpose of this research was to create a good assessment of the possible benefits and costs of System Wide Information Management (SWIM). This assessment was demanded by several stakeholders in order to convince them to invest in this new technology. By creating a general methodology this process can easily be redone by others for their specific use case (software application or service). This research focused on 3 use cases that were analyzed with the developed method. After the analysis of these 3 use cases it became clear that SWIM does create several benefits. Although they depend on the specific use cases, several general benefits were seen, such as better situational awareness, increased safety, etc. The cost investigation showed that most of the benefits come at a very acceptable price and thus the end balance was positive. However, SWIM is a relatively new system and thus it still has some problems, for example the immature standards or the lack of legal support. This needs to be solved in order to not reduce the calculated benefits.","","en","master thesis","","","","","","","","","Aerospace Engineering","Control & Operations","","","",""
"uuid:b60c3f44-93e8-479d-8cba-ac2f818096e8","http://resolver.tudelft.nl/uuid:b60c3f44-93e8-479d-8cba-ac2f818096e8","Joint Surface Downhole Microseismic Event Location","van Veen, D.J.","Vera Rodriguez, I.A. (mentor)","2016","MSc thesis collaboration between ETH Zurich and Schlumberger Gould Research, Cambridge, UK. - A hydraulic stimulation operation in the Marcellus shale, Pennsylvania, was monitored by means of coalescence microseismic mapping, with receiver arrays downhole, in a horizontal well, and at the surface, in a radial geometry. The results of each survey are combined to obtain joint surface-downhole microseismic event location estimates that harvest the horizontal location constraint that results from surface monitoring, and the vertical location constraint that is expected from downhole monitoring. Downhole recordings were processed using two anisotropic velocity models with different axes of symmetry: one vertical (VTI) and one hor- izontal (HTI). Both velocity models are compared to determine which of the two is most suitable for a joint inversion. It was found that the HTI model removes an angle-dependent location uncertainty for calibration perforation shots that is present in the VTI model. However, the VTI model constrains the depth of the microseismic events better than the HTI model and is therefore the model that is used for the joint inversion. From the comparison it is suggested that an orthorhombic anisotropic velocity model could be requisite for more accurate location estimates. The joint inversion was performed by first matching surface- and downhole microseismic events in time (up to 40% success rate) and then in space (up to 97% success rate). Subsequently, the four-dimensional probability distribution functions (pdfs) that result from coalescence microseismic mapping of downhole data, are smoothed in horizontal direction to relax the horizontal location constraint, and then multiplied with the pdfs that result from surface monitoring. This yielded joint event locations that exhibit a horizontal position close to the one obtained from surface monitoring, and a vertical position close to the one obtained from downhole monitoring, a result that is considered an improvement with regards to the individual monitoring methods.","microseismic; geophysics; hydraulic fracturing; HTI; VTI; anisotropy; transverse; joint-location","en","master thesis","","","","","","","","2017-08-31","Civil Engineering and Geosciences","Geoscience & Engineering","","IDEA-League Applied Geophysics","",""
"uuid:f27e6029-bae2-498c-a9c6-c5b658cd6334","http://resolver.tudelft.nl/uuid:f27e6029-bae2-498c-a9c6-c5b658cd6334","Valuation of mortgage offer options","Sun, L.","Oosterlee, C.W. (mentor)","2016","","mortgage; mortgage offers; mortgage options; mortgage option value; the least squares method; the stochastic grid bundling method","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","","",""
"uuid:6269f634-8dea-46d1-9201-51e3d31cb1b8","http://resolver.tudelft.nl/uuid:6269f634-8dea-46d1-9201-51e3d31cb1b8","Een analyse van de bootstrap binnen eindige populaties","Kooijman, K.","Nane, G.F. (mentor)","2016","","eindige populatie; bootstrap; citation analysis; betrouwbaarheidsinterval","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","","TW3050",""
"uuid:0a0685fc-ce0a-4580-a355-978810005b71","http://resolver.tudelft.nl/uuid:0a0685fc-ce0a-4580-a355-978810005b71","Image Reconstuction in MRI: The Possibilities of Portable Low-cost MRI Scanners","Wijchers, M.S.","Van Gijzen, M.B. (mentor); Remis, R.F. (mentor)","2016","Hydrocephalus is one of the most common abnormalities affecting the nervous system of children around the globe. Especially in developing countries hydrocephalus is a large problem. It is often left untreated resulting in suffering, brain damage, developmental delays and ultimately death. This makes an early detection more opportune than ever. In standard MRI it is typical to adjust the hardware of the imager to minimise the effort needed in the reconstruction. MRI scanners are also heavy and expensive. Therefore a start is made in developing a sufficiently accurate image reconstruction algoritm that is able to effectively process signals produced by a low-cost MRI scanner. An analytic signal is set up for the new prototype magnetic resonance imager and a measurement model consisting of a set of linear equations is derived. For two dimensions, a new encoding method (rotating the magnet) is implemented in a Matlab program. The Matlab program constructs the signal with a Shepp-Logan phantom image and then solves the set of linear equations to reconstruct the original image. Care needs to be taken when it comes to several practical aspects as wel as theoretical issues. The model is analysed on its performance through tests on the Nyquist rate and its sensitivity to perturbations on the background magnetic flux density field. Satisfying the Nyquist rate is important when translating discrete signal samples in a continous signal. Perturbations due to human measurement errors have to be avoided where as perturbations due to limiting tools can be dealt with. Another study looks at the right balance between the number of rotations, the frequency bandwidth and the effective rank of the system matrix. The performance of iterative solvers CGLS and CGNE are tested. CGLS is the better choice when noise is included in the model. Which is the incentive for a short study about the p-norm. CGLS has been combined with Tikhonov regularisation and showed the best performance with regularisation matrix L=I and extra a-priori information compared to other regularisation matrices.","MRI; Magnetic resonance modeling; Image reconstruction; Portable low-cost MRI; iterative solvers; Tikhonov regularisation","en","master thesis","","","","","","","","2016-08-31","Electrical Engineering, Mathematics and Computer Science","Numerical Analysis","","Applied Mathematics","",""
"uuid:8d87b84d-b53d-476d-a85e-5cc115f7cd47","http://resolver.tudelft.nl/uuid:8d87b84d-b53d-476d-a85e-5cc115f7cd47","Data driven Orientation Determination of Multicomponent Seismic Sensors","Patterson, F.","Bagaini, C. (mentor); Van Manen, D.-J. (mentor)","2016","The orientation of a three component sensor in seabed seismic needs to be known to successfully process the acquired data. Although it is often measured, the data processing centers are requested to estimate it when these measurements are corrupted or to validate them. A large class of methods used in the industry perform well in the presence of uniform shot coverage but suffer when shots are partly missing, for example because of obstructions. I develop multiple novel algorithms that either make use of the source-receiver geometry or the knowledge of the seabed elastic properties around the receiver to determine the orientation of the three sensor axes. The seabed properties are used to model the response of the seabed to an incident pressure wave. The new routines are tested on real and synthetic data. All the new approaches perform better than Schlumbergers legacy proprietary processing software in retrieving a known receiver orientation from synthetic data, if the shot coverage is non-uniform. The application to real data also indicates that the new algorithm matches the results of the existing methods when the shot coverage is uniform and outperform them when there are large gaps. Especially the model based approach solves the problem of determining the orientation of a receiver entirely if the knowledge of the compressional and shear wave velocities of the seabeds is good enough.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","IDEA League Joint Master's in Applied Geophysics","",""
"uuid:409b668b-deb8-43ce-9047-6a25dc689279","http://resolver.tudelft.nl/uuid:409b668b-deb8-43ce-9047-6a25dc689279","Seismo-acoustic characterization of the ambient noise field","Selais, Y.A.Y.","Evers, L. (mentor); Smets, P. (mentor)","2016","Atmospheric ambient noise (microbaroms) as well as crustal ambient noise (microseisms) both generated form non-linear interactions between ocean waves have been observed on co- located arrays to analyse the atmosphere and the source mechanism. Data has been provided by NORSAR for the month of January to make this analysis. Beamforming and Fisher coherency detector were used to extract apparent velocities and backazimuths of the signals. The backazimuths from array observations have been correlated with the backazimuths of ocean waves modelling of both microbaroms and microseisms and microbarom source trans- mission loss analysis have been incorporated. Microbaroms and microseisms have shown to have similar source throughout the full period, however, for 4 days (day 16 to day 20) the source direction is different in both microbaroms and microseism and the reason for that could be because of the microseisms coupling mechanisms. Microbaroms had stratospheric duct during the full period ranging from approximately 25-35 km. Co-located microbaroms and microseisms array observations show that it is possible to distinguish between atmospheric effect on microbaroms and a source effect which affects both microbaroms and microseisms. Further analysis of the coupling mechanisms and propagation of microseisms is required to investigate the use of microseisms to aid in atmospheric studies.","ambient noise; microbaroms; microseisms; infrasound","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience and Engineering","","","",""
"uuid:6edaefd6-3b25-4d12-aa91-8ab8dc9f7bb1","http://resolver.tudelft.nl/uuid:6edaefd6-3b25-4d12-aa91-8ab8dc9f7bb1","Stimulating Creative Dialogues between Humans & Things: DIY in the age of the IoT","Amram, T.","Giaccardi, E. (mentor); Hoftijzer, J.W. (mentor)","2016","Expressing creativity answers to fundamental human needs such as Autonomy, Competence and Self-Esteem (Sheldon, Elliot et al. 2001). However, as industrialisation caused the first consumer revolution, passive consumption of mass-produced objects became the norm. A contemporary response to this passive consumption can be found in an increasingly popular DIY subculture: the ‘Maker Movement’. The Maker Movement is characterised by its affinity for making technology and production accessible through open collaboration (Dougherty 2012) and mediating technologies (Hoftijzer, 2009). This thesis is guided by the principle that the process of design should be democratised. The author postulates that in order to genuinely advance a democratic design process, the amateur designer should have access to ethnographic tools as well. The research carried out in this project aims at understanding how Makers conceptualise and explores how they could use data from their everyday ‘Things’ (Giaccardi et al. 2016) as an ethnographic resource to understand their own design space. Ultimately, this thesis introduces the notion of ‘data-inspired DIY’ as a new creative practice, herein embodied in a set of speculative technologies.","DIY; IoT; Maker Movement; Things as co-ethnographers; Thing Ethnography; Meta Design; Data-Inspired; Quantified Self","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Design for Interaction","",""
"uuid:2643732c-04d1-48d1-85cd-be29c15ea43b","http://resolver.tudelft.nl/uuid:2643732c-04d1-48d1-85cd-be29c15ea43b","Indoor training and thermal comfort: Improving thermal comfort of human subjects training indoors by a configuration of parameters, specifically airflows","Theodoridou, E.","Bluyssen, P.M. (mentor)","2016","In our modern cities, the large majority of the activities take place indoors and training is no exception. Thermal comfort at high metabolic rates inside buildings is to be maintained as much as possible without sacrificing energy saving. Indoor thermal comfort level is the result of a combination of various parameters and air movement is one of them together with air temperature, air humidity and radiant temperature. Making use of localized airflows can compensate for the necessity of cooling a room. Most research, though, has been focused on people with low metabolic rate, meaning that more research on improving thermal comfort of people training is required. The aim of this study is to test how airflows with different characteristics and more specifically air velocity, turbulence intensity and power spectral density can improve thermal comfort of people training indoors. Training subjects were asked to evaluate themselves a number of different airflows. More specifically, human subjects were asked to exercise indoors using a cycling machine. At certain moments during this procedure airflows were introduced and subjects were asked to fill in a questionnaire concerning their thermal condition. This procedure was repeated for each of the airflows. This way, the impact of the three parameters mentioned above on the thermal condition of a subject training indoors could be evaluated. Before the experimental procedure described, the airflows to be experienced by the subjects had to be determined. Consequently, information about the pattern of the airflows created by various set-ups was gathered by recording and further analyzing the velocity value by time in different locations inside the airflows. Throughout this analysis, the most appropriate airflows for the experiments with human subjects were chosen. As reflected in the answers of the participants the introduction of the airflows had, in overall, a positive impact. Furthermore, the increase of the velocity and the turbulence intensity and the existence of power in frequencies higher than 0.1 Hz resulted in an increased cooling capacity. The increase in turbulence intensity and the existence of power in higher frequencies resulted, additionally, in a less pleasant for the subjects airflow. Based on the results, the optimum between the airflows could be selected as well.","thermal comfort; airflows; indoor training","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Building Engineering","",""
"uuid:6d21738b-35bc-452d-936a-af4fbc2d2ee1","http://resolver.tudelft.nl/uuid:6d21738b-35bc-452d-936a-af4fbc2d2ee1","High Resolution, Fully Digital Photon-Counting Image Sensors in DSM CMOS Technologies","Ulku, A.C.","Charbon, E. (mentor)","2016","Single-Photon Avalanche Diodes (SPAD) have gradually become the top choice for time-resolved imaging applications thanks to their high timing resolution and single-photon sensitivity. However, a variety of factors complicate the implementation of SPAD sensors with large pixel arrays that achieve comparable specifications with competing technologies. The major issues that must be addressed to increase the scalability of SPAD sensors include fill factor, pixel array uniformity and power consumption. In addition, the integration of SPADs into deep sub-micron CMOS process technologies introduces its own challenges such as the lack of high voltage support and dead spaces that restrict pixel miniaturization. In this thesis, a time-gated, fully digital pixel with an in-pixel memory was presented. The pixel functionality and basic parameters were tested in a 110 nm 4×4 array. In addition, the scalability of this architecture was demonstrated by designing a 512×512 sensor in 0.18 ?um technology. Several performance boosting techniques were implemented in different variants of each chip. The impact of different SPAD structures on overall sensor performance was investigated. Finally, a 512×1 linear sensor with maximum 12 V excess bias was designed to operate the SPAD with high photon sensitivity.","","en","master thesis","","","","","","","","2018-01-31","Electrical Engineering, Mathematics and Computer Science","Quantum Engineering","","","",""
"uuid:ca800f03-7c8c-4a98-aa38-726588a5bb19","http://resolver.tudelft.nl/uuid:ca800f03-7c8c-4a98-aa38-726588a5bb19","Design of a Sub-harmonically Injection-Locked TDC Array for Space Applications","Padmanabhan, P.","Charbon, E. (mentor)","2016","Over the past two decades, successful orbital missions undertaken to map the Earth, Mars, Mercury and the Moon have proved that light detection and ranging (LIDAR) measurements are powerful catalysts in space exploration. LIDAR based technologies help evaluate the topography of a land and its surface features along with gathering data to understand the habitability of planetary bodies by creating depth maps. In systems which involve time-of-flight (TOF) techniques, depth maps are created by sending a laser pulse to a target and detecting the reflected pulse by an appropriate photodetector. The detected signal is then supported by electronic circuitry which records the TOF and estimates the distance between the target and the laser source. Bound by the amount of measurements that a LIDAR should perform, there are many technical challenges due to varying system requirements like payload constraints, operating conditions in space, instrument size and power budget. In this thesis, two major tasks were carried out, targeting imaging application in Ultraviolet (UV) and visible spectral range. A front-end readout circuit specifically designed for III-Nitride based UV avalanche photodiodes (APDs) is presented. Specific challenges of the readout are discussed in detail followed by measurement results. Further, a time-correlated single-photon counting (TCSPC) based TOF sensor is presented with silicon based single-photon avalanche diodes (SPADs) at the detector end. The sensor is fabricated in a 65 nm 3D IC CMOS technology, where the SPADs are integrated in the top tier and the processing electronics in the bottom tier. The 8x8 time- to-digital converter (TDC) array used to measure the TOF achieves a resolution of about 60 ps, thus providing a minimum measurable range of about 9 mm. The maximum achievable dynamic range is 150 m. The array incorporates concepts like coupling and injection locking to minimise the overall system jitter and provide a superior phase noise performance.","","en","master thesis","","","","","","","","2018-01-01","Electrical Engineering, Mathematics and Computer Science","Quantum Engineering","","Applied Quantum Architectures","",""
"uuid:f6e08f87-267c-4a79-a6cb-72494a859145","http://resolver.tudelft.nl/uuid:f6e08f87-267c-4a79-a6cb-72494a859145","Effects of Structural Failures on the Safe Flight Envelope of Aircraft: A database approach to ight envelope prediction and protection","Nabi, H.N.","de Visser, C.C. (mentor)","2016","","","en","master thesis","","","","","","","","2021-08-31","Aerospace Engineering","Control and Operations","","Control & Simulation","",""
"uuid:8aa89b7e-8825-41ef-a3d5-41dc01a793fc","http://resolver.tudelft.nl/uuid:8aa89b7e-8825-41ef-a3d5-41dc01a793fc","Biomechanical evaluation of an additively manufactured patient-specific knee-sparing tumor endoprosthesis","Brandt, J.A.H.W.","Zadpoor, A.A. (mentor)","2016","Additive manufacturing techniques like electron beam melting are promising methods for the production of patient-specific implants. Current state-of-the-art preoperative imaging and computer assisted surgery techniques allow orthopaedic surgeons to perform tumor resections with smaller surgical margins while still ensuring complete cancer removal. Together these technological improvements have cleared the road for the development of tumor endoprostheses sparing the existing knee joint. A novel knee-sparing patient-specific tumor endoprosthesis has been developed that is additively manufactured. An automated design tool is being developed that will optimize the implant design for load transfer and primary fixation. This research will contribute by experimentally evaluating the knee-sparing tumor implant design implanted in April 2014 in the Leiden University Medical Center. Tensile testing in combination with digital image correlation (DIC) is used to test different primary fixation configurations. The knee-sparing implant used obtains its primary fixation from flanges, screws and an antirotation tube. Ingrowth of bone into the porous titanium surface makes sure that secondary fixation is obtained. Custom distal femoral bone specimens were designed and manufactured to perform the tensile experiments. Three pairs of bone screws form the experimental variables. The angle with respect to the horizontal in the coronal plane of the two _8.0 mm spongiosa screws is _1. The presence/absence of the two _3.5 mm corticalis screws of the medial and lateral flange is _2 and the presence/absence of the two corticalis screws of the posterior flange is _3. With three different angles _1 this makes for a total of 12 different design configurations. Each configuration was tested three times. The force versus displacement curves were recorded and the surface displacement and strain values of the bone specimens were measured with DIC.The force data combined with the surface displacement values resulted in the grip forces at certain displacements between bone specimen and implant. From the DIC data also the major versus minor principal strain plots were extracted. The presence of the _3.5 mm corticalis screws had the largest effect on the resulting grip force, with the highest grip forces seen when both pairs of corticalis screws was present. The strain distributions following from the digital image correlation measurements endorsed this. The optimal configuration was believed to be _1 = 30◦, _2 = present and _3 = present.","","en","master thesis","","","","","","","","2021-08-31","Mechanical, Maritime and Materials Engineering","BioMechanical Engineering","","","",""
"uuid:ce7d235a-b725-47d8-a9c0-c44f050c57ce","http://resolver.tudelft.nl/uuid:ce7d235a-b725-47d8-a9c0-c44f050c57ce","Improvement of fatigue damage prediction for the Balder J-lay tower during transits","Rentoulis, I.","Metrikine, A.V. (mentor)","2016","Heerema Marine Contractors (HMC) is a contractor that transports and installs large structures at sea. For this purpose, HMC operates four crane vessels of which three are semi-submersibles (Thialf, Balder and Hermod) and one is a mono hull (Aegir). The Balder is equipped with a J-lay tower for pipelay in deep water. During transits, when the Balder sails at a shallow draught over the open ocean from continent to continent, wave-induced motions of the Balder result in fatigue damage to the tower, as the tower angle with the horizontal can only be reduced to 75 degrees. In order to ensure safe operations and prolong the period that the tower can be profitably used for projects, HMC has installed several sensors on critical locations on the tower to monitor the fatigue damage during transits and pipelay operations. Alongside the measurements, HMC has developed a numerical tool to predict damage during transits. During a Balder transit from West-Africa to Europe, it was noted that the a priori prediction of the tool overestimated the wave-induced fatigue damage as was measured during the transit. The goal of this graduation study is to determine how to improve the prediction of wave-induced fatigue to the Balder J-lay tower during transits by comparing the results of predictions with actual measurements. By comparing the fatigue damage procedures used for determining the fatigue via the different models and analysing the intermediate steps, it was concluded that the major contributor to the inconsistencies between the measurements and the predictions were the incorrect response amplitude operators (RAOs) used for determining Balder’s response in waves. The inconsistent RAOs result from the fact that the diffraction software used to calculate them is not capable off accurately assessing the vessel motion behaviour for the case of shallow draughts used during transits. The problem was tackled by identifying new RAOs for the transit draught based on the available measurements and hindcast data. Among other parameters investigated, a major one is the historical weather database, which is the basic input of the prediction tool. The updated RAOs result in a much better agreement between the measured and the calculated vessel motions. This in turn enables a great improvement in the accuracy of the fatigue calculations, quantified to be at least an order of magnitude. Furthermore, it was concluded that the prediction tool does not assess correctly the probabilistic fatigue output due to the fact that it is not able to capture the weather persistency. A new numerical model developed for the purposes of the assignment is able to capture the effect by using hindcast time series instead of wave scatters. The last part of the research concerns a preliminary assessment of the high-frequency fatigue induced by vibrations of the tower in its eigenmodes. The knowledge gained during this project will allow HMC to make more accurate predictions of fatigue damage to the Balder tower during transits and plan mitigation actions accordingly.","fatigue; Response Amplitude Operators (RAOs) identification; Hindcast data; measurements; weather persistency; Rayleigh; rainflow; finite element models; High-Frequency Vibrations","en","master thesis","","","","","","","","2021-08-31","Mechanical, Maritime and Materials Engineering","","","MSc Offshore and Dredging Engineering","",""
"uuid:73db50f6-c4b5-4319-925e-e63b77eac3da","http://resolver.tudelft.nl/uuid:73db50f6-c4b5-4319-925e-e63b77eac3da","Innovation projects and their promising project management approach","Fijn, E.","van Beers, C.P. (mentor); Ortt, J.R. (mentor); van der Voort, H.G. (mentor); Maas, F. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Values Technology and Innovation","","Management of Technology","",""
"uuid:560e5165-b15a-44d0-93cf-fe34040a2b6c","http://resolver.tudelft.nl/uuid:560e5165-b15a-44d0-93cf-fe34040a2b6c","Electromagnetic Evaluation and Optimization of a Combined Inductive Power Transfer and Inductive Healing Road","Visser, E.R.A.","Bauer, P. (mentor); Prasanth, V. (mentor)","2016","Electrical vehicles (EVs) can be charged wirelessly with an inductive power transfer (IPT) system that uses a magnetic field to deliver power to the EV. Such a system could be embedded in the highway to charge EVs while they are driving. The magnetic field of the IPT system is created using coils that are exited with a high frequency current. Recent developments has created a low maintenance asphalt that can be healed by means of induction heating. Steel wool is added to the asphalt to make it suitable for induction heating, which is also done using coils that are exited with high frequency currents. This thesis will investigate the feasibility of a combined inductive healing asphalt (IHA) and IPT highway. The main goal will be to reduce the losses in the IHA for an efficient operation of the IPT system, while maintaining the possibility to heat the asphalt by means of induction heating when desired. Also a preliminary economical analysis to estimate the financial profit of a low maintenance road is performed. A detailed loss model for IPT systems is created that is used to describe the power transfer efficiency and predict the losses in the asphalt. Four concepts are suggested to obtain a feasible combination, which are (1) a sectioned road that separates the IPT and the IHA systems in the geometry, (2) IHA with anisotropic conductivity and permeability, (3) asphalt with dedicated heating elements and (4) asphalt with frequency dependent hysteresis losses. The loss models are verified using an experimental setup, which shows that the models created for concepts (1)-(3) provide an accurate description of the system. Based on the models and the experimental results it is concluded that the concepts (1) and (3) can provide a feasible combination of an IHA highway in combination with IPT systems. The design of such a highway would however need careful considerations with respect to the dimensioning of the systems and external sources might be needed to apply induction healing for some parts of the highway. The result of the preliminary economical analysis shows that the expected reduction in maintenance will be more than the additional implementation costs and therefore the concepts are also economically feasible.","Inductive Power Transfer; Induction Heating; Inductive Healing Asphalt; Low Maintenance Road","en","master thesis","","","","","","","","2017-08-31","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","","",""
"uuid:11ebbfcc-05e1-4135-b97e-d54bed02aa8d","http://resolver.tudelft.nl/uuid:11ebbfcc-05e1-4135-b97e-d54bed02aa8d","A Practical Approach to Evolutionary Algorithm based Automated Mechanism Design: Research to the usability of automated mechanism design","Kranendonk, E.","Wolsfslag, W.J. (mentor)","2016","The complexity of current mechanisms continues to increase and their users keep on demanding even more. Current design methods are not sufficient to keep on fulfilling those requirements. Automated design methods have shown to be a viable solution to keep up with the demands and research has started to make this possible for mechanisms design. But there is a big difference in the predicted performance before and the performance after manufacturing. To minimize the effects of production errors while minimizing its computational costs we introduced a novel robust optimization method. In the genetic programming algorithm we add uniform noise to the production dimensions. Every generation the fitness of a mechanism is calculated with this noise and that fitness is added to a memory bank of that mechanism until that memory is full. The average of the fitness values in the memory bank is used as the fitness value to rank the mechanisms of a population. By changing the maximum bounds of the noise and the maximum amount of values possible in the memory-bank we can influence the robustness of the final mechanisms and computational costs of the algorithm. The optimal settings to get the most robust design with the least computation cost was by keeping the noise between ± 2 times the ISO-2768-m norm and limiting the maximum amount of fitness calculations in the memory-bank to 8. Further decrease of the computational cost was achieved by directly calculating the incidence matrix from the incidence string instead of the iterative method. We also improved the accuracy of the predicted performance by introducing a better weight and moment of inertia calculation based on more accurate descriptions of the shape of the mechanism. The combination of these adjustments resulted in automated mechanism design method that generates robust mechanisms for a wide range of problems within an acceptable time span.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Biomechanical Engineering","","BioRobotics","",""
"uuid:d084df7f-9210-456f-b682-23f994224be2","http://resolver.tudelft.nl/uuid:d084df7f-9210-456f-b682-23f994224be2","Anisotropic traveltime inversion of hard rock targets: Synthetic and Field Study at the Grimsel Test Site","Toledo Zambrano, T.","Doetsch, J. (mentor); Manukyan, E. (mentor); Schmelzbach, C. (mentor)","2016","","Grimsel Test Site; Seismic anisotropy; anisotropic traveltime inversion","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience and Engineering","","","IDEA League",""
"uuid:14acc987-bf93-4439-a3b2-634615d7f449","http://resolver.tudelft.nl/uuid:14acc987-bf93-4439-a3b2-634615d7f449","Reliability Forecasting for Simulation-based Workforce Planning","Papathanasiou, M.","Verbraeck, A. (mentor); Cunningham, S.W. (mentor); Warnier, M. (mentor); Wenzler, I. (mentor); Fumarola, M. (mentor)","2016","The problem owner of the present study is a consulting company that provides simulation-based workforce planning advice to a big manufacturing firm XYZ. The latter pertains the number of engineers of various skill levels that are needed for the repair of health care equipment in hospitals of a large country. The prediction of machine failures (reliability forecasting) is a crucial input to the simulations that affects the quality of the business advice. Currently, the problem owner follows a reliability forecasting approach based on lifetime models following the HPP [1]. Nevertheless, this practice has several limitations as: i) the predictive performance is not always satisfactory due to data overfitting (Liang, 2011), ii) real-world systems do not generally comply with the HPP traits (Kurien, Sekhon & Chawla, 1993), namely constant failure rates of a memoryless failure process, while reliability is non-linear and complex due to a bunch of factors (Chatterjee & Bandopadhyay, 2012). In the view of the above, the problem owner needs to increase the efficiency of workforce planning that will finally lead to cost savings for firm XYZ. It is believed that a more efficient planning can be achieved through the improvement of the forecasting approach. Forecasting should fulfil certain requirements, namely it should predict the failure patterns of multiple machines, at an acceptable level of accuracy, with a high degree of automation. Thus, the study’s research objective is defined as: to provide an automated forecasting framework that detects and predicts the failure patterns of multiple machines with acceptable accuracy. For achieving the research objective, firstly, a clarification of the forecasting requirements is done through a semi-structured interview with the problem owner. Among others, it is clarified that accuracy is the hourly absolute deviation between the actual and the forecasted inter-failure time of a machine (MAE), and it concerns only its next failure (one-step ahead forecasting). Additionally, for a bunch of reasons, two different levels of acceptable accuracy are defined, the 1st with minimum accuracy of 120h (1 working week), and the 2nd of 2160h (1 quarter). Secondly, the identification of the most promising forecasting approach that can fulfil the given requirements is done through a literature review. Time series forecasting is found to be the most promising approach as it: i) outperforms reliability models that follow the NHPP in terms of accuracy (Ho & Xie, 1998; Dindarloo & Siami-Irdermoosa, 2015; Fan & Fan, 2015), ii) is able for automated and large-scale application (Wagner et al., 2011). Subsequently, a case study, which pertains reliability forecasting of radiation treatment machines maintained by firm XYZ, is conducted in order to evaluate the time series approach. The reliability metric of Time-Between-Failures (TBF) is used for forecasting, whilst the time series cross-validation method is employed for its evaluation. The time series approach followed is based on the use of three parametric methods (ARIMA, Exponential Smoothing, Optimized Theta) and two artificial neural networks (FFNN, RGMDH) applied on the machine group level (2 groups) and on the individual machine level (5 machines). In this context, experimentations take place under both full and adjusted for outliers data conditions. Moreover, the related repair data, expressed by Time-To-Repair (TTR) and by a dummy variable that represents the use of spare items, is used in the TBF forecasting with ARIMAX models. The case study demonstrates that: i) on the machine group level, the series are white noise involving that the failure process is memoryless and failure patterns cannot be detected, ii) on the individual machine level, the best performing forecast model of every machine examined satisfies the 2nd level of acceptable accuracy. The MAE error metric of the best forecast model for each machine examined is substantially less than 2160h. Thus, the present study has succeeded in its objective. The reliability forecasting framework produced constitutes a holistic approach to the prediction of machine failures, as with its various and at a degree, complementary methods can deal with all the basic types of failure data (e.g. autocorrelations, seasonality, trend, non/linearity, etc.) The framework formed is provided to the problem owner allowing for the transformation of the workforce planning of firm XYZ from an annual to a quarterly basis. The recommendations for the problem owner as well as for future research are: first, the execution of experimental simulations with a planning horizon of 3 months in order to evaluate the possible cost savings for firm XYZ. Second, the collection of new relevant to machine failures data (e.g. machine utilization, purchase date), and third, the extension and evaluation of the forecasting framework with the inclusion of these new data and/or with new methods (e.g. hybrid, FFNN with external covariates) and techniques (e.g. time series clustering). Fourth, the application and re-evaluation of the reliability forecasting framework formed when the failure data of 2016 become available. Fifth, the use of failure behavior’s variability as a stakeholder management tool when the problem owner deals with forecasting projects. Last, the use of the time series cross-validation method for the evaluation of forecast models and the great amount of attention on the potential existence of outliers in the dataset. On reflection, the contribution of the present thesis is multi-dimensional. First, a holistic and multi-method reliability forecasting framework that can deal with almost any failure process has been produced. This framework can be used in relevant projects as it can be extended and adjusted to the conditions of each project. Second, the aforementioned framework has been built though a state-of-the-art analytical forecasting process that can also be used by the problem owner in different projects. Third, there is a clear potential for cost savings for firm XYZ if workforce planning is adjusted in a quarterly horizon. Fourth, there is a knowledge contribution to the performance of various time series methods (e.g. Optimized Theta, RGMDH) in the context of reliability forecasting. Fifth, there is a clear contribution to the increase of the domain knowledge of reliability forecasting in health care equipment in general, and in radiation treatment machines in particular. Last, it has been highlighted that the initial evaluation of the variability of the failure behavior of a set of machines can serve as a stakeholder management tool as regards the final forecasting deliverable.","Workforce Planning; Reliability Forecasting; Machine Failures; Time Series Analysis; R.","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi Actor Systems","","M.Sc. in Engineering and Policy Analysis (EPA)","",""
"uuid:9ca7eee6-5445-47f1-ae16-7215ddf42514","http://resolver.tudelft.nl/uuid:9ca7eee6-5445-47f1-ae16-7215ddf42514","Project management maturity of the Dutch Water Boards: An exploratory research in maturity models","Alta, F.P.","Bakker, H.L.M. (mentor); Bosch-Rekveldt, M.G.C. (mentor); Lousberg, H.M.J. (mentor)","2016","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Construction Management and Engineering","",""
"uuid:aa994e2d-7136-4875-9a6d-4adc18cce0f1","http://resolver.tudelft.nl/uuid:aa994e2d-7136-4875-9a6d-4adc18cce0f1","An Explorative Study on Adoption Drivers at the BoP: Enabling profitability and social impact for improved cooking stoves in developing countries","Naiknavare, A.","Kamp, L.M. (mentor)","2016","","BoP; revenue models; social impact; profitability; adoption drivers; rate of retention; recoupment of costs; adoption models; developing countries","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","MSc Management of Technology","",""
"uuid:aafcf8fe-4292-406b-9c58-bc368fe55994","http://resolver.tudelft.nl/uuid:aafcf8fe-4292-406b-9c58-bc368fe55994","Scheduling the Spark Framework under the Mesos Resource Manager","van den Bogert, J.","Epema, D.H.J. (mentor)","2016","Using clusters of servers and datacenters to process large numbers of data- and computation-intensive jobs is becoming mainstream. The need for large clusters is driven by the fact that many workloads are growing at a faster rate than the advances in single computer performance. To manage processing in a cluster of multiple computers, several frameworks have appeared over the years. These frameworks supply the user with convenient computation constructs and abstract from the low-level implementations to relieve the burden of inter-process communication and task placement in a cluster. However, these frameworks often assume total control over a static partition of the cluster, leading to under-utilization in times when one framework is over-committed with work, whereas another framework is idling. To overcome this under-utilization, and multiplex frameworks in clusters, cluster schedulers have been proposed, which sit on top of the hardware resources and schedule hardware resource leases to frameworks. It is not well described how these systems differ from each other. Furthermore, how to achieve \emph{performance balance} between frameworks, such that multiple frameworks achieve similar performance metrics when running time-varying workloads is relatively unexplored. We define a taxonomy which describes the combination of cluster schedulers and frameworks, called Two-Level schedulers in general. We characterize the multiple state-of-the-art cluster schedulers that are described in the literature or are used in practice. We distinguish multiple aspects which define the cluster schedulers. These aspects can dictate how frameworks interface with the cluster scheduler and can also influence framework performance. We aim to achieve performance balance for multiple \gls{spark} frameworks running under the \gls{mesos} cluster scheduler. First, we evaluate the performance of a single framework running single interactive data analytics queries. We find multiple configuration parameters which influence the performance for interactive queries. However, we conclude that using \gls{sparkmesos} for interactive queries results in either inefficient use of resources, or does not allow us to multiplex resources over multiple frameworks. We continue with achieving performance balance for multiple frameworks for non-interactive queries. We first establish a baseline performance balance, which we attain by using knowledge of the possibly different workload intensities run by the frameworks in a real cluster. Afterwards, we achieve similar performance balance, compared to the baseline, for up to three frameworks without knowing the workload intensity a priori. This is achieved by using a feedback loop controller, which updates resource share sizes allocated to frameworks dynamically, based on the online performance metrics of the frameworks.","spark; cluster; scheduler; mesos; datacenter","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Software Technology - Distributed Systems","","Computer Science, Software Technology","IN5000",""
"uuid:a4c07b84-24ed-417a-9ad7-779bdd2422f0","http://resolver.tudelft.nl/uuid:a4c07b84-24ed-417a-9ad7-779bdd2422f0","A tailored fMRI image processing pipeline based on ITK","Cao, Y.","Hendriks, E.A. (mentor)","2016","In neuroscience research, fMRI image analysis is a major approach to identify the relationships between brain region and behavior. fMRI provides delayed real time brain activity signals non-invasively. Spatial preprocessing and statistical analysis needs to be performed to find the brain region that is responsive to certain tasks or stimulations. This project introduce an implementation of a tailored fMRI image processing pipeline, FIONA (Functional Imaging Overlay for NAvigated TMS), based on Insight Toolkit (ITK). The major functionalities of the software include preprocessing (realignment, co- registration, spatial smoothing), GLM analysis, hypothesis testing and multiple comparison correction (mainly FDR). FIONA is designed for usual clinical users or fMRI analyst who may not be an expert in neuroscience. It is designed for navigated TMS as its intended use. We also did benchmarking on different ITK optimizers comparing their performance in realignment section, to select an overall best optimizer. We test our whole implementation on different level noisy fMRI datasets.","fMRI; Image registration; GLM; FDR","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Intelligent Systems","","EIT Digital (Digital Media Technology)","",""
"uuid:284086f2-2f07-4395-9941-68392f3da68b","http://resolver.tudelft.nl/uuid:284086f2-2f07-4395-9941-68392f3da68b","Decentralized Event-Triggered Control Over Wireless Sensor Actuator Network: Design for Energy Conservation & Implementation","Paul, S.","Mazo, M. (mentor)","2016","Over the last decade there has been a shift in focus from traditional sampled-data control to event-triggered control (ETC) as it promises to be better suited for systems with shared communication media, especially wireless systems. The mote lifetime in case of wireless sensor actuator networks (WSAN) is crucial and can be improved by utilizing efficient control algorithms. Unlike sampled-data control, event-triggered control (ETC) algorithms require updates from sensors only when it is essential to maintain the stability of the system and ensure satisfactory control performance. Hence, ETC approach to control over WSAN systems can provide significant energy and transmission savings. Moreover, wireless motes spend considerable energy just by idle listening as the communication radio is periodically switched on as per the currently used duty cycling technique. Hence, providing a passive wake-up radio extension to wireless motes can prevent this energy wastage by switching on the radio only when communication is required. This thesis presents event-triggered control as a viable option to implement control over wireless sensor actuator networks (WSAN). To achieve this, a Hardware-in-Loop system is developed, in which dynamics of a plant are simulated on a real-time hardware and controlled via a wireless sensor actuator network. The system also provides a LabVIEW user interface depicting relevant real-time information. A comparative experimental study is done between periodic sampling and event-triggered approach of controlling the system. It is observed that the event-triggered approach achieves similar control performance and provides sensor transmission savings of more than 40%. Further, to reduce the energy wasted by the sensors in idle listening mode passive wake-up radio extensions are added to the sensors. Energy savings of 38% are obtained by using ETC with passive wake-up radios.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Center for Systems and Control","","","",""
"uuid:fd2f0a02-4f78-454f-80ee-c96e45e062db","http://resolver.tudelft.nl/uuid:fd2f0a02-4f78-454f-80ee-c96e45e062db","Distributed Energy Management in Smart Thermal Grids with Uncertain Demands","","","2016","Smart Thermal Grids (STGs) represent a new concept in the energy sector that involves the use of the smart grid concept in thermal energy networks connecting users, such as households, buildings and greenhouses, to each other via a transport line of thermal energy. In this concept, there exists an energy management system that aims to improve the efficiency, reliability and sustainability of the energy production and the distribution of energy. This highlights the necessity of constructing a high level control unit which sets the operating points of the production units such as boilers, micro Combined Heat Power (CHP) generators, and chillers for every agent. In this thesis, we develop a framework for the energy management which incorporates the model of the smart thermal grid and the energy demand profile of the agents. This framework is based on a Model Predictive Control (MPC) strategy in which the grid is addressed as a large-scale uncertain system. We formulate a mixed-integer chance-constrained optimization problem for the planning of the operation of the production units for all agents in the grid in the presence of uncertain thermal energy demand profile. In order to deal with the chance constraints together with integer variables, the robust randomized method, which was particularly developed for this problem, is employed. This technique allows us to handle mixed-integer problems and stochastic programming in a unified framework and provide a-priori probabilistic guarantees for the obtained solution. Motivated by the need for a more flexible and scalable framework, we then extend this method to distributed computation schemes using the Alternating Direction Method of Multipliers (ADMM). The resulting performance enhancement in terms of the total operational costs observed in the simulation was substantial and comparable with the centralized control approach. Finally, we investigate the opportunity of improving the efficiency of energy usage when seasonal storage systems exist in the thermal grids. We first propose a dynamic model for the seasonal storage systems and then, incorporate it in the energy management problem formulation. Due to the annual cyclic dynamical behavior of the seasonal storage systems, this leads to a multi-rate albeit very complex optimization problem. To this end, we develop a hierarchical MPC to solve such problem together with a discussion on the resulting optimization problems in a receding horizon setting. The technical developments were validated on a realistic benchmark problem (three-agent thermal grid in Utrecht, The Netherlands). The simulation results show that the proposed method was able to provide better usage of the seasonal storage.","","en","master thesis","","","","","","","","2016-08-30","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control","","Systems and Control","",""
"uuid:a132a3ed-704a-48e1-b371-e99e3ffbca5d","http://resolver.tudelft.nl/uuid:a132a3ed-704a-48e1-b371-e99e3ffbca5d","Moving Sound: the future of in-car sound systems","Van Grol, E.H.","Van Dijk, M.P. (mentor)","2016","The design of a new in-car sound system using parametric technology for Bang&Olufsen Automotive. Bang & Olufsen (B&O) is a brand with a great heritage in the field of sound and design. The automotive activities of B&O are positioned in the segment of the market that is positioned above the upper-class. B&O products are famous for their surprising ‘magic’ like interactions. The products often react on someone or something approaching, which surprises the user positively and invites them to interact with the product. In cars, this ‘magic’ happens once the user starts the sound system: the tweeter then moves out of the dashboard. However, B&O finds itself in a difficult position: substitute products are coming on the market rapidly and it is easy for car manufacturers to change to these. Since B&O does not have direct contact with the end-users of their products, they might lose track of what the final consumers want. A series of interviews with car dealers shows that B&O might lack focus on the future. Research to users and future developments gradually reveals a model on user behaviour inside cars and the role of sound in this. The sound-behaviour model describes two important choices drivers and passengers make in the (self-driving and maybe shared) car of the future. The first choice is whether passengers want to connect with the act of travelling (an authentic experience) or want to do something else (take a wormhole to another task). This balance is already existent in current cars. The other choice is whether passengers want to be social or alone. When being together in the car, one can choose to socialise or retreat. When laying both choices in a grid, four different types of behaviour become visible, all with completely different needs regarding sound. Authenticity Retreat is about focus on driving on your own, like driving a motorcycle. When being in this mode, occupants hear engine sounds and other driving sounds from the environment. Authenticity Social is about enjoying the act of travelling together in the car, like exploring a new city. In this mode, voices are enhanced. Wormhole Retreat behaviour is doing a completely different task in the car, like working or watching a movie. If the task requires sound, this sound is enhanced, but if the passenger needs silence, the product should provide as much individual silence as possible. Wormhole Social behaviour is about catching up with friends or family, either digital or analogue. In this mode, (telephone) conversations are enhanced. The final concept is able to deliver all four ‘soundscapes’ to all individual passengers. In order to avoid restless ‘hopping’ behaviour, passengers should be enabled to make a clear choice. Since different passengers can have different needs, this means that every passenger gets the power to individually change the sound-mode of the product. A technology that enables individual sound-zones is ‘spotlight speaker’ technology, which uses ultrasound as a carrier of hearable sound and thus creates a very narrow beam of sound. After building a working prototype using this technology, the sound-behaviour model was validated by confronting respondents with the prototype. With this prototype, the two choices of the model were translated into two on/off buttons: ‘share’ (toggles social and retreat) and ‘input’ (toggles authenticity and wormhole). The final interaction design is based on the analogy of a ‘portal’, which is simple, inviting and magic. Passengers are able to wave away from other occupants to retreat and wave towards them to join their sound bubble. To avoid unwanted reflections, the system is located on the roof of the car. The products communicates its state by turning the brackets with the sound-producing transducers either towards other passengers in the car (shared) or away from them (individual). In good B&O tradition, the product is produced using aluminium and cloth. Several options regarding colours and material are possible to integrate the product in the roof. The main limitation is the sound quality of the technology, which needs serious development. As an intermediate concept, B&O could first launch the technology as add-on to current systems, with voice enhancement and a personal phone connection. B&O ‘Plus’ is focused on what users need in the car of the future, while still having the magic of Bang&Olufsen.","Bang Olufsen Automotive; Parametric Sound Design; Magic; Spotlight; VIP; B&O","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Integrated Product Design","","","",""
"uuid:0da53df4-df77-4afd-9bc0-89ccca593719","http://resolver.tudelft.nl/uuid:0da53df4-df77-4afd-9bc0-89ccca593719","Thermal Simulation of Low Concentration PV/Thermal System using a Computational Fluid Dynamics Software","Stylianou, S.","Smets, A.H.M. (mentor)","2016","Cogenra company has created a low concentration PV/Thermal system that produces both thermal energy and electricity simultaneously, mainly for commercial and industrial applications. The realization of a system that combines photovoltaic modules, solar thermal collectors and concentrating mirrors makes it a complicated system to study. So far, only simple studies have been made on Cogenra’s LCPVT system including a 2-dimensional model. In this project, the possibility of using a Computational Fluid Dynamics software for analysing the low concentration PV/Thermal system of Cogenra has been studied. The CFD software Ansys Fluent has been used, in which a model was created in accordance to the Cogenra LCPVT system. After validating the results, the model has been used for analysing the system’s performance under various conditions in order to realize the system’s losses. Furthermore, due to the numerous components of the system, the analysis of the LCPVT system becomes a multi-variable problem. For this reason, three main parameters (mass flow rate, optical concentration, PV type) that affect the system’s performance has been chosen and studied in order to improve the system’s overall efficiency. Since the system has both electrical and thermal outputs, an equivalent efficiency was determined to express the two different efficiency terms. For the purpose of comparing the performance of the LCPVT system with the traditional photovoltaic modules, one other simple model was created in Ansys Fluent. This model has also been simulated under the same conditions as the Cogenra system in order to observe the difference in output between the LCPVT system and the photovoltaic modules. The low concentration PV/Thermal system has also been compared with other solar thermal systems such as a PV/Thermal system, a Concentrated Thermal system and a simple Solar Thermal System.","CPVT; CFD; thermal; electrical; efficiency; concentration; simulation","en","master thesis","","","","","","","","2016-08-30","Applied Sciences","Electrical Sustainable Energy","","Sustainable Energy Technologies","",""
"uuid:17b6dead-1727-432c-9c74-c0ca84773d03","http://resolver.tudelft.nl/uuid:17b6dead-1727-432c-9c74-c0ca84773d03","City Cleaning 2030","Jeukens, P.I.M.","van Grondelle, E.D. (mentor)","2016","Starting out by following the Vision in Product Design methodology as developed by Hekkert and Van Dijk at the Delft University of Technology, the domain of this project needed to be framed and limited to a certain timeframe. The domain correspons to the title of this project exactly as it covers all aspects of the project’s scope.Consequently, the state of the art has been analyzed and deconstructed in order to discover the Daseinsgrund and the meaning of the current line of products. In addition to this, the more traditional aspects of design research were studied, including the brand, a market analysis, the company's core competences and the product portfolio. The Kärcher Group is rapidly expanding and enlarging the product portfolio. They enhance the marketing mix in order to generate more revenue and make the relatively large expenses which are invested in research and development possible. Subsequently to this research and key to the vision in product design methodology is the creation of a future vision. This vision is supported by future context factors that apply to the domain of city cleaning in 2030. The future framework that follows out of this phase is divided in two dimensions that represent the way individuals value their health and the way they relate to their environment. Based on these future attitudes and the conclusions from the earlier analysis are the future vision and the aspirations encapsulated in the mission statement. To get detached from the existing situation, the analogy of “a canary that warns miners for life threatening conditions in the mines” is used. The characteristics that match this situation form the basis for the guiding principles that the design has to meet. Hence are the canary’s features of being failsafe, vigilant, comforting, optimistic, and the fact that he is beloved like a mascotte required to be included. The project’s final outcome is the creation of a design strategy that incorporates an unique value proposition and envisions the accompanying elements. Seeing the entire study, it is recommended for Kärcher to shift its activities from the focus on product development towards offering 'the value of ensuring optimal conditions in the urban living environment’. Furthermore, it is advised to move from a passive role as the manufacturer of city sweepers into the active role of shaping the optimal conditions in the urban playground.In order to create these optimal conditions, it is needed to take action. The design strategy integrates these aspects in the CC2030 system that incoporates the concepts that sense the conditions, translate this information, and intervene or execute. The accompanying elements are respectively a sensing beacon at street level, an exoskeleton and an autonomous street sweeper. To push the envelope of innovation and establish itself as an authority in this field, the strategy includes the advice to facilitate a network of experts in the urban field and create a platform to create dedicated relationships with suppliers. Leading the discussion on the forefront of innovation makes Kärcher capable of guiding and even dictating the direction and gain competitive advantage on the long term.","design strategy; value proposition; Vision In Product design; municipal services; living environment","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Design Aesthetics","Strategic Product Design, Advanced Automotive Design",""
"uuid:419d9ee2-2478-478e-b99a-f1478655cee2","http://resolver.tudelft.nl/uuid:419d9ee2-2478-478e-b99a-f1478655cee2","Shopping Without Waste","Meulendijks, A.R.","Balkenende, A.R. (mentor)","2016","This thesis started with a wish to improve the world. The waste problem is addressed: it becomes clear that in the Netherlands 10 million tons of waste is generated by consumers 32.9% of this comes from packaging materials and 1.5% comes from plastic waste. Next to that, a person generates 50 kg of food waste on a yearly basis. When wastage in the food production chain is included this adds up to 157 kg per year. People are aware of the problems and know that behaviour needs to change and they are willing to. Packaging free supermarkets give customers the opportunity to buy the exact amount of food needed and to reuse their own packaging materials. Although it is unclear for them which actions are beneficial and which are not. Food often has a larger environmental impact than packaging materials. It is possible to limit the amount of food and packaging waste. A good start to lower the environmental impact of food is to lower the amount of food waste. This can be done by letting customers take only the exact amount of food they need. If packaging materials are reused the amount of packaging waste can also be reduced. To achieve this products can be offered from dispensers. It is important to understand that packaging materials are not only waste. Their functions are: bundling, transportation, protection and preservation, information and discarding facilitation. For some products it is impossible to offer them without packaging materials because they are needed for protection or preservation. Offering products without packaging material should not cause more food waste. Packaging free shops throughout Europe are rising in numbers. Customers find them interesting, but is still it hard for the majority of them to start doing their daily groceries in these shops. The pull factors from these shops are the simplicity, the visibility of the content, the biological and fair products that are offered, the personal approach from the staff members, the fact that customers can bring their own packaging materials and the fact that it is better for the environment. Nonetheless it is hard for many of these new supermarket concepts to stay in business; the two supermarkets in the Netherlands (Opgewecht and Bag&Buy) do not exist anymore. The reason for this is that there are too many thresholds for customers to do their daily groceries here. The largest thresholds are; 1. The time it takes inside the shop to take, weigh and pay the products. 2. The thinking step before going to the supermarket. They do not know how much and which kind of packaging materials they need to bring. 3. Maybe the most important threshold which (unconsciously) holds back many customers are the uncertainties; “How much do I need to pay in the end?”, “Can I really do this?”, “Will I not spill on the floor?” and “Do I have enough packaging materials with me?” Once it is inside your packaging it is impossible to put it back. If they encountered a bad first experience with the system it is unlikely that customers will come back. The assignment for this graduation thesis is; Design a product which makes the filling process in a packaging free supermarket faster and easier for the customer while presenting product and price information. It should be easier to do the daily groceries without generating unnecessary waste and to give the customer a satisfied feeling. As a solution NuMee (Dutch for “TakeNow”) is designed mainly to shorten the time it costs to take, weigh and pay the products and to take away insecurities. NuMee allows more customers to buy products at the same time and improves the user experience. The NuMee design makes it possible for people to bring their own packaging materials. It can also work well in a hybrid store; products which can be sold without packaging materials are offered without, while products that need their packaging will still be offered in a packaging. People can choose the exact amount they need, making it possible to limit food waste. Through the app or personal webpage more information about the products can be found. These two media can also give positive feedback, so customers understand the benefits of with their extra effort to shop in these special shops instead of the normal supermarket. They will present the environmental burden customers avoid with saving food and packaging materials. The NuMee design is tested with a first prototype. The interaction with the key needs to be changed. The scale needs to interact faster in order to achieve the intended user interaction. To test this a second prototype is made. Scale experts, experts in the field of dispenser business and experts in users tests are consulted for the iteration step. The second prototype is made, tests will be conducted after this graduation thesis. To conclude the NuMee system can have a positive impact on a social, environmental and economical level. It allows customers to take the food they need in the right amount in their own packaging materials. This lowers the amount of food waste and packaging waste. The system does not need to be more expensive than offering products within pre-packed packagings if the right products are chosen. There are already companies interested in NuMee. To really know the impact NuMee has further development is needed and more tests need to be conducted. Recommendations for further development can be given based on the building process of prototype 1 and 2 and the test with the prototype 1. The development of the scale is most important. The scale needs to be approved by the NMi (the Dutch Metrology institute). The payment method with the key should be tested with retailers to see if it really fits in hybrid stores. The interaction with the customer must be tested thoroughly. Possible types of misuse should be analysed and a solution must be found before implementation.","waste prevention; food; packaging; dispenser; supermarket","en","master thesis","","","","","","","Campus only","2017-08-30","Industrial Design Engineering","Design Engineering","","","",""
"uuid:8cfab6f8-3167-45e9-a790-0568bb5c594e","http://resolver.tudelft.nl/uuid:8cfab6f8-3167-45e9-a790-0568bb5c594e","Spark ablation: a new route towards tunablemagnetic La-Fe-Si alloy nanoparticles","Geutjens, R.","Garcia Espallargas, S.J. (mentor)","2016","The development of humanity has led to an increase in energy demand. To keep up with this need, scientists and engineers are constantly pushing the limits of technology to develop lighter, stronger and more energy efficient devices. Some recent developments in magnetocaloric materials have made it possible to use the magnetocaloric effect for cooling near room-temperature. Compared to vapor-compression refrigeration systems, magnetocaloric cooling systems can save up to 30% of energy. These materials also allow for greener, simpler and more reliable cooling systems. One of the promising materials is the La(Fe,Si)13 alloy. However, research showed that the magnetocaloric efficiency of these materials could be increased by converting it into nanoparticles. In this thesis, we report a novel, elegant and green method for the production of high purity-ternary alloy nanoparticles through high-frequency spark ablation in the gas-phase. Here, the nanoparticles were produced by the ablation of LaFe11.5Si1.5 electrodes. Then the composition, morphology, crystallinity, and magnetism of these particles was characterized. The results indicated that the oxidation of the nanoparticles is a key factor in the magnetic behavior. Based on these findings, some further experiments were performed. The first step was to add hydrogen to the carrier gas. This is known to reduce the oxidation and in this case, forms LaFe11.5Si1.5Hx. From these experiments, it became apparent that the concentration of hydrogen present during the formation of the particles played a significant role in their magnetic behavior. Next, we performed a study on the influence of the post-treatments on the particles magnetic response. Finally, we demonstrated that it is possible to create core-shell particles. Here, the shell can protect the core against oxidation and has the potential to prevent particle-particle interactions which could lead to superparamagnetism. The nanoparticles, as produced during this thesis, are not yet ready for the use in commercial and industrial applications. Nevertheless, this research introduces new insights of use for the further development of ternary alloy nanoparticles, core-shell particles, and La-Fe-Si alloy (magnetic) nanoparticles.","","en","master thesis","","","","","","","","2016-08-30","Aerospace Engineering","Aerospace Structures and Materials","","","",""
"uuid:c2ea0af1-b11c-4d06-8b59-d5db1a8efe67","http://resolver.tudelft.nl/uuid:c2ea0af1-b11c-4d06-8b59-d5db1a8efe67","Steel–Concrete–Steel Sandwich Immersed Tunnels For Large Spans","Bekarlar, K.Z.","Jonkman, S.N. (mentor); Bakker, K.J. (mentor); Braam, C.R. (mentor); 't Hart, C.M.P. (mentor)","2016","Traditional reinforced concrete tunnels show limits regarding large spans in the transverse direction. There was a lack of knowledge whether the steel-concrete-steel (SCS) sandwich tunnel can be a solution for tunnels with extreme large spans. Research was needed to understand the structural response of a large span SCS tunnel to the load applied. For the detailed analysis of the distribution of internal forces a finite element program was used. In order to compare a SCS sandwich tunnel for a large span with a reinforced tunnel, two base case tunnels were designed. From this comparison the critical span for each type was determined. It was seen that the reinforced tunnel critical span is 18 / 19 m, whereas the SCS tunnel could be designed for a span of 27 m (boundary condition reference project). In this thesis two 2-D models have been analysed in DIANA: one simplified model and a detailed model. Both models use linear-elastic material behaviour. The results of the hand calculations are first compared with that from the simplified model, to verify the simplified model. The results of both FEM models are compared and the differences are investigated. From the stress / strain analysis of the SCS tunnel cross section for a large span, it was seen that the tensile strength of the concrete was reached. This would result in the formation of tensile cracks. The allowable compressive stresses / strains were only locally exceeded. Concrete cracking and plasticity however may have impact on the degree of connection between the steel and concrete. Due to the cracks the shear stiffness of the steel and concrete connection can decrease. This may have impact on the overall stiffness of the structure. From the durability point of view these cracks have no impact on the durability of the structure since the concrete is situated in a confined space. Although the other side the exceedance of the stress is only locally, it might result in a redistribution of forces. By using a detailed FEM analysis, detailed insight was obtained in the distribution of internal design forces. This resulted in a significant reduction of the amount of steel applied. Namely 21 %. In absolute values, this is a reduction of 2,51 m3 of steel per meter in the axial direction. Since the reinforced concrete tunnel was not able to have a span up to 27 m, it was investigated, whether prestressing (post tensioning) the tunnel could be a solution. From the new design it was concluded that a span of 27 m is not a feasible solution when using prestressing. This is due to the large size of the prestress tendon anchors and the large axial forces which the concrete cross section could not resist. Further there was observed that a steel shell tunnel is a feasible solution for tunnels with large spans up to 28 m. From the costs analysis it was seen that steel shell tunnel variant 1 (regular steel shell) would cost 315 000 euros per meter length. Steel shell tunnel variant 2 (with steel cover plates on the inner side) is slightly more expensive than variant 1. The costs for this tunnel per meter length is 351 000 euros. The same analysis was performed for the SCS tunnel. This variant is with 421 000 euros per meter length, more expensive than the other two steel shell tunnel variants. It can be stated that in terms of costs, a reinforced concrete tunnel is the preferred solution for tunnels with a span up to 18 / 19 m. This is also the limiting span for a reinforced concrete tunnel. For a span from 19 till 28 m, the steel shell is a more cost efficient solution than the SCS tunnel. However, applying the SCS for spans shorter than 29 m, has some advantages as well, since the shear force and bending moment capacity of a SCS tunnel are larger than in case of a steel shell tunnel. This advantage can be important for changing boundary conditions or accidental loading on the tunnel structure, e.g. an earthquake loading, explosion, sunken ship on top of the tunnel, extra loading due to sedimentation on top of the tunnel, erosion below the tunnel floor or more ductile behaviour. When the construction area is in a region where the risks for earthquakes are significant, the SCS sandwich is the preferred solution for a span between 19-28 m. This is the case for the reference project used. Finally it can be stated that the SCS sandwich tunnel is the only solution available for spans larger than 28 m.","Immersed tunnels; Steel – concrete – steel sandwich immersed tunnels; Steel concrete composite; Large span; FEM analysis DIANA; Structural analysis","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Hydraulic Structures","",""
"uuid:e5a817a4-ce0a-4dd3-afd4-d70660b63d16","http://resolver.tudelft.nl/uuid:e5a817a4-ce0a-4dd3-afd4-d70660b63d16","Identifying and Managing Technical Debt in Complex Distributed Systems","de Vos, M.A.","Pouwelse, J.P. (mentor)","2016","The term technical debt has been used to described the increased cost of changing or maintaining a system due to expedient shortcuts taken during development, possibly due to financial or time constraints. The term has gained significant attention in software engineering research and the agile community. Tribler, a platform to share and discover content in a complete decentralized way, has accumulated a tremendous amount of technical debt over the last ten years of scientific research in the area of peer-to-peer networking. The platform suffers from a complex architecture, an unintuitive user interface, an incomplete and unstable testing framework and a large amount of unmaintained code. A new layered, flexible and component-based architecture that readies Tribler for the next decade of research is proposed and discussed. We lay the foundations for this new architecture by implementing a RESTful API and a new graphical user interface. By removing the old user interface, the amount of technical debt in Tribler is dramatically reduced. Additional work includes the pay off of various kind of technical debt by the means of major improvements to the testing framework, heavy modifications within the core of Tribler and changes in the infrastructure to make it more usable and robust. With the deletion of 12.581 lines, the modification of 765 lines and addition of 12.429 lines, we show that several important software metrics are improved and that we paid off a huge amount of technical debt. Raising awareness about technical debt in general is of uttermost importance if we wish to prevent deterioration of the system. Together with a code review policy and static code analysis tools to track code coverage and the amount of code violations, we hope to prevent huge amounts of technical debt in the future. We perform experiments to verify the stability and performance of various components in the core of Tribler and propose future work for the components that require more work. In our final experiment, we will test Tribler on a large scale and lay the foundations for an application testing framework that is useful for failure identification.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Distributed Systems","","","",""
"uuid:2bf6d9e6-2aee-4a3c-9c14-c16f5449d9f2","http://resolver.tudelft.nl/uuid:2bf6d9e6-2aee-4a3c-9c14-c16f5449d9f2","Maneuvering of Automated Vehicles in an Unsignalized Intersection: A Distributed Control Strategy","Mishra, L.","van Arem, B. (mentor)","2016","","Automated Vehicles; V2V communication; Simulation; Distributed Strategy; USARSIM; VISSIM","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport & Planning","","Transport, Infrastructure and Logistics","",""
"uuid:27e0e2f9-615b-4e31-adac-c5a4f46e9f64","http://resolver.tudelft.nl/uuid:27e0e2f9-615b-4e31-adac-c5a4f46e9f64","How Should Central Banks Respond to Inflating Real Estate Bubbles: A Qualitative Assessment of Industry Professionals’ Perceptions","van Eck, J.H.T.","van Beers, C.P. (mentor); Storm, S.T.H. (mentor); Pesch, U. (mentor)","2016","Central Banks do not respond to inflating real estate bubbles, but abide a policy that ‘mops up’ the damage after their collapse with the conventional tools of monetary policy. The financial crisis of 2008 disclosed the flaws of this method, which triggered a controversial debate among policymakers and economists. Currently there is an elevated risk for credit-fuelled bubbles in prime residential- and commercial real estate markets. A literature review identified four theoretical perspectives that propose a range of widely varying policy modifications, implying there is no consensus. This research targeted this knowledge gap by assessing the perceptions of knowledgeable real estate professionals to test and criticize the theories via semi-structured interviews. The findings of this study appointed a countercyclical restriction of Loan-To-Value ratios for residential – and commercial real estate financing as an appropriate step to pioneer a policy modification that aims to counteract inflating real estate bubbles","Real Estate Bubbles; Monetary Policy; Macro Prudential Policy; Loan-To-Value","en","master thesis","","","","","","","","","Technology, Policy and Management","Economics of Innovation","","Management of Technology","",""
"uuid:c93ab1fc-4c5b-4214-9063-5a4f80e065d6","http://resolver.tudelft.nl/uuid:c93ab1fc-4c5b-4214-9063-5a4f80e065d6","Determining the impacts of a human operated Multi Trailer System on Inter Terminal Transport performance","de Jong, R.A.","Corman, F. (mentor)","2016","","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Marine and Transport Technology","","Transport Engineering and Logistics","",""
"uuid:ef93503c-ce97-41d4-baf3-c7e7c08ed355","http://resolver.tudelft.nl/uuid:ef93503c-ce97-41d4-baf3-c7e7c08ed355","Extracting information from collections of musical performances using Dynamic Time Warping","Peperkamp, J.B.","Liem, C.C.S. (mentor); Hildebrandt, K.A. (mentor)","2016","When performing a piece of music, there are certain ways in which performers turn the score into their own musical interpretation of it. The difference between what is written and what is played can be substantial, and can vary between performers and even individual performances. In this thesis we examine what information can be gained from analyzing the differences that can be found when inspecting a collection of performances of a piece by various performers. We develop several techniques for analyzing the alignment yielded by Dynamic Time Warping and also explore the possibility of employing techniques from computer graphics to solve the task of finding an alignment. We test the techniques on synthetic data first, where applicable. We also test the techniques on a real dataset, a collection of performances of Chopin's mazurkas. Finally, we give some indication of how the techniques may most effectively be put to use based on the results of the tests.","Dynamic Time Warping; musical performance analysis; musical structure analysis","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Intelligent Systems","","Media and Knowledge Engineering","",""
"uuid:ce39c628-e060-401d-a7e6-dd527c296d4e","http://resolver.tudelft.nl/uuid:ce39c628-e060-401d-a7e6-dd527c296d4e","Drawing behavior of UHMWPE films made from solution casting: Influence of solvent quality","Enganati, S.K.","Bergsma, O.K. (mentor)","2016","Ultra-High Molecular Weight Polyethylene (UHMwPE) fibers are well known for their very high specific mechanical properties such as tensile strength and Young’s modulus. The solution spinning/solid state drawing process invented in the late 70s at DSM enables the production of these UHMwPE fibers with outstanding mechanical properties. The achievable maximum draw ratio of the UHMwPE fibers dictates their mechanical properties. One of the possible ways to improve this maximum draw ratio (λmax) is by decreasing the quality of the solvent used for its production. Modifying the solvent in the production of UHMwPE fibers might be a way to achieve higher properties. The focus of this project is mainly to study the influence of different solvents on the production of high strength UHMwPE films/fibers. Various ways were used to decrease the quality of the solvent to gel cast UHMwPE films and then the effect of this solvent quality on the drawing properties was determined. Subsequently, the molecular between the entanglements (Me) of polyethylene chains in different solvents was investigated using shear rheology. Morphological analysis (SEM, XRD) was performed to understand the topology, crystallinity, crystal thickness of the films made in different solvents. It was found that solvent quality has a significant effect on the maximum attainable draw ratio. SEM analysis was able to distinguish between drawable and undrawable UHMwPE films. However, from XRD analysis it was found that the crystal thickness is not the controlling factor for the improvement in drawability. The required difference Me of polyethylene chains in different solvents was found but it was within the experimental accuracy which made it difficult to conclude. Furthermore, preliminary study of some of the selected solvents on the production of UHMwPE fibers was performed. The drawing results proved that one to one translation from gel casting (films) to solution spinning (fibers) process is not straightforward. And also suggested that optimization of the drawing conditions of fibers can possibly lead to achieve expected maximum draw ratios.","ultra high molecular weight polyethylene; mechanical properties; solvent quality","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures and Materials","","","",""
"uuid:161f7ec2-29e7-4eca-91ff-d84e40b053a4","http://resolver.tudelft.nl/uuid:161f7ec2-29e7-4eca-91ff-d84e40b053a4","Statistical Impact Prediction of Space Debris: The Uncertainty Propagation Approach","Hoogendoorn, R.","Mooij, E. (mentor); Geul, J. (mentor)","2016","Due to the increasing number of space debris objects in orbit around Earth, the need increases for accurate predictions of impact time and location. These predictions are inherently statistical due to the large number of uncertainties in predicting the atmospheric decay and entry trajectories of these objects. Currently, Monte Carlo methods are used to obtain statistical information about the impact time and location. A direct method is applied to propagate uncertainty in the state of a decaying object to obtain statistical information. This method propagates a PDF of the state in time, instead of single trajectories. Impact time distributions are obtained using this method and Monte Carlo for the Delta-K rocket body.","Atmospheric entry; Uncertainty propagation; Statistical Impact Prediction; Space Debris; PDF propagation; Atmospheric decay","en","master thesis","","","","","","","","2018-01-01","Aerospace Engineering","Astrodynamics & Space Missions","","","",""
"uuid:525c32ae-89c1-437b-bbb2-fe2013c699b1","http://resolver.tudelft.nl/uuid:525c32ae-89c1-437b-bbb2-fe2013c699b1","Structural Health Assessment through Vibration Monitoring on FPSOs","Tatsis, K.","Metrikine, A.V. (mentor); Lourens, E. (mentor)","2016","In the offshore oil and gas industry, production takes place at more and more remote locations, with Floating Production Storage Offloading units (FPSO’s) often being selected for field development. These units may remain on station during their entire lifetime while operating under adverse weather conditions. Inspections, which have to be performed on site, are therefore becoming a challenging and risky operation. Within this context, various Structural Health Monitoring (SHM) schemes are being explored in an attempt to ensure integrity of offshore units. The goal of this project is to study the feasibility of structural health assessment on FPSOs using vibration-based monitoring techniques, with the ultimate aim to minimize inspections of confined spaces. To this end, a typical panel structure on a FPSO hull is considered and modelled using the Finite Element (FE) method. As part of a ballast tank, the considered component is inevitably subject to structural degradation, with corrosion and fatigue cracks constituting the main degradation mechanisms. The constructed FE model is therefore appropriately parametrized in order to accommodate the simulation of the aforementioned damage conditions. The first part of this study, referred to as the forward problem, consists of modelling the dominant degradation mechanisms experienced by hull structures of FPSO’s, namely uniform corrosion, pitting corrosion and fatigue cracks. These are introduced with varying degrees of deterioration into the reference FE model of the said stiffened panel and the sensitivity of the vibrational characteristics, i.e. the natural frequencies, mode shapes and damping ratios, to these changes is investigated. The aim of this part is to extract the identifiable damage scenarios that will serve as the basis for the structural health assessment through the implementation of Operational Modal Analysis (OMA). In the second part of the study, the so-called inverse process, the stiffened panel is assumed to be monitored during normal operation using a conventional monitoring system (i.e. accelerometers). The latter is configured in such a way that observability of all modes is accomplished and robustness of the identified properties is ensured. Excitation of the structure is assumed to be sloshing-induced impulsive loads and the measured noisy signals are processed with a set of Stochastic Subspace Identification (SSI) algorithms, upon enhancement with a cluster analysis in order to enable automatic system identification. For each one of the damage scenarios, the dynamic properties are identified and cross-compared with those of the reference model. The feasibility of damage detection through vibration monitoring, along with the existing restrictions, is finally determined and a possible extension of the proposed formulation is discussed.","FPSO; Structural Health Monitoring (SHM); Operational Modal Analysis (OMA); System identification; Damage detection","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Offshore and Dredging Engineering","",""
"uuid:25c36d14-6609-4a03-8ae2-f3e4ffd1ee2d","http://resolver.tudelft.nl/uuid:25c36d14-6609-4a03-8ae2-f3e4ffd1ee2d","Analysis of the influencing factors of dynamic loads acting on the operating systems of beam balanced bascule bridges","Antohe, D.","Metrikine, A.V. (mentor)","2016","There are functional movable bridges in The Netherlands dating since the 1950s. If these bridges were to be reassed based on the current design code for movable bridges, then more than 90% will not meet the exigencies stated there. However, if these bridges were to be reassed based on field measurements, then this percentage will be lowered considerably. Then why should existing bridges be reassed with the rules for new bridges? This research intends to lower the conservativeness of the current design codes based on an analytical approach, by including both the theoretical considerations behind the standards and the real data given by field easurements of an existing movable bridge. The aim is to create a dynamic model which reduces the gap between theory and reality. Since the available measurements were performed on a beam balanced type of movable bascule bridge, only this type is analysed. The model is created for a straight rack type of mechanism, and the parameters are afterwards adjusted so that the behaviour of a curved rack type of operating mechanism is simulated properly. The bridge is modelled by means of equations of motion which are intended to simulate its behaviour in two situations: a complete opening cycle of the bridge and an emergency brake. The entire structure is considered as a three degrees of freedom model, interconnected by spring-dashpot elements that consider the stiffness and damping of the mechanical components of the bridge. The results are obtained at the level of the electro-motor, the movable deck and the balance part which includes the counterweight. The model predictions are obtained in terms of angular displacements, angular velocities and angular accelerations of the deck and/ or the balance part and the electro-motor torque. When the results obtained for the deck and balance part are compared with the same variables obtained from either the design standard or the measurements, there is an almost perfect match. The model predictions are always below the theoretical values, showing that the code is indeed conservative regarding these parameters. Moreover, the model predictions are also close to the measured variables, proving that the model is closer to the reality. The outcome in terms of the electromotor torque cannot be properly verified. The data from measurements is insuffiecient and characterized by some uncertainty, thus pertinent conclusions in terms of the electro-motor torque cannot be drawn. The only viable conclusion observable for the electro-motor torque is that the predictions are always below the design electro-motor torque, proving again the conservative nature of the standard with regard to this variable. The research proved to be accurate in modelling the behaviour of the bridge structure, including the rack, the movable deck, the hanger, the balance beam and the counterweight, but the model can be improved further on the side of the electro-motor. Recommendations are given at the end in terms of additional modelling procedures, other measurements that should be performed and more components that could be accounted for.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","","",""
"uuid:51849a15-2f89-447b-8661-ac5d12b44edb","http://resolver.tudelft.nl/uuid:51849a15-2f89-447b-8661-ac5d12b44edb","Virtual Inertia Emulation in islanded microgrids with energy storage system","Zheng, Y.","Elizondo, L.R. (mentor)","2016","The inertial response plays a vital role in electrical networks to reduce the rate of change of frequency (RoCoF) following a contingency event. If the inertia has a low or zero value due to a lack of synchronous generators, then the electrical network might experience large undesirable frequency deviations. To overcome this problem, in previous research, mathematical methods have been emerged to enhance the system stability in current source based microgrids. The battery energy storage system embodies the isolated microgrid operates as voltage source and the implementation of the demand side management in which frequency is used as a communication signal. The requirement for inertia frequency response is indicated. In this work, the power electronics methods to emulate virtual inertia have been found, developed and compared. An overview of existing methods has been presented, which includes the introduction of the principles and comparison between their advantages and disadvantages. Based on those techniques, an additional PID controller has been recommended for the isolated microgrids working as current source. Furthermore, three control schemes to incite inertial responses have been developed for the microgrids in voltage source type as follows: 1) swing equation method, 2) low pass filter, and 3) an adjustable rate limiter. introducing swing equation, adding low-pass filter and setting rate limiter.","islanded microgrids; current source; voltage source; inertia emulation","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","","",""
"uuid:190ebea1-60f9-44ad-b101-e2600f03dad7","http://resolver.tudelft.nl/uuid:190ebea1-60f9-44ad-b101-e2600f03dad7","Impact of Static Sea Surface Topography Variations on Ocean Surface Waves","Ying, Y.K.","Vuik, C. (mentor); Maas, L.R. (mentor)","2016","","","en","master thesis","","","","","","","","2016-08-30","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","","",""
"uuid:4892c8ce-2961-455d-9e6c-bb821e221363","http://resolver.tudelft.nl/uuid:4892c8ce-2961-455d-9e6c-bb821e221363","Wave-induced stem breakage in a vegetated foreshore and its implication on probability of flooding","Suh Heo, H.Y.","Jonkman, S.N. (mentor); Vuik, V. (mentor); van Vuren, B.G. (mentor); van Prooijen, B.C. (mentor)","2016","Various flood protection measures are studied across the globe, and nature-friendly and environmentally resilient methods are gaining more attention. As part of the building with nature initiative, the project BE SAFE (Bio-Engineering for SAFEty) studies the effects of a vegetated foreshore as a flood protection measure which is found to be very effective. Vegetation helps to reduce wave energy as the stems and canopy work as small hurdles and obstacles that the waves need to pass. In this process, the waves lose much of its energy and the wave height reduces. As a result, the wave height is lower at the shore and less force acts on the coastal dike. From previous research, vegetation is known to be a dominant measure of wave energy dissipation, but the detailed processes of how it interacts with waves is not well known. Until now, vegetation in the foreshore has either been completely ignored (by coastal dike managers) or considered healthy and abundant (by ecologists). This research acknowledges that vegetation exists, but its strength and stem density may vary depending on the location and time of the year. The focus of this research is understanding the interaction between vegetation and waves, as well as its implications to the probability of dike overtopping and flooding. As waves pass through vegetation, stems break from the wave forcing which results in a variation of stem density in time (season) and space (foreshore). In this research, the mechanical interaction between vegetation stems and wave force is assessed to understand the point of stem breakage. Further, the vegetation stem breakage is implemented into the wave energy balance and in the probabilistic model of V. Vuik to quantify the probability of overtopping and dike failure. Stem strength is quantified by the three-point bending test results from NIOZ (Royal Netherlands Institute for Sea Research), which is used to calculate the maximum allowable stress of the stem. This stem strength is compared to the wave-induced stress which is formulated by taking the Morison-type equation to quantify a uniform wave load acting over the submerged length of the stem. In this mechanical analysis, stems are assumed to break when the wave load exceeds stem strength. Stem breakage is then implemented into the wave energy balance formulas, through which the stem density variation affects the amount of wave energy dissipation by influencing the wave height transformation. A correction factor is introduced to take into account leaning stems, but the correction factor could also include other simplifications that are not accounted for. Further, the performance of the wave energy balance is assessed through a sensitivity analysis of incoming wave height and seasonal vegetation data. Seasonal vegetation data and stem breakage is implemented to the probabilistic model of V. Vuik which quantifies the probability of flooding (due to overtopping). Vegetation and correlation scenarios are tested to find the optimum approach and address the uncertainty in the result. Of the different vegetation scenarios, the most realistic approach is the percent stem breakage which evaluates wave load to the normal cumulative distribution function of stem strength. Further, the uncertainty of model results is reduced by using the correlation scenario with characteristic relations between vegetation parameters. Including vegetation stem breakage in the probabilistic model produces reasonable results, yet further research to calibrate the correction factor and to better define characteristic relations would strengthen the model result.","stem breaking; stem breakage; vegetated foreshore; wave energy balance; FORM; first-order reliability method; probability; flooding; overtopping; dike; probabilistic model; BE SAFE; Bio-Engineering for SAFEty; Building with Nature; BwN","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","","",""
"uuid:a78669fd-e6b3-4f9b-8690-3423f11d3354","http://resolver.tudelft.nl/uuid:a78669fd-e6b3-4f9b-8690-3423f11d3354","Topology optimization for heat flow manipulation","Neofytou, A.","Böttger, A.J. (mentor)","2016","The aim of this thesis is to explore the use of topology optimization in designing materials for heat flow manipulation. Specifically shielding, concentrating or even inverting the heat current was examined. The possibility to scale the problem from macro- to nano-dimensions by using topology optimization for nanoscale heat conduction was also investigated. An optimization tool for heat flux shielding, focusing and reversal is presented. Different objective functions were defined in order to shield, focus or reverse heat flow within an area surrounded by a composite material with anisotropic thermal conductivity. The composite material under consideration consisted of thermally conductive elliptical inclusions embedded in a poor conductive matrix. The effective thermal conductivity was defined based on the equivalent inclusion theory. The dependence of the anisotropic thermal conductivity on the angle that the inclusions make with the imposed gradient was then approximated via a coordinate transformation. The material distribution method for topology optimization was then used to find the optimum angle distribution of the inclusion angle for heat shielding, focusing and reversal. The use of topology optimization for heat conduction in nanostructures was also investigated. At such small length scales the classical Fourier model for heat conduction is no longer applicable. As the dimensions of a structure become comparable to the mean free path of heat carriers (electrons or phonons), heat transfer changes from diffusive to semi-ballistic. Length scale effects such as scattering of the heat carriers with other phonons, material interfaces and boundaries become important. To take into account all this length effects, a more accurate model is given by the kinetic theory. Specifically the Boltzmann Transport Equation (BTE) for phonon transport was used to model the transfer of heat. To solve the BTE, a deterministic discrete ordinates method (DOM) was used in combination with the finite element method. Using COMSOL Multiphysics, the Boltzmann transport equation was successfully coupled with the MMA optimization algorithm and a Helmholtz PDE based filter. The method was implemented for the optimization of a simple system in order to illustrate its applicability.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Materials Science and Engineering","","","",""
"uuid:ef9900a4-f6c6-49c8-ae74-9059d6431725","http://resolver.tudelft.nl/uuid:ef9900a4-f6c6-49c8-ae74-9059d6431725","Inter-pass ultrasonic impact treatment (UIT) of welds","Feng, H.","Hermans, M.J.M. (mentor)","2016","","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Materials Science and Engineering","","","",""
"uuid:4a874e77-f69e-41de-9d1a-3d3ff41bc9c7","http://resolver.tudelft.nl/uuid:4a874e77-f69e-41de-9d1a-3d3ff41bc9c7","Molecular simulation of NH3/IL mixture for absorption refrigeration cycle","Kabra, A.","Infante Ferreira, C.A. (mentor)","2016","A molecular based Monte Carlo (MC) simulation method is used to predict the performance of the absorption refrigeration cycle involving NH3/IL (refrigerant/absorbent) pair as a working fluid. To investigate the thermodynamic performance of the absorption refrigeration cycle, various properties such as density, excess enthalpy or absorption heat, heat capacity and solubility of refrigerant in the absorbent are required. For this reason, MC simulations as an alternative to experiments, are used to compute the required properties. MC simulations in the osmotic ensemble are used to compute the solubility of NH3 in ILs. The ideal part of the heat capacity of pure IL is computed using quantum mechanical calculations and the residual part along with the density are computed using MC simulations in the NPT ensemble. The excess enthalpy for the mixture is calculated using MC simulations in the osmotic ensemble in combination with the NPT ensemble. The simulations are performed at temperatures ranging from 303 K to 393 K and pressures ranging from 4 to 18 bar. The performance parameters such as Coefficient of Performance (COP) and circulation ratio (f) of NH3 paired with two ILs, [emim][Tf2N] and [emim][SCN], are investigated using MC simulations and compared with the results obtained from correlated experimental data. The results from the MC simulations are in reasonable agreement with those obtained from the correlated experimental data. Hence, MC simulations can be used as an inexpensive alternative for preliminary design considerations involving potential working fluids for absorption refrigeration cycles in the absence of available experimental data.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Process and Energy (P&E)","","","",""
"uuid:76278819-2c45-4110-8790-19ad87644678","http://resolver.tudelft.nl/uuid:76278819-2c45-4110-8790-19ad87644678","Behaviour of structural glass at high temperatures","Nodehi, Z.","Nijsse, R. (mentor); Veer, F.A. (mentor); Louter, C. (mentor); Schipper, H.R. (mentor); de Krom, D. (mentor); van den Berg, G. (mentor)","2016","This thesis presents a study on the behavior of none-structural and structural glass at high temperatures. The main objective of this master thesis is to investigate the capabilities of different glass types when these are used as a structural element with fire resistant requirements. It is therefore important to develop an understanding of the behaviour of various glass types subjected to fire. A literature study is done on glass properties, glass production and methods for making structural glass. The purpose of this study was to investigate the background information and to gain adequate knowledge about the state of art on the thesis topic. Moreover thermal and mechanical properties of glass were determined. Results of these studies show that all glass properties depend on temperature. Subsequently the finite element model (first DIANA model) was made to simulate a glass test which was performed by others at Empa in Switzerland. A laminated beam model was made in DIANA software. This beam consists of 3 soda lime glass layers and two sentry glass interlayers. This model was used for making a sensitivity analysis about the glass properties which were found during the literature studies. The sensitivity analysis showed that from the material properties uses as input for DIANA models, the heat specific property has the most influence on the glass temperature. As next step, six experiments series were performed at the laboratory of Efectis Netherlands BV in Bleiswijk. The purpose of these tests was to investigate fire behaviour of three different glass types. Annealed, fully tempered and heat strengthened glass were tested horizontally and vertically in different conditions during these experiments. Results of each test were analyzed and discussed before performing the next test. This way, the knowledge about the fire behaviour of the glass and the right method for test performing was developed step by step during this research. Annealed glass performance was very weak at high temperatures due the thermal break phenomenon. It seems to imply that annealed glass (without any improvement) does not have the capabilities to be used as a fire resistant structural element. Based on the different behavior of steel and glass at the high temperature, can be noted that the thermal break phenomenon is occurred at the joint locations (connection between glass and steel support). For this reason joint details need extra attentions if glass needs to be used as a fire resistant structural element. The tests observations shows that 550-600°C is the critical temperature for both fully tempered and heat strengthened glass types. The average furnace temperature was around 615°C when glass temperature reached the critical temperature (550°C). The glass behavior changed to plastic phase at the critical temperature. Therefore the glass was not solid anymore and behaved more like liquid material. Afterwards the glass was not able to carry any loads, even not the self-weight. Through the data analysis and the observations of test series 1 to 6 seems to imply that fully tempered and heat strengthened glass, with some improvements, have the capabilities to be used as a structural material under fire loads. It should be mentioned that the relation between maximum allowable tensile stress and temperature, need to be determined first. This information is essential in order to determine if fully tempered glass can be used as fire resistant element. Test setups and the results of the tests are discussed in the chapter “Tests at Efectis” in more details. Single glass layer is tested during all the experiments series. The results show that one layer was not sufficient. This means that using laminated glass is essential in order to have enough fire resistance. The interlayers and the outside glass layers can keep the temperature of the middle layer(s) (loadbearing part) below the critical temperature for certain amount of time. Second DIANA model was made for one of Efectis test in order to see if the finite element model could give a correct estimation of glass behavior at high temperatures. The model results were close to the ones found during the tests. This means DIANA model has the capabilities to predict glass behavior at high temperature. In order to be able to make a model with more precision, glass properties need to be determined. For this reason several material tests should be performed at the beginning of the next research. Metal wires are used for the deflection measuring. The results show that these wires are extended considerably during the performed tests. For this reason this method is not useful for a correct deflection measuring. A new measuring method should be determined in order to have no physical connection between measurement equipment and glass. This way makes measuring glass deflection directly (without any instrument like metal wire) possible. Thermocouples were used for measuring glass temperature surface. These thermocouples were installed on the glass surface. Thermocouples can be placed between layers of laminated glass during the glass production. This way makes more precision temperature measuring possible.","fire behaviour; testing; structural glass; annealed; fully tempered; heat strengthened","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Building Engineering","",""
"uuid:d2c67a64-f730-4bd6-9681-875dbb2e448e","http://resolver.tudelft.nl/uuid:d2c67a64-f730-4bd6-9681-875dbb2e448e","Technology for development: Understanding how frugal innovation processes can achieve both commercial success and developmental impact in the base of the pyramid","Kamath, R.","Kamp, L.M. (mentor)","2016","Advocates for the eradication of poverty through private sector initiatives contend that firms can establish new markets, achieve significant profits as well as eradicate poverty in the BoP through frugal innovations. However, the BoP markets have proved to be an extremely difficult proposition for firms and only a few frugal innovations have been able to meet with commercial and developmental success. There is simply, not enough clarity in existing literature on how frugal innovations can be developed and implemented so as to cause both private profits and socio-economic development in the BoP. Therefore, this study sought to understand the determinants of the success of frugal innovations in the BoP by adapting the conventional functions of the innovation system framework to the case of frugal innovation processes through literature review, expert interviews and case study analysis. The result was the first version of a systemic framework to understand what conditions have to be created and managed so that frugal innovation processes are successful both commercially and in terms of developmental impact in the BoP. The framework is a foundation upon which further empirical research can be conducted to understand the success of frugal innovations in the BoP.","","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","","",""
"uuid:07499c0a-c23d-4af7-a4e3-b41ad8e69954","http://resolver.tudelft.nl/uuid:07499c0a-c23d-4af7-a4e3-b41ad8e69954","Heat Recovery in the Drying Process of Milk Powder by Using a Liquid Sorption System","Pineda Quijano, D.F.","Infante Ferreira, C.A. (mentor)","2016","The last step in the production of milk powder is drying, a very energy intensive process that demands 30% to 40% of the total energy input of a typical plant. It takes place in Spray Dryers (SD), where concentrated milk is sprayed and placed in direct contact with hot and dry air that cools down and gains humidity as water is evaporated from milk. The warm and humid air leaving the SD contains a small portion of potentially recoverable sensible heat and a large portion of latent heat that is impractical to recover by direct condensation due to the low dew point of this stream and due to the presence of milk powder particles that become sticky at high relative-humidity values. In this research, the feasibility of a liquid sorption system for the recovery of the aforementioned latent heat was investigated. The system proposed has two main advantages: the dehumidification of air in the absorber for reuse in the SD, and the production of high pressure steam in the regenerator for integration to the steam network of the plant. A mathematical model of the system was implemented in Matlab, and several system configurations were evaluated. The calculations showed that a SD equipped with this system can achieve energy savings above 55% when using aqueous solutions of phosphoric acid as liquid desiccant. The challenge with this liquid desiccant remains on the construction materials","liquid sorption; heat pump; drying; heat recovery; energy efficiency; phosphoric acid; sodium hydroxide","en","master thesis","","","","","","","","2021-08-31","Mechanical, Maritime and Materials Engineering","Process and Energy","","","",""
"uuid:ab4bd69f-6ea2-460f-9b16-4c3d579b48de","http://resolver.tudelft.nl/uuid:ab4bd69f-6ea2-460f-9b16-4c3d579b48de","A High-Resolution Energy-Efficient Resistor-Based Temperature Sensor","Pan, S.","Makinwa, K.A.A. (mentor)","2016","High-resolution energy-efficient temperature sensors are often used for the temperature compensation of frequency references, and of a wide variety of other sensors. Of the various types of CMOS-compatible temperature sensors, those based on resistors are, at least in principle, the most energy efficient. The challenge is then how to design similarly energy-efficient readout circuitry. By using a second-order phase-domain delta-sigma ADC and a stable frequency reference, a resistor-based temperature sensor achieves 410uK resolution in a 5ms conversion time, while consuming only 160uW. This corresponds to a resolution FoM of 0.13pJ*K2, currently the best reported for a CMOS-compatible temperature sensor.","","en","master thesis","","","","","","","","2021-08-30","Electrical Engineering, Mathematics and Computer Science","Microelectronics","","","",""
"uuid:3b7705e6-69b2-4bc5-a611-201a6f1302e1","http://resolver.tudelft.nl/uuid:3b7705e6-69b2-4bc5-a611-201a6f1302e1","Thermo-Mechanical Fatigue-Lifetime Determination of Diesel Engine Exhaust Manifolds","Kalra, A.","Janssen, M. (mentor)","2016","The start-up and shut-down cycling of a heavy-duty truck engine leads to temperature and mechanical strain variations in the exhaust manifold, subjecting it to thermo-mechanical fatigue (TMF) damage. For the automotive manufacturers, it is essential to accurately predict the TMF lifetimes of the exhaust manifolds under service conditions. The study aims at predicting the TMF lifetime of a ferritic SiMo ductile iron, the material used for the exhaust manifolds in the heavy-duty trucks manufactured by DAF Trucks N.V. In a quest to predict TMF lifetime, two approaches are investigated in this study, namely, the Paris-law approach and Total Strain version of the Strain-Range Partitioning (TS-SRP) method. Paris-law approach is a linear-elastic fracture mechanical approach and predicts the TMF lifetimes based on the stress intensity range. The TS-SRP method is a strain based approach and predicts the TMF lifetimes based on the mechanical strain range and strain range versus cyclic-lifetime data. In this study, the strain range versus cyclic-lifetime data is generated by performing cyclic tests, until failure, at bithermal and isothermal conditions. The TMF lifetimes predicted by the Paris-law approach and TS-SRP method are compared with the actual (experimental) TMF lifetimes. The Paris-law approach and TS-SRP method coupled with bithermal testing are successful in accurately predicting the TMF lifetimes. However, the TS-SRP method coupled with isothermal testing failed to predict the TMF lifetimes accurately.","Thermo-mechanical Fatigue; High Temperature Fatigue; Fracture Mechanics; Strain-Range Partitioning; Bithermal Testing; Exhaust Manifold","en","master thesis","","","","","","","","2021-08-30","Mechanical, Maritime and Materials Engineering","Materials Science and Engineering","","","",""
"uuid:f5282734-04db-41f8-9e5f-5ecd0fa80985","http://resolver.tudelft.nl/uuid:f5282734-04db-41f8-9e5f-5ecd0fa80985","Development of a dynamic multi-bed Pressure Swing Adsorption process for high purity hydrogen production from Coke Oven Gas: A mathematical modeling approach","Ramani, B.","Buijs, W. (mentor); Jägers, G. (mentor)","2016","Coal is certain to play a major role in the world’s energy future due to its low cost and widespread distribution around the world. At the same time, being the largest contributor to global CO2 emissions from energy use, coal faces significant environmental challenges in terms of air pollution and global warming. The study focuses on developing a pressure swing adsorption (PSA) technology that will allow for continued operation of coal, while improving its energy-efficiency by better utilisation of its by-product coke oven gas from steel industries. A generic, fast and robust simulation tool for simulating a variety of pressure swing adsorption processes considering both equilibrium and kinetic effects using a detailed non-isothermal and non-isobaric model is developed using MATLAB in the study. The adsorption equilibrium data required for the model are first calculated from experimental results using the non-linear regression data fitting method. Activated carbon and zeolite LiX adsorbents are modelled using Spartan molecular modeling software and simulated for estimation of the adsorption equilibrium data. The isosteric heat of adsorption values estimated from the simulation of molecular models matched well with the values calculated from experimental results showing the potential of molecular modeling tool in estimating the adsorption equilibrium data in a simple, fast and cost-effective way. A series of rigorous parametric studies and breakthrough tests are performed using the developed mathematical model for better understanding of the effects of different factors on the PSA process performance. With the better understanding obtained from the above-mentioned parametric studies, the model is optimised by performing several simulation tests to achieve the best possible process performance in terms of purity and recovery of the H2 product, productivity of the adsorbents and energy consumption for compression of gases. The optimised 14-step multi-bed PSA cycle developed in this study allows for an improved energy efficiency of coal usage by better utilisation of its by-product coke oven gas by converting it into valuable high purity (>99.999%) hydrogen fuel for mobility applications with a recovery of over 75%.","coal; steel industry; coke oven gas; gas separation; hydrogen purification; adsorption equilibrium; heat of adsorption; molecular modeling; activated carbon; zeolite; dynamic mathematical modeling; pressure swing adsorption; breakthrough curves; parameter studies; model optimisation","en","master thesis","","","","","","","","2018-12-31","Mechanical, Maritime and Materials Engineering","Process and Energy","","","",""
"uuid:4cf2c369-b7b9-4db8-99da-9ab5b436f64e","http://resolver.tudelft.nl/uuid:4cf2c369-b7b9-4db8-99da-9ab5b436f64e","Solving the open day timetabling problem using integer linear programming","Gecmen, D.","Van den Berg, P.L. (mentor)","2016","The timetabling problem is known as a large class of problems that fall under the mathematical field of scheduling. A widely used method to solve timetabling problems is the integer linear programming method, which we have used to solve the open day timetabling problem. In this thesis we have addressed a timetabling problem for the open day at the Christian College Groevenbeek. This open day consists of two separate day parts: the morning and the afternoon. For both day parts we have received a data set. These data sets consist of students and their preferred studies. To solve the problem we created several mathematical models formulated as an ILP. After that, we implemented these models in AIMMS to solve them for the data sets we have received. The first model we created was the feasibility model. We used this model to determine the appropriate number of lecture halls and lecture hall capacities when we have 4 rounds on both day parts. To achieve 4 rounds on both day parts we found that the best combination to have is 19 lecture halls with a capacity of 30 and 1 lecture hall with a capacity of 40. In addition, for a good quality schedule objective functions were considered. The first objective was to minimize the number of presentations. The second objective was to minimize the total workload. The workload of a teacher is the total time a teacher is present at the college, which consists of the number of presentations he has to give and the number of gaps he has in his schedule. A gap is a round where a teacher is not scheduled to give a presentation, but has to be present. We added each objective and their corresponding constraints to the feasibility model. After applying both models to the data sets we concluded that the second objective resulted in a better schedule, as it achieves the theoretical minimum number of presentations and creates zero gaps in the schedules for both day parts. When we combined the schedules for both day parts this did not result in a good schedule as some studies still contained gaps between the two day parts. To improve this we minimized the total workload for both data sets combined.","scheduling problem; integer linear programming; optimization","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Applied Mathematics","","","",""
"uuid:a2b6f58f-db23-45ea-952e-5092c21dded9","http://resolver.tudelft.nl/uuid:a2b6f58f-db23-45ea-952e-5092c21dded9","Compression Strength of Wrinkled Composite Laminates","Manikarnika, K.","Kassapoglou, C. (mentor)","2016","MSc degree in Wind Energy at DTU in Denmark, and MSc degree in Aerospace Engineering at TU Delft in the Netherlands - Wrinkle defects are prevalent in wind turbine blades and are a leading cause of blade failure. Accurate modeling of the compression strength of wrinkled composite laminates is needed by manufacturers. Based on the creation of a parametric nite element model of a through-thickness wrinkle defect, along with a series of geometric and material parameter studies, this research seeks to determine the compressive strength of various kinds of wrinkled composite laminates. A 2D FE model of alternating layers of elastic-plastic matrix and elastic orthotropic ber laminae is used. Plastic microbuckling (kinking) strength is given by the limit load, while delamination strength is determined from a maximum shear stress criterion. The main conclusion is that compression strength of a wrinkled laminate is strongly dependent on the maximum initial fiber misalignment angle. There is also strong sensitivity to the height to length ratio. Furthermore it is found that higher matrix yield strength increases compression strength. Finally, it is found that in hybrid wrinkles, an increasing amount of carbon increases the compression strength.","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures & Materials","","European Wind Energy Master (EWEM)","",""
"uuid:e14559e1-5bfc-4ef5-a143-9de2e26c1ec8","http://resolver.tudelft.nl/uuid:e14559e1-5bfc-4ef5-a143-9de2e26c1ec8","Recursive Subspace Identification with Predictive Control: a Nuclear Norm approach","Telsang, B.","van Wingerden, J.W. (mentor)","2016","The main contribution of this thesis is the development of an inherently adaptive controller which recursively implements a system identification and control routine. The main idea here, is that if system identification techniques can be made computationally light, and if they can be combined efficiently with a controller formulation, such a framework can be implemented on real world systems to achieve the adaptive nature in control. In the first part of the thesis, we focus on developing a computationally less expensive identification technique, by making various modifications on the N2SID algorithm, a nuclear norm subspace identification method. By allowing for early stopping of the ADMM algorithm (which plays a crucial role in N2SID) and by implementing an efficient alternative to Singular Value Thresholding, we are able to obtain significant reductions in computation time. Furthermore, we allow for a recursive implementation of N2SID by utilizing system information from the previous identification cycle. This results in a significant improvement of the convergence speed of the identification. In order to create an efficient interface between the identification technique and the controller, we formulate an MPC control framework, which does not require the explicit computation of the system matrices at each identification cycle. This helps in reducing the computation time of the recursive algorithm. The overall methodology is such that an inherently adaptive controller is in play at every time instant. To test this algorithm, we implement it on different systems, including LTI and LPV systems.","Subspace identification; Nuclear norm; Recursive identification; Alternating Direction Method of Multipliers (ADMM); Model predictive control; Singular value thresholding","en","master thesis","","","","","","","","2016-08-29","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control (DCSC)","","","",""
"uuid:522bb16a-49a2-480d-b12e-d68d4621789b","http://resolver.tudelft.nl/uuid:522bb16a-49a2-480d-b12e-d68d4621789b","The effect of overlapping passive zones in sand investigated by a geotechnical centrifuge model","El Banna, E.S.D.","Arend, D.E. (mentor); Bakker, K.J. (mentor); Everts, H.J. (mentor); Hicks, M.A. (mentor); Askarinejad, A. (mentor)","2016","This thesis presents an investigation on the effect of overlapping passive zones in sand. The hypothesis of larger stresses in the soil and a larger ultimate resistance capacity in case of overlapping passive zones was firstly investigated in the numerical study by Joosse (2015). He introduced an intensification factor as a design optimization tool for economic design and optimization of retaining walls in narrow trenches. The intensification factor is defined as the ratio between the ultimate passive capacity in a restrained situation (overlapping passive zones) and in an unrestrained situation (no overlapping effects). To verify the numerical design approach, a physical model was developed and used for centrifuge testing by Hopman (2016). The developed actuator is used again in this thesis to do extensive centrifuge tests to model the effect of overlapping passive zones in sand. Together with the newly developed sand model preparation method, as introduced in this thesis, it is possible to produce accurate and repeatable sand samples. The main objectives of this thesis are to investigate the effects of a variation in density and the effect of a layered sand sample on the intensification factor. The intensification factors in this thesis are determined at wall displacements corresponding to 0.01, 0.02 and 0.04 times the embedded depth (D) as well as at the wall displacement corresponding to the ultimate passive load. The ultimate passive load is found by the plateau state in the Load-displacement curve and by the development of the full shear plane as visualized by image analysis. It turns out that the ultimate passive load and the formation of the associated shear band have a significant dependency on the relative density. Although the ultimate passive loads differ significantly, the intensification factors at the plateau states are similar with 1.33 and 1.34 for the loose and dense sand respectively. The shapes of the shear planes as visualized by image analysis are in accordance to theory and numerical calculations. Grain size seems to be of less importance on the ultimate passive load. Also the intensification factor at plateau state for coarse sand is similar to that of fine sand. Investigation on the friction between the plexiglass and sand showed that by applying membranes to the plexiglass with silicon grease, frictionless boundaries could be created. It turns out that the friction has a significant impact on the width of the shear planes in the physical model. The observed shear planes are smaller than in a frictionless environment. The shear planes develop up to 29% further in the frictionless unrestrained situation. The reduction in friction also means a reduction in measured loads by the actuator which must be taken into account when using the results from the load vs. displacement data. The intensification for a trench with varying embedded depth but constant trench width (W) over embedded depth ratio is investigated with centrifuge tests. When comparing an embedded depth of 3 m and 5 m with a constant W/D ratio of 2.0, the intensification factor turns out to be identical. The shape of the shear plane is proportional to the embedded depth. The intensification in layered sand consisting of loose sand on top of dense sand in an equal division is investigated. The load curves for the layered sand are bounded by the load curves of homogenous loose and dense sands at the bottom and top respectively. Because the loads in unrestrained situation are higher in proportion to the restrained loads, the intensification turns out to be significantly less than for homogenous sands.","geotechnical; centrifuge; overlapping; passive; zones; sample; preparation; method","en","master thesis","","","","","","","","2018-08-29","Civil Engineering and Geosciences","Geoscience & Engineering","","Geo-Engineering","",""
"uuid:5e188e81-9a6f-4e30-b38e-6fee3e40131f","http://resolver.tudelft.nl/uuid:5e188e81-9a6f-4e30-b38e-6fee3e40131f","An efficient algorithm for the VRPTW with short routes","Van Tatenhove, J.","Gijswijt, D.C. (mentor); Gebler, E.D. (mentor)","2016","We introduce a competitive algorithm for the Vehicle Routing Problem with Time Windows (VRPTW). The algorithm will be tweaked to perform optimally in a specific situation, since this algorithm will be used when planning the delivery of groceries in an urban setting. We compare this algorithm to established algorithms on benchmark instances, and then we tweak further and introduce new concepts in order to fully support the specific demands of the company the algorithm is created for. The algorithm outperforms state-of-the-art algorithms when routes are short.","","en","master thesis","","","","","","","","2019-08-29","Electrical Engineering, Mathematics and Computer Science","Mathematics","","","",""
"uuid:d8188239-e20e-40b1-bc0c-760aa85edb99","http://resolver.tudelft.nl/uuid:d8188239-e20e-40b1-bc0c-760aa85edb99","Portfolio Scheduling for Risk Management in Business-Critical Workloads Hosted by Cloud Datacenters","Oikonomou, G.","Iosup, A. (mentor)","2016","Large enterprises and governments shift their long-running and critical operations to run in cloud environments. The on-demand business-critical workloads raise important challenges in datacenter operation, requiring efficient online scheduling of workloads with unprecendented dynamic behavior, yet under strict service level agreements (SLAs). The topic of this work is on the resource management in clouds and especially on VM placement of business-critical workloads using portfolio scheduling, an online scheduling technique that dynamically selects a scheduling policy from a set (portfolio) by optimizing given utility functions. We focus more on utility functions which address risks in clouds of applications not meeting specific SLAs. By using risk-aware portfolio scheduling, we achieve to mitigate the severity of resource contention experienced by workloads (Operational Risk) and secure that, even if a whole datacenter fails, workloads can be absorbed by the remaining datacenters in the system (Disaster Recoverability Risk). We investigate portfolio scheduling by experimenting with real production and synthetic workloads derived from the production environment of Solvinity, a hosting provider company in the Netherlands.","portfolio scheduling; datacenter resource management; risk management; risk tolerance; operational risk; disaster recoverability risk","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Software Technology","","","PDS Group",""
"uuid:5cc9ecf4-0e39-44c7-865e-85b5407d335a","http://resolver.tudelft.nl/uuid:5cc9ecf4-0e39-44c7-865e-85b5407d335a","Microstructual evolution of a dual phase steel during continuous annealing","Wu, X.","Sietsma, J. (mentor)","2016","With a combination of high strength and good formability, dual-phase steels have gained much interest in the automobile industry. The final microstructure, which leads to mechanical properties of the product, is strongly dependent on the processing of the steel. Apart from hot-rolling and cold-rolling, the parameters during continuous annealing are of great importance to control the properties of the final product. This thesis is aimed to study the influence of heating rates on the microstructual evolution of a DP-steel during continuous annealing lines. The microstructual evolution evolves several phenomena, such as ferrite recrystallization, pearlite degradation, and austenite phase transformation. Experimental approaches as dilatometry and SEM are used to investigate the evolution, by applying different heating rates and stop-quenching at different temperature and soaking times. Modelling approaches, CA-model and phase field model, are also introduced to provide a further insight into the phenomena.","dual-phase steel; austenitization; continuous annealing","en","master thesis","","","","","","","","2017-08-29","Mechanical, Maritime and Materials Engineering","Materials Science and Engineering (MSE)","","","",""
"uuid:c60b67e2-0f3b-4879-a998-eae3b6170f00","http://resolver.tudelft.nl/uuid:c60b67e2-0f3b-4879-a998-eae3b6170f00","Hardware Acceleration of Computer Vision Application","Al-Naqshbndi, S.","Bertels, K.L.M. (mentor)","2016","Demand for computing is growing, especially in the form of digital image processing and computer vision because of their applications in military equipment, industrial environment, automobiles, medical and entertainment products. Edge detection is one of the fundamental steps in digital image processing and computer vision and Canny is the most widely used edge detection algorithm. This algorithm is comprised of computationally intensive stages like smoothing, gradient computation, non-maximal suppression etc, which have high processing demands, especially for embedded computing. Hardware/software co-design appears to be a promising solution to satisfy the computation demands of many applications, especially image processing and computer vision. It has especially gained popularity due to the tight coupling between reconfigurable hardware and software on a single chip, such as Zynq by Xilinx and the availability of better tools. However, programmability of such devices is still a challenge. In this thesis, we present an efficient mapping of Canny edge detection application on Zedboard which uses Zynq as target processor. The goal is to accelerate the application without losing the accuracy as compared to the original software implementation. We highlight various challenges faced by the developer in the form of detailed memory and communication profiling and careful analysis of applications and the requirement to understand the hardware details. We highlight how tools like MCProf, can be useful to perform intelligent application mapping on processors like Zynq. We back our study with simulation and actual implementation results at kernel and application level to provide a quantitative comparison of various optimizations. We obtained upto 7x and 3.18x speedup at kernel- and application-level, respectively. Though we have used Canny as case-study, the analysis presented in this thesis is also applicable to other image processing and computer vision applications.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Computer Engineering","","","",""
"uuid:4957df8e-3de1-4b5e-8231-731287a4ede4","http://resolver.tudelft.nl/uuid:4957df8e-3de1-4b5e-8231-731287a4ede4","Modeling and simulating the Zika outbreak under deep uncertainty: a multi-method multi-resolution approach","Schwarz, P.","Pruyt, E. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi Actor Systems","","Engineering & Policy Analysis","",""
"uuid:60250061-7853-407f-af37-b7aac524d32e","http://resolver.tudelft.nl/uuid:60250061-7853-407f-af37-b7aac524d32e","Business air travel decision making and the effect of increasing ticket prices","Noordman, R.","van Wee, G.P. (mentor); Annema, J.A. (mentor); Kwakkel, J.H. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","Transport & Logistics","",""
"uuid:3e4e265b-b84c-474b-85b9-9840a5c85464","http://resolver.tudelft.nl/uuid:3e4e265b-b84c-474b-85b9-9840a5c85464","Truck Platooning: Enablers, Barriers, Potential and Impacts","Bakermans, B.A.","van Arem, B. (mentor); Wiegmans, B.W. (mentor); van Duin, J.H.R. (mentor); de Vreeze, M. (mentor)","2016","Truck platooning refers to the automated operation of multiple trucks. High expectations rest on truck platooning as to alleviate negative impacts caused by road freight transport like congestion, accidents and pollution. Furthermore, driver shortages may be partially solved by the concept of truck platooning. This study identified enablers and barriers of truck platooning and assessed their effect on the potential implementation of the concept. The implementation and possible adoption of truck platooning as a transport mode is a complex and uncertain process. The magnitude of the consequences of platooning are in general unknown and many different stakeholders should be involved in the implementation process. Based on this research it can be concluded that several barriers still have to be mitigated and that cooperation between different competing companies and industries is essential for the successful implementation of truck platooning.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport & Planning","","","",""
"uuid:9827d7c4-1885-41a4-a505-bbc47fc899e2","http://resolver.tudelft.nl/uuid:9827d7c4-1885-41a4-a505-bbc47fc899e2","Image-Based News Recommendation","Corsini, F.","Larson, M.A. (mentor)","2016","","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Intelligent Systems","","Multimedia Computing","",""
"uuid:f206d7e0-e9b6-48e2-92c8-ef9e0e6e033f","http://resolver.tudelft.nl/uuid:f206d7e0-e9b6-48e2-92c8-ef9e0e6e033f","Exploring business model innovation in the Polish SME sector","Sobieraj, M.","Bouwman, W.A.G.A. (mentor); Roosenboom-Kwee, Z. (mentor)","2016","The business model (BM) and business model innovation (BMI) concepts have been given increasing attention in recent years by both academic researchers and business environment. BMI is seen as essential for company’s survival and growth and is increasingly being used to explain differences in firm performance. Nevertheless, little is known about how SMEs conduct BMI in practice; and what is the role of company’s top management and their leadership in this process. This research explores empirically the BMI process in the context of Polish SMEs. First, the semantic web, which depicts the relationship between the role of management team, its capabilities and leadership; and business model innovation process in SMEs is developed. Subsequently this framework is applied to analyze cases of two Polish SMEs, one start-up and one established small business, which went through BMI process. Findings from this research allow to better understand the nature of the business model innovation process in SMEs and the role of the top management and leadership in it. Therefore, this research helps to further advance the business model innovation theory. In addition, such insights have a practical relevance since they help to define the best managerial practices for stimulating business model innovation in SMEs.","Business Model Innovation; Leadership; SMEs","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","","",""
"uuid:ea068ea3-8891-405b-8d45-bfac1a331a96","http://resolver.tudelft.nl/uuid:ea068ea3-8891-405b-8d45-bfac1a331a96","Design tool for an offshore harbour","Kassimi, A.","Vellinga, T. (mentor)","2016","In this report a design tool has been developed for offshore harbours. The main objective of the design tool is to recommend an optimum value for the offshore distance of the berth, the breakwater length and the breakwater type. This recommendation has to be made based on CAPEX and OPEX estimates for the various structures the offshore harbour consists of, and based on downtime caused by wave action. After the determination of the functionality and usability requirements the design tool has to fulfill, an overview of all relevant design parameters and processes is presented. These parameters and processes are incorporated in the design tool via four modules: a cost module, a breakwater design module, a wave calculation module and a waiting cost module. Three verifications have been performed on the design tool, as well as a sensitivity analysis in order to obtain insight in the performance of the tool and the sensitivity of its model formulations to uncertainties in the input. Based on the results of these analyses, conclusions can be drawn with regard to the reliability and sensitivity of the design tool.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Ports & Waterways","",""
"uuid:1d5e11e4-1e61-469f-a9c2-d47a1b1ec2d3","http://resolver.tudelft.nl/uuid:1d5e11e4-1e61-469f-a9c2-d47a1b1ec2d3","Regularizing AdaBoost to prevent overfitting on label noise","Meijer, D.W.J.","Tax, D.M.J. (mentor)","2016","In this work, we have shown that AdaBoost is prone to overfitting when the training set contains mislabeled objects. We proposed that this is in part because the error estimate used to weight base classifiers and (indirectly) objects is biased in this scenario. We have shown that an unbiased estimator can prevent the overfitting, but such an estimator is not easily found when the training set is untrustworthy. To remedy this, we introduced the ValidBoost algorithm, which tries to unbias the error estimation AdaBoost makes by taking a validation set from the noisy training set. The size of the validation set increases logarithmically with the number of iterations, reaching 50% of the entire training set in the final iteration. The error ValidBoost uses is a weighted average of the default training error and this validation error, with the weight of the validation error also increasing logarithmically with the number of iterations. We have seen that ValidBoost performs well in comparison to AdaBoost and LogitBoost in the presence of label noise. Compared to Bylander & Tate’s method, a similar variant of AdaBoost also based on the use of validation sets to avoid overfitting, it performs very similar when using decision stumps as base classifiers, and slightly better when using decision trees as classifiers. Perhaps this is because we are using more training data than Bylander & Tate are and decision trees require this extra input in order for the base classifiers themselves not to overfit. The fact that ValidBoost samples a new validation set at every iteration also lead to positive effects. Whereas the other methods might stagnate or stop early because a classifier has been trained with an error above the threshold imposed by boosting, ValidBoost will have a different training set next iteration, thus no reason to stop early and a different error (possibly below the error threshold). This effect can also be seen when using non-deterministic base classifiers with AdaBoost. Because of the relative success of ValidBoost, we reasoned about its core idea and how this can be translated into other domains. Several suggestions were made for possible ways to incorporate it into bagging, neural networks or decision trees. Experiments on such an implementation in bagging (random forests specifically) showed promise, while experiments on an implementation in neural networks worked less well.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Intelligent Systems","","Pattern Recognition & Bio Informatics, PRLab","",""
"uuid:310c611c-2225-4fa1-8a92-bd48d82a6722","http://resolver.tudelft.nl/uuid:310c611c-2225-4fa1-8a92-bd48d82a6722","Investigation of Accuracy, Speed and Stability of Hyper-Reduction Techniques for Nonlinear FE","Ravichandran, T.K.","van Ostayen, R.A.J. (mentor)","2016","The field of Hyper-reduction for Nonlinear Finite Element Method attempts to address the large durations due to repeated evaluation and assembly of the internal force and Jacobian. Stability, accuracy and speed are three aspects of thesemethods that has been dealt with in this thesis. There are two methods that are popular within the FEMframework, these are, DEIM and it variants, and ECSW. By construction, DEIM is quite unstable and has convergence issues, as the Lagrangian structure is not preserved during hyper-reduction. A recent paper by Chaturantabut, preserves the structure while using DEIM and hence assures stability and passivity of the hyper-reduced model in the context of reducing internal forces that are scalar-valued. With this thesis the possibility of restoring the structure in the context of FEM, i.e., reduction of vector-valued internal forces is investigated. It is found that, the extension of structure preserving DEIM to FEM, did not work as expected, owing to certain characteristics of FEM. In DEIM, traditionally the degrees of freedoms (dofs) at which the internal force is evaluated is equal to the number of force modes. The effect of having more number of evaluations as compared to force modes is investigated. It is found that increase in the number of evaluations does improve accuracy and also the resulting stability, with increases in the computation time. ECSW is a recent hyper-reduction technique, and is stable as a result of the preservation of the Lagrangian structure. The properties of this method are investigated. As a conclusion to this thesis, a study is performed on the different methods across five examples of varied complexity. It is found that UDEIM with nodal collocation performs well with accuracy, speed and stability across all examples.","Nonlinear Model order reduction; hyper-reduction; Galerkin projection; ECSW; DEIM; structure preserving DEIM; nonlinear finite element; refine mesh","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering (PME)","","","",""
"uuid:f6008da9-d688-4b9f-9880-8d7c3b51a777","http://resolver.tudelft.nl/uuid:f6008da9-d688-4b9f-9880-8d7c3b51a777","Reinforcement Learning Policy Approximation by Behavior Trees: using Genetic Algoritms","Janssen, Y.S.","Scheper, K.Y.W. (mentor)","2016","","","en","master thesis","","","","","","","","2017-08-29","Aerospace Engineering","Control and Operations","","Control & Simulation","",""
"uuid:2b00d5e5-3b2f-4ac9-9f5c-dda9d92fe052","http://resolver.tudelft.nl/uuid:2b00d5e5-3b2f-4ac9-9f5c-dda9d92fe052","Review of the Applicability of Discrete Event Simulation for Process Optimization in Mining","Both, C.","Lottermoser, B. (mentor); Buxton, M.W.N. (mentor); Berner, M. (mentor)","2016","Although DES has become a well-accepted tool for decision-making in the mining industry, the time consuming modelling work and high programming effort is normally only considered feasible for large-scale projects examined in project-based case studies. This is why Knauf has developed an adapted simulation tool for mining for the use on a cross-project basis. There could be achieved valuable operational goals in each of the two performed case studies by the use of the adapted simulation tool. The quarry operations have been examined for profitability of stockpile use for ore transport, influence of dumper size on ore transport and transport capacity by increasing hauling distance using variable equipment composition. Gained knowledge during the case studies has been transferred to a simulation procedure suited for simulation studies in quarry mining. Important phases of simulation have been identified and strategies for successful completion including pre-designed spreadsheets have been developed. The procedure highly values the verification and validation of achieved results. Consistently, suitable test methods are proposed for each phase result, which include statistical tests for the data collection and data preparation phase as well as fixed value test and internal validity test for implementation and execution of the simulation model. The comparison of results achieved by simulation approach and deterministic spreadsheet approach has revealed that simulation is beneficial when interactions of equipment are hardly predictable due to their dependence on dynamic processes. Although the profitable use of stockpiling could be generally determined by both analytical methods, the DES model could benefit from reproduction of the decision-making behavior of the real stockpiling system and thus could determine most effective equipment composition. On the other hand, ineffective use of a bigger sized dump truck by increased hauling distance could be easily analyzed by deterministic calculation by leading only to small deviations compared to the simulation model. Although modeling effort of the simulation model is low by the helpful use of pre-defined modules, work effort of experimental evaluation, excluding data collection, is still six times higher than spreadsheet analysis due to the need of verification and validation methods and creation of confidence intervals for simulation results. Thus, applicability of DES should be limited to the analysis of more complex interaction of equipment. Reduction of work effort can possibly be achieved by replacing statistical distributions by deterministic values in the simulation model. However, every simplification must be checked individually for the unbiasedness of KPI’s.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","European Mining, Minerals and Environmental Program (EMMEP)","",""
"uuid:57344938-4672-4531-b38e-ce3c1ebfe7f6","http://resolver.tudelft.nl/uuid:57344938-4672-4531-b38e-ce3c1ebfe7f6","The sustainable energy advice market and its impact on public sustainable energy policy-making: Designing a framework and a case application for the Netherlands","Matton, R.Y.","Scholten, D.J. (mentor)","2016","Sustainable energy advice is provided in a market in which sustainable energy (SE) consultancies, such as the Energy Research Institute of the Netherlands (ECN), provide SE research for policies and public policy-makers demand for research. However, the Sustainable Energy Advice Market (SEAM) and the way in which the SEAM’s impact on SE public policy-making can be measured, has not been researched in the scientific literature or understood within ECN Policy Studies. On the basis of a literature study, a report analysis and expert interviews, we developed an impact measurement framework. To start with, the approach consists of a market delineation approach in order establish which consultancies can have an impact on SE policy-making. Subsequently, three impact indicators are identified to qualify and quantify the impact of the advisories on SE policy-making processes. Two of the three indicators show usability, validity and reliability, whereas the third indicator needs development in further research. This research will contribute to the current scientific literature by providing impact indicators – an impact measurement framework - to describe and quantify the impact of SE consultancies on public policy-making. Further research could test the validity of the framework in related advisory markets.","sustainable energy; impact measurement; framework design; public policy; consultancy","en","master thesis","","","","","","","","","Technology, Policy and Management","Economics of Technology and Innovation","","The Master Sustainable Energy Technology (SET)","",""
"uuid:f02a00bd-e7b3-4ad8-aa03-94618e60bcc1","http://resolver.tudelft.nl/uuid:f02a00bd-e7b3-4ad8-aa03-94618e60bcc1","Additively manufactured self-defending bone implants to prevent implant-associated infection","van Hengel, I.A.J.","Apachitei, I. (mentor); Zadpoor, A.A. (mentor); Zaat, S.A.J. (mentor)","2016","Background Due to an aging population, the incidence of total joint replacements has been increasing and is expected to double in the coming decade. Along with implantation, the incidence of implant-associated infection (IAI) has risen and forms a tremendous burden for health care. Therefore, effective preventive measures for IAI are desperately needed. The recent developments of additive manufacturing (AM) processes possess great potential to provide personalized orthopedic implants for future patients. Furthermore, AM generates new strategies to prevent IAI through the synthesis of antibacterial surfaces on AM implants that limit bacterial adhesion and/or promote bacterial killing. This study aimed to investigate the biomaterial characteristics and antibacterial activity of AM porous Ti6Al4V bonemini-implants. Methods Porous mini-implants designed in Solidworks and Matlab were synthesized with the AM process of selective laser melting (SLM). Thereafter the titanium oxide (TiO2) surface layer of these SLM mini-implants was modified by a plasma electrolytic oxdiation (PEO) process with a calcium and phosphate based electrolyte bearing silver nanoparticles (NPs). PEOprocessing resulted in PEO treated and silver-based (SLM PEO+Ag) mini-implants. SLM mini-implants were compared with solid miniimplants. Surface morphology of the mini-implants was analyzed by scanning electron microscopy (SEM), chemical composition by energy dispersive X-ray spectroscopy (EDS) and phase composition by X-ray diffraction (XRD). Silver ion release characteristics of SLM PEO+Ag mini-implants were studied for 1 month in a biomimetic environment. The antibacterial properties of the SLMPEO+Ag mini-implantswere evaluated by in vitro and ex vivo tests with Methicillin-resistant Staphylococcus aureus (MRSA). Furthermore, the suitability of mini-implants for in vivo testing was investigated through the implantation in a murine cadaver. Results The design resulted in mini-implants with interconnected porosity and increased surface area compared to solid mini-implants. The presence of silver NPs onto SLM PEO+Ag mini-implants was confirmed by SEM and EDS. In addition, the formation of hydroxyapatite was demonstrated by XRD analysis. Silver ion release from SLM PEO+Ag mini-implants persisted up to 1 month and was enhanced for SLM PEO+Ag compared to solid PEO+Ag mini-implants. Furthermore, antibacterial testing indicated that leaching activity by SLM PEO+Ag mini-implants resulted in a larger zone of MRSA growth inhibition compared to solid PEO+Ag mini-implants. Furthermore, SLMPEO+Ag mini-implants had a bactericidal effect on MRSA in in vitro and ex vivo experiments. Moreover, MRSA biofilm formation was prevented by SLM PEO+Ag mini-implants. Cadaver implantation demonstrated the suitability of the mini-implants for use in an in vivo murine bone infection model. Conclusion Porous, SLM PEO+Ag mini-implants demonstrated enhanced antibacterial properties compared to solid PEO+Ag mini-implants. Therefore, SLM PEO+Ag mini-implants possess great potential for further development towards clinically applicable antibacterial bone implants that prevent IAI.","","en","master thesis","","","","","","","","2020-08-29","Mechanical, Maritime and Materials Engineering","Biomechanical Engineering","","Biomaterials & Tissue Biomechanics","",""
"uuid:7e0069d4-89ae-4b44-9370-2c33bb1a97c3","http://resolver.tudelft.nl/uuid:7e0069d4-89ae-4b44-9370-2c33bb1a97c3","Chemical Reactions in Breakthrough Separations","Gonzalez, R.A.","Vlugt, T.J.H. (mentor)","2016","The goal of this work is to develop a model that can accurately represent reactive adsorption, and to apply that model to determine the feasibility of H2 production using steam methane reforming (SMR). A 1-D model of reactive adsorption that takes into account adsorption kinetics, and chemical reactions of gas species in a non-isothermal (inclusion of heat balance), and isobaric reactor was developed in MATLAB. The purpose of the model is to help understand the potential benefits of reactive adsorption systems. Examples of such systems are sorption enhanced steam methane reforming (SESMR) and sorption enhanced water gas shift (SEWGS), which are simulated using the model. The results obtained from the model are validated by comparison to the results of experiments and simulations done by Xiu et al. (2002) and Van Selow et al. (2009). The results show good agreement with those of the literature used, and show that the model can be adapted for use with different chemical reactions and adsorption kinetics. Using the SESMR model developed based on the experimental data of Xiu et al. (2002), four cases consisting of two major cases in which two regeneration fluids, and two sub-cases considering adiabatic (during the regeneration step) and non-adiabatic operation of the reactor were studied. The selection criteria used to determine which of the cases is the best alternative, was the option that provided the lowest power consumption per mole of H2 produced. The option that showed the lowest power consumption was that which used CH4 as the regeneration fluid with heat transfer from the wall. The power consumption of this option is 262.8Wmol-1 H2 which produced 4.1522 mol hr-1 H2 while operating at a temperature of 773 K. In comparison a SMR that produces the same molar flow rate of H2 operates at 900 K, but has a power consumption of 195.4 W mol-1 H2. Due to the different outlet compositions of both reactors, a specification of 99.99% purity of H2 is introduced so that the H2 produced can be used in a fuel cell. The purification of H2 is performed by a system of adsorption columns composed of two layers in series of activated carbon BPL 4x10 and zeolite 13x, as proposed by Delgado et al. (2014). The purification system designed for the SMR exhaust gases recovers 88.0% of the H2 with a system productivity of 6.03 mol H2 kg-1 h-1, and produces a stream of 100% pure H2. The purification system designed for the SESMR gases was reduced in size by approximately 27% when compared to that of the SMR. It also showed a greater H2 recovery at 90.1%, and a productivity of 10.21 mol H2 kg-1 h-1, while maintaining a purity near 100%. Combining the reactors and purification systems proposed, a simple economical analysis is performed to determine the feasibility of a plant using SESMR and another using SMR. The resulting experimental SESMR plant is more complicated when compared to the plant that uses the SMR, because it uses more equipment. The additional equipment is required to heat the regeneration fluid, and to be able to run the process semi-continuously. The plant using SMR has cooling, heating, and electric duties of 175.3, 221.3 and 26 W, respectively. In comparison the plant using SESMR has cooling, heating and electric duties of 145.6, 300.12 and 23 W, respectively. While the cooling duty for the plant using the SMR is higher than the one using the SESMR, the heating duty of the SESMR plant is approximately 30% higher. The large increase in required heat in the SESMR results in a lower operational income of 81.94 £/year when compared to the SMR which has one of 172.85 £/year. As such, it is difficult to justify the use of SESMR over SMR for the production of H2.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Process and Energy","","","",""
"uuid:2a64dd6e-2110-4c9e-8404-632054a33db9","http://resolver.tudelft.nl/uuid:2a64dd6e-2110-4c9e-8404-632054a33db9","Simulated Shadowgraphy of Transonic Wing-Bound Shock Waves","Onnink, T.S.","Schrijer, F.F.J. (mentor)","2016","","","en","master thesis","","","","","","","","2018-09-01","Aerospace Engineering","Aerodynamics, Wind Energy & Propulsion","","","",""
"uuid:d26bcc4c-08f5-40c4-9178-98c79917d918","http://resolver.tudelft.nl/uuid:d26bcc4c-08f5-40c4-9178-98c79917d918","Software Performance Engineering in Complex Distributed Systems","Versluis, L.F.D.","Pouwelse, J.A. (mentor)","2016","Performance is a make-or-break quality for software. When making changes it is essential to ensure no performance regression has occurred i.e. the program performs more slowly or consumes more resources than previous versions. Tribler is the result of ten years scientific research in complex distributed systems. Over the course of years Tribler’s performance has fallen below acceptable user experience levels, mainly because there is a lack of software performance engineering. In this work, we lay the foundations for a regression testing systems that allows developers to continuously monitor the metrics that are having the most impact on the performance of Tribler. Applying this system gave us a deep insight in the greatest bottleneck of the performance of Tribler: synchronous, blocking disk operations. Resolution of this bottleneck includes a major refactoring effort of the message synchronization system Dispersy. Implementation of a novel, non-blocking disk operation framework allowed us to increase the throughput of Tribler’s API by up to 150% and reduce its response times by up to 57.5%.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Software Technology","","Distributed Systems","",""
"uuid:89ec49e8-ad58-4688-8360-d3de7b25a36a","http://resolver.tudelft.nl/uuid:89ec49e8-ad58-4688-8360-d3de7b25a36a","Real-time transfer synchronisation for public transport services using passenger data","Gavriilidou, A.","Cats, O. (mentor); Hoogendoorn, S.P. (mentor)","2016","","public transport; holding control; transfer synchronisation; rule-based; passenger data comparison; on-board crowding","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport & Planning","","","",""
"uuid:1e3a106b-236d-4e9c-8153-dc707a0fdc59","http://resolver.tudelft.nl/uuid:1e3a106b-236d-4e9c-8153-dc707a0fdc59","Investigation into the effect of relaxed static stability on a business jet's preliminary design","Decloedt, D.","Voskuijl, M. (mentor)","2016","","static margin; relaxed static stability; business jet; the Initiator; weight estimation; inertia estimation; stability and control derivatives","en","master thesis","","","","","","","","","Aerospace Engineering","Flight Performance and Propulsion","","","",""
"uuid:19ad9853-fb74-430e-89e6-6e7f8b08ee5f","http://resolver.tudelft.nl/uuid:19ad9853-fb74-430e-89e6-6e7f8b08ee5f","Impact and Sensitivity of diverse parameters over the cost structure of Reverse Osmosis Systems","Rangel Escajadillo, V.","van der Meer, W.G.J. (mentor)","2016","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","","",""
"uuid:adef294a-56f3-4e17-a17e-61b7518a19b9","http://resolver.tudelft.nl/uuid:adef294a-56f3-4e17-a17e-61b7518a19b9","Residual Current Protection of a Meshed DC Distribution Grid with Multiple Grounding Points","Vandeventer, E.M.","Mackay, L.J. (mentor)","2016","Due to the emergence of renewable sources of energy and the progress in sustainable technologies, it could become very interesting in terms of energy savings and cost efficiency to switch the consumer level of the electrical network from AC to DC. This way, less power conversion would be required, and the meshing of the grid would be possible, hence increasing the reliability and flexibility of the power supply. Meshing the grid, however, is a challenge in itself. Indeed, it can be desirable to have several power sources connected to the same system, in order to provide for all the loads connected to the network when one of the sources fails. But having multiple sources in the network also means having several grounding points. Using the AC grounding methods, such as resistive grounding, proves problematic. As a matter of fact, as there are several grounding points in the network, they form a loop in the ground where the current can flow. If the voltage across the grounding points is not null, then a current will be able to flow through the ground. In AC, this was not such a problem, but DC ground currents will corrode the infrastructure around the network, which will prove harmful over time. It is thus necessary to devise a new way of grounding the system. The method proposed in this MSc thesis is capacitive grounding, which consists of using capacitors to ground the system, instead of resistors or inductors. This will ensure that no current flows through the ground in steady-state, and will consequently prevent corrosion. Grounding the grid through capacitors enables the use of the residual current measurement method to protect the meshed network against ground faults. Coupled to a smart communication system that divides the network in protection zones, this method will ensure the selectivity of the protection scheme, and will also discriminate net currents circulating through the grid from actual ground fault currents. The protective relays will be able to determine where the ground fault is on the poles with the polarity of the residual current measurement, and will only disconnect that pole, leaving the other half of the network in operation. This selective disconnection will also help improve the reliability of the system, and the consumers will be able to use the network safely.","bipolar; meshed; multiple grounding points; DC network; safety; ground fault","en","master thesis","","","","","","","","2017-09-01","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","DC systems, Energy conversion & Storage","",""
"uuid:e75a119a-bf25-418d-86a0-4bc6e337673f","http://resolver.tudelft.nl/uuid:e75a119a-bf25-418d-86a0-4bc6e337673f","Designing for Shared Decision Making","Thomson, K.","Beekman, Q. (mentor); Melles, M. (mentor); Brouwers, C. (mentor); Damman, O. (mentor)","2016","","Healthcare; Shared Decision Making; Patient-physician communication; Familial Hypercholesterolaemia; Application","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design for Interaction","","","",""
"uuid:5d7963b4-b3fa-40ed-b2f4-7fb1518b7ab8","http://resolver.tudelft.nl/uuid:5d7963b4-b3fa-40ed-b2f4-7fb1518b7ab8","Thermodynamic Model of a Screw Compressor","van Bommel, L.L.","Infante Ferreira, C.A. (mentor)","2016","A Compressor Resorption Heat Pump (CRHP) is a potential contribution to energy reduction in applications in which waste streams are upgraded, with limited energy addition, into high value process streams for reuse in industry. Previous research concluded that a CRHP with a wet screw compressor is a suitable option for many applications. An ammonia/water mixture was found to be the most appropriate fit, in terms of thermodynamic behaviour, for such an application. Objective of this thesis was to develop an integration of a geometry model and a thermodynamic model suitable for further optimisation of the wet twin-screw compressor. The integration of the geometry model and the thermodynamic model was carried out in modelling tool Matlab/Simulink, with inclusion of the physical properties of the working fluid. The development of the integrated dynamic model was carried out based on research for a heat pump process with a pre-selected geometry and a homogeneous two-phase fluid. The existing geometry model was transformed from shaft rotation based to time based equations to achieve the dynamic model requirements and the possibility of modelling the process in Simulink. The geometry model provides inputs to the thermodynamic model that dynamically describes the wet twin-screw compressor from the suction phase through compression to the discharge phase. The thermodynamic model requires inclusion of physical properties of the fluid and these were added by importing the physical properties through Refprop via Fluidprop. Mechanical constraints of a wet twin-screw compressor inevitably lead to internal leakage paths that reduce the compressor efficiency. The leakage paths have been included together with factors for friction, flow loss, etc. to represent the process in a more realistic way. The integrated model has been validated with the calculated result by model case A and measured results from the experimental set-up by Zaytsev [1]. A number of variations have been applied to the integrated model as examples of how to evaluate options for improvements. Making use of the developed integrated model parameters can be varied to show the influence on the compressor. The evaluations used a specific set of boundary conditions from previous research, using the geometry specified by Zaytsev [1]. The effects of three input parameters on the output and efficiency were evaluated: rotor length, discharge port area and vapour quality. The main result of the evaluation is that per boundary condition, the inputs from the geometry model have to be adjusted to achieve an optimal design of the twin-screw compressor. Further research to find the optimal design can be done with the help of the model that was developed for this thesis.","screw compressor; CRHP; Compression resorption heat pump; Thermodynamic model screw compressor; compressor heat pump; two phase compressor","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Process and Energy (P&E)","","","",""
"uuid:0281ea6d-aed3-4dc8-b6ce-c22a0e8fc2cf","http://resolver.tudelft.nl/uuid:0281ea6d-aed3-4dc8-b6ce-c22a0e8fc2cf","Traceability in cyber risk assessment: A design science approach","Syauta, K.J.","Janssen, M.F.W.H.A. (mentor); Pieters, W. (mentor); Hulstijn, J. (mentor)","2016","","traceability; risk assessment; cyber risk; design science","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering Systems and Services","","Management of Technology","",""
"uuid:57010309-6d13-4766-ab16-1902af5778fd","http://resolver.tudelft.nl/uuid:57010309-6d13-4766-ab16-1902af5778fd","Designing a seamless passenger journey: Picking up luggage from origin to destination and back","Marie, A.H.L.","Bergema, C.P.A.M. (mentor); Ottens, R. (mentor)","2016","Royal Dutch Airlines (KLM) and the Technical University Delft, faculty Industrial Design Engineering (IDE) are working together on the Personalised Airport System for Seamless Mobility and Experience (PASSME) project.. There are two problems derived from the PASSME project. First, airport processes are not seamless, because passengers spend relatively much unwanted time at the airport. PASSME’s objective is to reduce the time spent on handling luggage by 30 minutes. Secondly, the current state of luggage check-in for passengers is experienced as negative in 21% of the cases. It is experienced as heavy, a hassle and it creates stress. PASSME aims to improve the entire quality of experience of the door-to-door journey for at least 70% of the passengers. During the internal research at KLM, different problems were found at the luggage check-in. Passengers spend on average 6 minutes and 11 seconds at the SSDOP and 9 minutes and 50 seconds at the CUDOP. When researching the current situation for passengers several things can be concluded. A quantitative research is performed (n=86) to research the experience passengers have during their passenger journey with check-in luggage. From the research it can be deduced that passengers travelling by public transport, experience the journey less positively (4.7 out of 7), than passengers who use other types of transportation (car 5.9 out of 7, taxi 6.2 out of 7). These passengers need to travel with their check-in luggage by train, bus, metro and other public transports to reach the airport. And this travelling on public transport with their check-in luggage, causes a more negative experience and emotions, because carrying the luggage from bus to train or storing the luggage, is experienced as not very positive. After this research it became clear where improvement was possible for passenger and airline. The experience can be improved for passengers who travel by public transport. Unwanted airport time can be reduced at the luggage check-in and at the reclaim belt. The solution is a door-to-door luggage service. The luggage is picked-up by a logistic partner at the home address of the passenger. The luggage is transported by a logistic partner to the accommodation of the passenger. This means that the luggage is removed out of the passenger journey from home to airport. The door-to-door luggage service improves the passenger experience when travelling to the airport by public transport from a 4.7 to 6.3. Because of this service, the passenger does not have to handle the check-in luggage anymore from origin to airport. Since this service removes the check-in luggage from the airport, the passenger does not have to spend time at the luggage check-in area nor at the reclaim area. In total, the waiting time and processing time for passengers can be reduced from 29 minutes to 34 minutes, when the door-to-door service is used.","seamless flow; KLM; luggage; door-to-door; passenger journey; improve experience; reduce unwanted airport time","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Strategic Product Design","","","",""
"uuid:bf6904a1-2256-4299-9477-110a7be6c65f","http://resolver.tudelft.nl/uuid:bf6904a1-2256-4299-9477-110a7be6c65f","Developing scenarios for the European plastics industry based on the uncertainties of shale gas","Engels, C.J.W.","van de Poel, I.R. (mentor); Cuppen, E.H.W.J. (mentor); Pesch, U. (mentor)","2016","","shale gas; petrochemical industry; polyolefin; uncertainties; scenarios; decision making space; value chain-polymers","en","master thesis","","","","","","","","","Technology, Policy and Management","Values Technology and Innovation","","Management of Technology","",""
"uuid:df30b90f-95f0-4489-aaf7-77c5c543667f","http://resolver.tudelft.nl/uuid:df30b90f-95f0-4489-aaf7-77c5c543667f","decision support system for airline disruption management at KLM Cityhopper","Aslan, K.","Veeke, H.P.M. (mentor); Lodewijks, G. (mentor)","2016","Abstract: The operations of KLM Cityhopper (KLC) is a complex undertaking. Many safety, statutory and contractual rules and agreements need to be considered for managing its operations. Hence, KLC spends considerable time, effort and money on planning its resources carefully. Unfortunately, it is all too common that unforeseen events during the day of operation can make the carefully constructed timetable, fleet and crew schedules infeasible – hereby having an adverse effect on passenger itineraries. Thereto, KLC has an entity named Operations Control where human experts control the safety of operations and deal with complications that have a negative effect on the flight schedule. Currently, there is little to no insight in the decision-making process. Operators use different information systems to assess the necessary steps to mitigate a problem. However, it is difficult to evaluate if all options have been analyzed, and if all the required information is consulted. Furthermore, the required information may not even be readily available or accessible. To identify any potential shortcomings, the KLC airline disruption management process is analyzed using the Delft Systems Approach. It is found that no information is available on the possible solutions to a problem and the consequences thereof. Thereto, a decision support system that can help to solve the complex problem of reallocating aircraft is presented. A metaheuristic based on tabu search is implemented to explore a plethora of solutions that is then presented to the operator. The operator can choose from this pool of solutions and select the most desirable one given the situation at hand. Interviews with operators and computational tests show that the system is capable of presenting quality solutions in relatively short computational time.","Delft Systems Approach; decision support system; operations research; tabu search; airline disruption management; fleet recovery","en","master thesis","","","","","","","","2021-08-01","Mechanical, Maritime and Materials Engineering","Marine & Transport Technology","","","","52.30953, 4.81114"
"uuid:d123709a-a48e-4c07-a808-0ae01de0f458","http://resolver.tudelft.nl/uuid:d123709a-a48e-4c07-a808-0ae01de0f458","Seismic Inversion for Identification of Soil Stiffness and Damping for Offshore Wind Turbines","Armstrong, M.","Metrikine, A.V. (mentor); Verschuur, D.J. (mentor); van Dalen, K.N. (mentor); Versteijlen, W.G. (mentor)","2016","Non-invasive in-situ measurements can determine the soil properties in an undisturbed way, providing a more accurate characterization of the soil stiffness and damping. This report focuses on the multichannel analysis of surface waves (MASW) technique. The measured dispersion of a propagating seismic wave is used to characterize the variation of small-strain soil stiffness with depth while a modified halfbandwidth method is used to estimate the intrinsic soil damping variation with depth. Both require solving an inversion problem with a global optimization strategy. The equations governing wave propagation in layered visco-elastic media are used to form a analytic forward model. Sensitivity studies are performed and it is found that for a soil damping ratio below 10%, the migration of the real part of the wavenumber is minor. Therefore, a decoupled approach is used, where the soil stiffness is estimated first, followed by an estimation of the soil damping. The modal dispersion curves are shown to be very sensitive to the shear wave velocity of the soil, especially in the near surface layers. This is desirable as this sensitivity allows for a more accurate inversion. Additionally, there is a high sensitivity to the layer thickness of the model and it is shown that the discretization of the model, especially near the surface, has a large impact on the predicted modal dispersion curves. To estimate the stiffness profile, modal inversion is performed with a genetic algorithm. The misfit function is based on the determinant of the secular function, including a determinant normalization. Synthetic inversions were first applied to verify that the technique is successful. Surface wave data from the Westermeerwind offshore wind farm is analyzed and several estimates are made for the stiffness profile versus depth using different inversion settings. It is shown that these estimates agree well with a seismic cone penetration test (SCPT) estimate for the same site. An inversion is also performed on a set of measurements taken at the North-Sea Gjøa site. It is shown that the inversion is in good agreement with the published predictions for the stiffness at this site. The damping profile is inverted based on a modified half-power bandwidth method.A wavelet compression is used to select a smart set of root locations which leads to an efficient and balanced inversion. Synthetic inversions show that the method is successful in estimating the soil damping profile. The damping inversion technique is very promising and the next step should be taken to apply this to the measured data from the Gjøa and IJsselmeer sites.","soil stiffness identification; soil damping identification; surface wave analysis; offshore wind turbine","en","master thesis","","","","","","","","2021-08-20","Civil Engineering and Geosciences","Structural Engineering","","","",""
"uuid:fd9a3f9a-fb65-4a74-84e6-d0d51a245e0b","http://resolver.tudelft.nl/uuid:fd9a3f9a-fb65-4a74-84e6-d0d51a245e0b","Design of experiment to measure swab pressure and mitigation of swab pressure due to cone pop-out","Holtkamp, F.T.M.","Metrikine, A. (mentor)","2016","","","en","master thesis","","","","","","","","2021-08-29","Civil Engineering and Geosciences","Structural Engineering","","Offshore & Dredging Engineering","",""
"uuid:575d368c-7be1-4000-889f-af1f12fd2943","http://resolver.tudelft.nl/uuid:575d368c-7be1-4000-889f-af1f12fd2943","Ammonia Turbine Design for Ocean Thermal Energy Conversion systems","Dankerlui, P.R.","Pecnik, R. (mentor)","2016","Ocean Thermal Energy Conversion (OTEC) is a baseload renewable energy technology capable of converting the vast solar heat stored in the world’s oceans into useful electricity. Reducing the cost of electricity generation is the key for successful commercial breakthrough. This can be achieved by reducing plant capital cost and by improving overall efficiency. Unlike other components, improving turbine efficiency works both ways, since it directly improves overall efficiency while at the same time allows the other components to be made smaller. In this work, a methodology is developed for designing turbines for Ammonia OTEC systems. The methodology comprises the following steps: determination of the turbine architecture, generation of a design envelope, preliminary design and detailed 3D design with CFD analysis. The turbine architecture is determined for a theoretical OTEC plant having a power of 10 MW􁍞,􁍧􁍞􁍭, while the rest of the methodology is applied to the working conditions of a 500 kW􁍞,􁍧􁍞􁍭 plant currently under development. It was found that for 10 MW􁍞,􁍧􁍞􁍭, the operating conditions are more favorable for axial turbines. The preliminary design method applied a meanline design code, called zTurbo, in conjunction with Traupel’s loss model and a genetic algorithm to search the design space. Four turbine designs with total to static efficiencies ranging from 89.1 % to 91.2% resulted from the preliminary design method. It was noted that the large blade height to mean diameter ratio has a large influence on all the loss sources. Furthermore, it was found that all stators had converging meridional channels, indicating a limit on the attainable eulerian work. The performance and the validity of the preliminary designs was assessed by applying CFD analysis on a detailed design made from the simplest preliminary design. The efficiency differed by 0.45 %-points, which conforms very well to the capabilities of mean line design algorithms combined with classical loss models. The maximum differences in velocities, temperatures and pressures were found to be 4.63%, 1.88% and 0.78% respectively. A larger difference was found for the rotor kinetic energy loss coefficient. However, the CFD results indicated flow reversals at the hub, possibly causing the high value for the rotor loss coefficient. Finally, the pitch was investigated with CFD by decreasing the number of stator blades and subsequently, the number of rotor blades by 4 at a time. This resulted in a turbine with 32 stator blades and 26 rotor blades. The efficiency for the turbine rose from 88.63% to 89.69%, while the stator blades and rotor blades decreased by 10 and 8 respectively.","","en","master thesis","","","","","","","","2021-08-29","Mechanical, Maritime and Materials Engineering","Process & Energy","","","",""
"uuid:e683a140-2e13-4b67-9b28-76d7f8c4bd8e","http://resolver.tudelft.nl/uuid:e683a140-2e13-4b67-9b28-76d7f8c4bd8e","Applicability of Diffuse Reflectance Spectroscopy in accurate guidance of pedicle screws","Swamy, A.","Dankelman, J. (mentor)","2016","","","en","master thesis","","","","","","","","2021-08-29","Mechanical, Maritime and Materials Engineering","Biomechanical Engineering","","","",""
"uuid:224197d7-c410-4ea1-a179-15e5d5a68bc5","http://resolver.tudelft.nl/uuid:224197d7-c410-4ea1-a179-15e5d5a68bc5","Investigation of heat transfer between the steel strip and the heat pipe in the heat pipe assisted annealing concept","Devendran, K.","de Jong, W. (mentor); Celik, M. (mentor); Pronk, P. (mentor)","2016","","heat pipe; annealing; steel industry; web traction; steel strip; film lubrication; bvp4c","en","master thesis","","","","","","","","2021-08-29","Mechanical, Maritime and Materials Engineering","Process and Energy","","","",""
"uuid:65f5adb0-a881-4e30-a7cf-6f22cfa40d48","http://resolver.tudelft.nl/uuid:65f5adb0-a881-4e30-a7cf-6f22cfa40d48","Durability of Ultra Silent Pavements: A laboratory and computational study","Smits, N.C.","Scarpas, A. (mentor)","2016","Ultra-silent pavements (USP) are developed as surface layers with excellent noise reducing proper- ties, mainly due to their high air void content. However, the open structure of USP facilitates water intrusion and makes them vulnerable to moisture damage. Past experience has shown that USP mix- tures often exhibited ravelling, and poor adhesion with the underlying layer due to moisture. This study aims to quantify the effects of moisture on the strength of USP mixtures and on their adhesion to the base course. Three different mixtures were tested and their strength degradation due to moisture was evaluated through indirect tensile tests, whereas the interlaying bonding between the USP and the underlying layer was investigated using tensile adhesion tests. Both tests were performed at dry and wet conditions, on fresh and aged materials. The results showed that moisture does not necessarily decrease the strength of USP mixtures. However, the results revealed the adverse effect of moisture on interlayer bonding. Furthermore, moisture diffusion simulations were performed using the finite element method to in- vestigate how different values of the moisture diffusion coefficient of the binder affects the saturation of the mixture. For this, very fine meshes of each USP mixture were made using 2D images obtained by means of CT-scanner. The analysis revealed that mixture design influences the rate in which saturation takes place. Other properties of the USP like the dynamic stiffness modulus and the cohesive strengths are determined in the laboratory to be used for structural finite element analysis. Via a moving wheel load model, the tensile stresses at the bottom of the USP layer were compared to the adhesion strengths measured in the lab.","","en","master thesis","","","","","","","","2021-08-29","Civil Engineering and Geosciences","Structural Engineering","","Pavement Engineering","",""
"uuid:08fc576d-7df8-4e8f-b813-52325df5d559","http://resolver.tudelft.nl/uuid:08fc576d-7df8-4e8f-b813-52325df5d559","Load Side Detection of Series Arcs in DC Microgrids by Simulation and Experimental Validation","Liu, Z.","Ramirez Elizondo, L.M. (mentor)","2016","With increasing scale of usage for renewable energy resource, DC micro-grids system becomes more and more popular for the application of different kinds of power electronics in DC. However DC arc fault introduces major safety concerns due to the randomness and instability, especially absence of zero current crossing, which makes it is hard to be detected and eliminated. In this thesis, a novel proposed series dc arc detection methods were validated both in theoretical simulation by MATLAB and in real-time experiments by creating real arcing in the constructed equivalent DC system. By changing the parameters and type of both the circuit and the setting of detection method, the arc detection performs with sensitivity, selectivity, speed and reliability harvested. Additionally, the research and practical tests have also been conducted to study the characteristics of series dc arc, which contributes to the determination of the appropriate setting value for the arc detection algorithm.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy (ESE)","","","",""
"uuid:c7db7b84-7cb4-458e-b804-500f2624d9cd","http://resolver.tudelft.nl/uuid:c7db7b84-7cb4-458e-b804-500f2624d9cd","Design of a 3D printed steerable bipolar electrosurgical tool","Hovland, K.","Sakes, A. (mentor); Breedveld, P. (mentor)","2016","Current electrosurgical instruments often lack the capability to be optimally customized to a certain procedure/patient. It simply wouldn’t be feasible to produce ten or hundred different configurations in terms of dimensions and geometry of the same device. Most instruments come in one configuration. This study aims to increase the flexibility of these instruments by introducing Additive Manufacturing techniques to the design process. AM has nearly infinite possibilities and can make it easier and more cost efficient to build unique devices. Different dimensions can be implemented in the computer model and transferred to the printer with ease. By combining such a device with electrical components and and two degrees of freedom in the tip, creates a unique device which breaks into new territory. The mechanical part of the prototype in this project functions as desired. Unfortunately, the electrical system could not be sufficiently tested, due to the safety issues involved. However, it does seem feasible to manufacture an electrosurgical device with steering capabilities through Additive Manufacturing.","Additive Manufacturing; Electrosurgery; Bipolar; Steerable","en","master thesis","","","","","","","","2020-08-29","Mechanical, Maritime and Materials Engineering","Biomechanical Engineering","","","",""
"uuid:72a6c8d6-7e1a-4019-9152-18985cdae13b","http://resolver.tudelft.nl/uuid:72a6c8d6-7e1a-4019-9152-18985cdae13b","Develop a LES-based air quality model by nesting DALES in LOTOS-EUROS","Sun, J.","Vlemmix, T. (mentor); de Roode, S.R. (mentor)","2016","Nitrogen dioxide (NO2) is one of the nitrogen oxides (NOx) pollutants. Not only the NO2 itself is toxic to human health, but also the precursors of a number of hazardous secondary air pollutants, such as nitric acid, tropospheric ozone and nitrate component of particulate matters. Besides, NO2 is also an essential substances involving in ozone destruction in the stratosphere. The main source of NO2 over urban is combustion processes from traffic. Jeopardized by the severe situation, the monitoring and observation to this harmful trace gas is important. For urban regions, the in-situ and remote sensing techniques are combined. However, these measurements can be problematic due to the meteorological conditions or atmospheric processes, such as clouds. Besides, the retrieval of the measurements provides limited information on concentration fields under various a-priori assumptions. Alternatively, the atmospheric dispersion modeling is in use to study the air quality, which provides a more complete deterministic description of pollutants dispersion problem. Currently, the dominating atmospheric dispersion models are based on the parameterization. These models are efficient to simulate meso-scale or macro-scale atmospheric dispersion, with spatial resolution of magnitude of kilometers. Considering on urban scale, however, this resolution is too coarse to resolve the air pollutants, where the emission sources are close to receptors. Instead, a more effective technique is large eddy simulation (LES). It applies a low-pass filter that effective removes small-scale turbulences from numerical calculation. By nesting DALES (Dutch Atmospheric Large Eddy Simulation) into LOTOS-EUROS (LOng Term Ozone Simulation-EURopean Operational Smog model), an air quality module is developed to evaluate the LES-based air quality model by comparing with LOTOS-EUROS. The conclusion of this thesis consists of two parts. The first one is the sensitivity study, where the properties of DALES original chemical module are explored. These properties includes the sensitivity of NO2 concentration to background ozone level, reaction rate coefficient, clouds perturbation and turbulent control. In the second part, simulation over Rotterdam is operated on a relative coarse resolution, as the consequence of the limitation of restore space and process capability. The slab averaged profiles are not significantly different because of the strongly constrained concentration boundary condition by LOTOS-EUROS. Although attempt to study the difference of concentration field due to the dynamics scheme is not achieved, DALES still has much higher resolution compared with LOTOS-EUROS. Hence, the spatial variability in DALES is more detailed. Conclusively, the DALES air quality module performs consistently with LOTOS-EUROS. The improvement in terms of chemical mechanism, the emission inventory, the capability of processing. etc. will complete this module in future.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience and Remote Sensing","","","",""
"uuid:90ca6667-9d40-4d5a-b28d-2acd6a17ec2d","http://resolver.tudelft.nl/uuid:90ca6667-9d40-4d5a-b28d-2acd6a17ec2d","Increasing customer loyalty with the My Vodafone app","Odenhoven, M.P.","Hultink, H.J. (mentor); Creusen, M.E.H. (mentor)","2016","A design intervention to stimulate loyalty is aimed at the MyVodafone App, a digital self-service tool primarily aimed at cost control through credit- and billing information. Although repeat usage of the app is suboptimal, the app fits with the vision of the brand to empower people and make them in control of their service. As a centerpiece to the CARE customer care program, the app is becoming a strategic asset for customer interaction and is becoming increasingly personal, with a vision to create a fully personalized experience. Vodafone has formulated the ambition to Reward Loyalty, although the initiatives are highly transactional and not personal, as is the vision of Digital, this transactional nature is found throughout the Dutch telecom market. The design challenge formulated is to recognize, cultivate and harvest loyalty in a personal yet scalable way. Loyalty is found to be dependent on customer-, environmental- and dyadicrelationship characterics along with perceptions of the firm. It is dynamic and built up over time throughout critical moments in the lifecyle, which implies different degrees of loyalty. True loyalty, and it’s economic benefits, is only unlocked if a fair relationship is built up for which multiple tactics exist. The economic benefits of true loyalty do not come automatically, but rather must be harvested actively. The type of relationship Vodafone customers have today is characterized by a lack of recognition, not thinking along, a steep change in interaction density before retention and a lack of benevolence trust. What customers desire is Vodafone to think in their best interest based on the insights they have, and empathisize with the customer by sharing knowledge. The design direction boiled down from these key insights is formulated as pro-actively guiding customers in an ongoing, recognized interaction throughout the cycle,to build an honest relation and generate true loyalty. Requirements and wishes were formulated that translate this vision into design specifications, surrounding the themes of Performance, Recognition, Communication, Equity and Operational constaints. Business stakeholders’ wishes regarding the renewal journey were integrated too. Three personas are drafted with different levels of behavioral and attitudinal loyalty by variation of antecedents of loyalty in the profiles. Mechanisms on how to stimulate benevolence trust are presented and integrated into the creative process. The outcomes of the creative process are ideas, that are presented along the themes of ‘insights data & offers’, ‘omni-channel & real-life integration’, ‘feedback’, ‘content, updates & expertise’ and ‘personal assistant’. The most promising ideas were selected and integrated into two concepts, MyVodafone Buddy and Vodafone Rode Draad. Buddy is a digital assistant that thinks along, and helps customers to consciously get the most out of their phone. Rode draad is an interactive timeline giving insight and control in the past, the present and the future at Vodafone. Three rounds of iteration were made, in the first round the Rode Draad was selected based on its’ desirability, feasibility and viability. In the second round content is generated to elaborate the concept, and in the third round the elaborated concept was evaluated by customers. The results of the third round are very positive, a large majority sees the added value of the concept, thinks it improves their relation with Vodafone in terms of transparency, honesty and trust, and indicates that the concept has a positive contribution to their loyalty to Vodafone. De Rode Draad timeline is a highly flexible paradigm, and is detailled to include service and commercial messages that improve the perceived recognition and honesty. (For example; interaction loggings, surveys,contact moments by Vodafone, guidance at key moments, special throwbacks on anniversaries, expiration dates, etc.). From the interactions with customers, it introduces surveys that are tracked personally to enable NPS tracking/follow-up & advanced loyalty profiling. Aesthetics and interactions are defined to build on the Notification Center in Starship, to accomodate both user cognition and technical feasbility, and additional suggestions for an omni-channel interface are made. A customer journey & service blueprint put the Rode Draad in perspective, and show that it has the potential to not only improve personalisation through a relational approach, but is also an asset in service delivery. The concept is designed and approved to be compliant with the relevant Dutch laws (Cookie, privacy, spam). The input of the service blueprint is transformed into a feasibility study and roadmap that maximize organisational commitment and minimize risk, by building a scalable capability on top of the existing notification center, letting evidence accumulate and then gradually expanding the concept. A ‘gate-keeper’ is recommended to guide this process, with a consolidated team of multi-disciplinary designers. Lastly a business case is presented that displays a 4,6 million upside potential (NPV) in light of a 1,3 million investment, based on modest assumptions driven by company data and the websurvey evaluating the Rode Draad.","Loyalty; Telecom","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","","",""
"uuid:7ecb77e6-5097-4d80-a7f9-5fcd8bb52e18","http://resolver.tudelft.nl/uuid:7ecb77e6-5097-4d80-a7f9-5fcd8bb52e18","Activity choice behaviour of transferring passengers at railway stations","Wijgergangs, K.","Hoogendoorn, S.P. (mentor); Daamen, W. (mentor); Annema, J.A. (mentor); Ton, D. (mentor); Lamberts, M.F.M. (mentor)","2016","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport & Planning","","","",""
"uuid:8681695d-7c4d-4c3c-b4c8-ea027fcb229c","http://resolver.tudelft.nl/uuid:8681695d-7c4d-4c3c-b4c8-ea027fcb229c","Gas condensate allocation based on an equation of state model","van Breukelen, I.","Jansen, J.D. (mentor)","2016","The current allocation method for gas condensates in the offshore gas fields (of Shell and partners) of the Southern North sea is unreliable. The allocated values of the gas condensates are physically not possible. The current allocation method shows condensate gas ratios that are extremely varying (and sometimes increasing) over time, while it is known that gas condensate ratios either stay constant or decrease over time (decreasing reservoir pressure). The method that is currently used for condensate allocation is based on unreliable samples and measurements. The method provides a relative allocation by using reconciliation factors to compensate for the difference of onshore and offshore measurements. This thesis explores alternative condensate allocation methods, and proposes that using the thermodynamic behaviour of fluids will increase the accuracy of gas condensate allocation. The properties of the fluids differ per field and can be described by the equation of state of Peng-Robinson. The properties of the fields come from PVT reports. The PVT reports are based on experiments on samples that were taken before a field started producing. The thermodynamic models of the reservoir fluids were made with a software package called PVTSIM that is based on the equation of state. The fluid models were validated with measured data at an offshore slugcatcher. The measured data had some uncertainty factors, but did show a good match with the modelled results. The new model is used to calculate the total amount of produced condensate. This calculated total amount is compared with the actual measured produced amount of condensate. The difference between both volumes is less than 2\%. The currently allocated condensate is also compared with the calculated condensate, which shows big differences. The new method based on the equation of state of the reservoir fluids yields accurate results with physically expected trends.","gas condensate; allocation; Southern North Sea; equation of state; Peng Robinson; PVTSIM; hydrocarbons","en","master thesis","","","","","","","","2026-08-26","Civil Engineering and Geosciences","Geoscience & Engineering","","","",""
"uuid:e145de3e-b23a-44a3-b819-effad4617c8b","http://resolver.tudelft.nl/uuid:e145de3e-b23a-44a3-b819-effad4617c8b","Visualising the status of building elements in construction projects: An approach to using BIM4D techniques to visualise infield status of building elements","van Breukelen, T.","van Nederveen, G.A. (mentor); Verhoef, R. (mentor); Bakker, H.L.M. (mentor)","2016","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Construction Management and Engineering","",""
"uuid:79df112b-f765-44f4-8bea-015ccbc077c6","http://resolver.tudelft.nl/uuid:79df112b-f765-44f4-8bea-015ccbc077c6","Representative Models for History Matching and Robust Optimization","Yap, F.K.","Jansen, J.D. (mentor); De Barros, E.G.D. (mentor)","2016","","Speedup; Representative Models; Representative Ensembles; Robust Optimization; History Matching; Acceleration","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience and Engineering","","","",""
"uuid:4871c75f-268d-4bc2-89d0-8c80029bd1c2","http://resolver.tudelft.nl/uuid:4871c75f-268d-4bc2-89d0-8c80029bd1c2","Updated rock mechanical design criteria for the Liikavaara Östra open pit","Csicsek, A.","Rinne, M. (mentor); Sjöberg, J. (mentor)","2016","The assessment of rock mechanical data is a fundamental step in the process of evaluating the profitability of an open pit mine. The steepness of bench, interramp and resulting overall slope angles have a direct effect on the stripping ratio, therefore on the profitability of the mining operation. With the recently challenging metal prices in mind, the maximization of slope angles is even more important nowadays, prompting detailed investigations in the topic. The primary goal of this master thesis is to provide updated rock mechanical design criteria for the Liikavaara Östra open pit in Northern Sweden, by collecting and assessing the already available information from the area, as well as evaluating the raw data of the drilling campaign undertaken in 2016. After the compilation of the rock mechanical and geological data of the site, the bench scale slope stability was assessed with probabilistic and deterministic approach. During the process, two failure criteria (Mohr-Coulomb and Barton-Bandis) and two groundwater conditions (drained and undrained) were tested. Based on the gained experience in the neighbouring Aitik mine, the drained Barton-Bandis scenario was used both in probabilistic and deterministic approach to recommend bench face angles. Based on the findings of the bench slope analysis, an increase of the bench and interramp angles (compared to the previous design study) is possible, with the presumption of improved smooth blasting techniques and minimized back break. In identified areas with excessively poor rock quality, the application of external rock support was assessed in order to maintain reasonable bench angles and avoid mining of additional waste rock. The application of external slope support practices deemed feasible in the footwall domains of the pit, where with the utilization of support methods, the possible relocation of the E10 road can be avoided. The overall slope stability was also assessed in the footwall with limit equilibrium analysis methods. The footwall of the pit was found to be stable with the new design criteria, although it is sensitive to the presence of groundwater pressures. The master thesis was conducted as part of the European Mining, Minerals and Environmental Program (EMMEP), European Mining Course (EMC) track. The EMC is a triple-degree master program at TU Delft, RWTH Aachen and Aalto University.","slope stability; kinematic analysis; probabilistic analysis; deterministic analysis; deterministic analysis; Liikavaara Östra","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","EMC is a triple-degree master program at TU Delft, RWTH Aachen and Aalto University","",""
"uuid:131b6977-bcca-4862-9a63-b80df5070f43","http://resolver.tudelft.nl/uuid:131b6977-bcca-4862-9a63-b80df5070f43","Investigation of Trailing Edge Sub-Components in Wind Turbine Blades","Lachance-Barrett, S.","Teixeira de Freitas, S. (mentor)","2016","","","en","master thesis","","","","","","","","2017-08-26","Aerospace Engineering","Aerospace Structures & Materials","","","",""
"uuid:9d609886-372d-466d-9cca-7e7b26b26a7f","http://resolver.tudelft.nl/uuid:9d609886-372d-466d-9cca-7e7b26b26a7f","Impact of Satellite Fragmentations in GEO Graveyard Orbits","Roelen, L.","Noomen, R. (mentor)","2016","","","en","master thesis","","","","","","","","2017-08-26","Aerospace Engineering","Space Engineering","","","",""
"uuid:3c509ad4-3f26-46c9-87df-1e76459ea600","http://resolver.tudelft.nl/uuid:3c509ad4-3f26-46c9-87df-1e76459ea600","Extending the SSVI model with arbitrage-free conditions","Hendriks, S.","Oosterlee, C.W. (mentor); Martini, C. (mentor)","2016","","","en","master thesis","","","","","","","","2017-08-26","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","Numerical Analysis","",""
"uuid:4f62f38a-63bc-4dfd-80b9-d857ab4cde79","http://resolver.tudelft.nl/uuid:4f62f38a-63bc-4dfd-80b9-d857ab4cde79","Analysis of the refinery project execution strategy from an organizational effectiveness perspective: Studying the impacts of concurrent execution on taskforce satisfaction in the EPC phase of a refinery project","Patil, H.V.","Bosch-Rekveldt, M.G.C. (mentor); Stikkelman, R.M. (mentor); Bakker, H.L.M. (mentor)","2016","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Construction Management and Engineering","",""
"uuid:790d4a5c-514f-4758-beb3-c615bf2b5d20","http://resolver.tudelft.nl/uuid:790d4a5c-514f-4758-beb3-c615bf2b5d20","The development of the new Skil® Multi-Tasker","Rijksen, M.","Buijs, J.A. (mentor); Flipsen, S.F.J. (mentor)","2016","This thesis describes the graduation process and result for the faculty of Industrial Design Engineering (IDE) from the Delft University of Technology. This graduation project accomplishes the masters degree Integrated Product Design (IPD). The project starts with an analysis of the current tool and its context. This phase of the process is meant to get to know the subject, gather insights and to get used to the jargon. After the analysis a research is executed. This research contains two parts, a theoretical part and a field research. The theoretical research is to investigate the context of the tool further. The field research is done to find user insights, experiences and demands or needs. These findings provide stepping stones for the development of a new power tool. To be sure the research is used properly in the design the data from the research is structured and analyzed. After this analysis the main findings are used in the ideation phase to develop new product ideas. Potential ideas are made to process the user insights in the new tool. High potential ideas are elaborated and finally compared to each other. When this is done, a final product idea is realized. The realization ends up with a prototype. This prototype is used for product validation and recommendations The final result is a prototype and its validation. These two parts of the result create a stepping stone for further steps in the product development process for Skil. The prototype is a fully functional product and can be used for tests. The validation is done using the prototype and provides user experience about this usage and functionality of this prototype.","oscillating tool; power tools; multi-Tool; multi-Tasker; multitool; skil","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design Engineering","","","",""
"uuid:30ae5e11-a713-48f1-b68c-fc57d6486e02","http://resolver.tudelft.nl/uuid:30ae5e11-a713-48f1-b68c-fc57d6486e02","Vision Assisted Motion Planning of Robotic Arm For Service Robots","Vatsyayan, A.","Babuska, R. (mentor)","2016","In this thesis a vision assisted system is developed for manipulation of a robotic arm which is to be used in unconstrained environments with service robots. The vision module comprises of segmentation and object tracking that allows the user to select the object they want to grasp. It is shown that GrabCut segmentation improves the efficiency of the Tracking-Learning-Detection (TLD) tracker. The Moveit! platform for solving motion planning problems is used in this thesis. Apart from the default Open Motion Planning Library (OMPL) in Moveit!, Stochastic Trajectory Optimization for Motion Planing (STOMP) and Search-Based Planning Library (SBPL) have also been explored to solve motion planning problems. Inverse kinematics based genetic search is used for generation of waypoints in challenging manipulation tasks and has been incorporated into the Moveit! framework. The waypoints generated through genetic search have shown to be valid. Additionally an analysis is done to evaluate the performance of different motion planning libraries to find implementable solutions in cases of varying relative position to arm and clearances to nearby obstacles. It is shown that certain motion planning libraries have superior performance for varying clearance and position of goal. A contextual awareness module is developed that determines the best planning algorithm for the clearance from obstacles and relative position of the target pose to the arm. A flexible framework is created that incorporates the vision module, genetic search, contextual awareness and allows for switching between the three motion planning libraries. The system is also tested on the robotic arm at Robot Care Systems.","","en","master thesis","","","","","","","","2019-08-26","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control","","","",""
"uuid:066d0e98-6c5e-4864-9cbf-8681c622eff6","http://resolver.tudelft.nl/uuid:066d0e98-6c5e-4864-9cbf-8681c622eff6","Redesign of a solar home system: An expandable and understandable solar home system for rural Cambodian families","Siebinga, R.","Diehl, J.C. (mentor)","2016","Kamworks is a social enterprise with a vision to improve access to electricity in Cambodia’s rural areas by selling solar products to the local population. Their main product, a solar home system, fulfils a household’s basic electricity needs. Creating an improved generation of solar home systems can contribute to further development of the county and its inhabitants. Product and service analyses together with previous experiences of Kamworks’ managing team defined six problems surrounding the current generation of solar home systems. Prioritizing these problems determined two project tracks: to create a modular system of which the power capacity can be increased, and to create a new interface that solves misunderstanding and misuse problems. Both tracks were solved separately but simultaneously, with a focus on people who are directly and often involved with the product. Hotline data was analysed to determine misuse and misunderstanding problems. Current users and Kamworks’ staff were tested to evaluate their knowledge level of basic electricity consumption and understanding of the current interface. Results showed participants had a low knowledge level and based their estimations on personal experiences rather than calculations. Most understood the current interface but could not estimate battery levels correctly. Idea generation based on a list of requirements led to three interface concepts which were tested at the homes of six current system owners. An interface showing five levels of battery percentage and three levels of power consumption was preferred by most participants. An evaluation of requirements rated the same concept the highest. A modular solar home system’s customer journey was presented to five participants to check their interest in a modular system. Results from a literature study on design for upgradability described specifications for a modular product. Technical limitations and possibilities described basic electronics requirements. Idea generation based on a drafted list of requirements led to multiple concepts for a new battery box, mounting system and physical display. All were evaluated by a focus group of Kamworks SHS team and a requirements evaluation. A separate display, H-shaped mounting system and yellow polyester-fibreglass battery box were selected to be further developed. Results from both concept evaluations were implemented. The final comprehensive modular solar home system was presented, together with description of its costs, dimensions, production, assembly, installation, and user manual. Real-sized prototypes of the display and battery box were created to evaluate the final design. An assembly test with two of Kamworks’ employees resulted in an optimised assembly process and changes to power sockets’ placements. A user test with nine participants evaluated the appearance, three standard actions, and placement of the battery box and display. Results showed the appearances of the prototypes are preferred over the current generation. Participants were able to charge a phone, plug in an appliance, and push the circuit breaker. They would place two battery boxes side by side and place the display on a wall above. The final design is an expandable and understandable system, and an answer to both problem tracks. Other benefits are a lower cost price, a compact shape, a simplified installation, and an improved appearance. However the design is not ready to be implemented yet. A number of fully functioning prototypes should be extensively (user) tested to ensure the design and electronics function as intended.","solar energy; solar home system; Cambodia; product design; base of the pyramid","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design Engineering","","Integrated product design","",""
"uuid:792a7523-81f8-40fa-98c3-ec90c01663cd","http://resolver.tudelft.nl/uuid:792a7523-81f8-40fa-98c3-ec90c01663cd","Pull test to validate photogrammetrically predicted friction angle of rock dicontinuities","Dzugala, M.","Rinne, M. (mentor)","2016","The accurate estimation of the mechanical properties of a rock joints is crucial in terms of safety when it comes to design of slopes in open pit mines or caverns used for the storage of hazardous materials, for instance – nuclear waste. Photogrammetry provides a simple and objective method of joints roughness assessment, without the need for expensive and time consuming laboratory tests or imprecise empirical methods. In this thesis photogrammetric method developed in KARMO II project was used to estimate the roughness, shear strength and friction angle of a discontinuity of big scale sample. That estimation was done by analyzing the profiles of digital models of joint surface. Surface length and slope measurement methods were used to calculate the values of joint roughness coefficient of analyzed surfaces. Alternatively, JRC was also estimated empirically, with hand profiling method, and the same calculations were done for JRC values obtained this way. Next, Barton – Bandis criterion was used to calculate the shear strength and friction angle of the joint. Additionally, the shear strength and friction angle of the rock discontinuity were obtained experimentally with multistage pull testing. The results obtained with all methods were analyzed and compared. JRC values from photogrammetrically created digital models of the joint surface were overestimated due to the models not being dense enough which resulted in high noise to signal ratio. High JRC values combined with low normal load used during the pull test constrained the applicability of Barton – Bandis criterion and the linear interpolation was used to determine the shear strength and friction angle of tested rock surfaces. Values of shear strength obtained with photogrammetrically created models were overestimates in relation to the results of the pull test by approximately 45%. The errors made during this research are analyzed in the thesis and recommendations on how to improve reliability of the results are made. Main error in photogrammetric prediction was too low density of the point clouds and in laboratory test too low stiffness of the test arrangement. The alternative methodology for photogrammetric studies used in previous stage of the KARMO I project was tested in this thesis and was proven to give significantly higher accuracy of generated digital models. The stiffness of the testing machine and proper positioning of the sample halves on top of each other were identified as the most sensitive aspects of methodology of big scale pull test when it comes to reliability of the results.","pull test; friction angle; rock joints; shear strength; JRC prediction; photogrammetry","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","European Mining, Minerals and Environmental Program (EMMEP)","",""
"uuid:5b213614-b723-49f7-a557-9c02b06c19ef","http://resolver.tudelft.nl/uuid:5b213614-b723-49f7-a557-9c02b06c19ef","Reliability comparison of multilevel converters for wind turbine systems","Lyrakis, E.","Polinder, H. (mentor)","2016","The extension of the reliability and hence the lifetime of the power converters used in wind turbine systems gains growing interest given the potential to drive the costs further down. This study conducts a detailed comparison among several suggested multilevel converter topologies so as to investigate which converters achieve a more even distribution of the load among their electronic components and consequently achieve an extended lifetime expectancy. Additionally it analyzes a lot of factors that play a deteriorating or favorable role in the fatigue of a multilevel power converter. The relevant research is being conducted through simulations conducted with the help of Matlab and Simulink. The course of this thesis consists of several processing steps: (a) the mechanical modelling of the wind turbine (b) the electrical modelling of the generator and the converter (c) the detailed analysis of the switching methods (d) the power loss analysis of the individual components of each topology according to operation (e) the thermal modelling of these components and (f) the lifetime estimation. The concept of the model relies on the analytical approximation of the dynamic response of all the aforementioned parts to random generated wind profiles. By utilizing the above tools a gradual transition is achieved from the initial wind speed input to the electrical parameters of the generator and the converters, to the power losses of the power semiconductors of each topology, to the temperature profile of the devices and finally to the estimated consumed lifetime. At the end, provided the fatigue estimations but also with the help of the data collected from the intermediate steps an integrated comparison is performed regarding the reliability of the examined converter types.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy (ESE)","","","",""
"uuid:0c5444bb-8ca8-498f-807e-81e3286bc914","http://resolver.tudelft.nl/uuid:0c5444bb-8ca8-498f-807e-81e3286bc914","Numerical thermal back-calculation of the Kerava Solar Village underground thermal energy storage","Oosterbaan, H.","Rinne, M. (mentor); Janiszewski, M. (mentor)","2016","With increasing pressure to reduce the fraction of energy coming from fossil fuels, there is an increased need for research into feasible, and sustainable energy sources, such as solar energy. The problem with solar energy is the mismatch between supply and demand, and so the energy needs to be stored. This thesis is a part of the project titled “Tackling the Challenges of a Solar-Community Concept in High Latitudes”, and aims in helping to design a thermal energy storage system for southern Finland that is economically feasible and has a high performance. For this purpose, a back-calculation of the underground thermal energy storage (UTES) of the Kerava Solar Village was performed. The main objective was to calibrate the numerical models to be used in an optimization by quantifying the thermal properties of the surrounding granite and soil. The UTES of the Kerava Solar Village consisted of a rock pit filled with water and two surrounding rings of boreholes. From the 1st of June until the 31st of August 1984, the rock pit was continuously charged, and the energy flows from the boreholes were negligible. COMSOL Multiphysics 5.2® was used to create a model in which the temperature of the rock pit was used as the heat source and the heat propagation through the surrounding rock as the output to which the historical data was compared. The best replication of the historical temperature inside the rock near the surface was achieved with a conductivity of 2.8 and 1.0 W/(m·K) for granite and soil respectively. When looking at the deeper sections, the best fit was obtained for a conductivity of 5.5 and 1.0 W/(m·K) for granite and soil respectively. These results are conflicting, and the realistic range of the conductivity for granite is 3-4 W/(m·K), and so the thermal conductivity could not be estimated with confidence. The main problem throughout the back-calculation was the lack of data. To perform a successful back-calculation, all the parameters of the system need to be known, such as the geology, hydrology, and detailed technical drawings, but also the temperature distribution inside the heat source, and heat storage medium.","back-calculation; Kerava Solar Village; Underground Thermal Energy Storage","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","","","European Mining, Minerals and Environmental Program (EMMEP)","",""
"uuid:86dd172a-ef16-4df9-bee3-9c2026f4554a","http://resolver.tudelft.nl/uuid:86dd172a-ef16-4df9-bee3-9c2026f4554a","Full Color High Definition Fused Filament Fabrication","Botterman, B.","Babuska, R. (mentor)","2016","Although Full Color (FC) High Definition (HD) Additive Manufacturing (AM) machines are available on the market today, a FC HD Fused Filament Fabrication (FFF) printer has yet to enter the competitive FFF market. Leapfrog, a FFF printer manufacturer, holds a patent that allows the application of coating on 3D printed FFF filament after it has been deposited. In this thesis work a prototype has been realized that serves as proof of the FC HD FFF concept and forms the basis of the commercial product which will be launched in the near future. Using inkjet technology high development costs can be avoided if existing coating devices are used in the machine. The challenges are to achieve a maximum outer surface coating of the 3D printed object and minimize the time it takes to coat a 3D printed object. This time is increased dramatically by applying a coating layer after every 3D printed layer compared to no coating. Using the prototype machine several coating methods have been explored of which the most promising method is perpendicular coating. Here, after 3D printing a layer, droplets are jetted onto the outward facing surface of the object. The total coating time is minimized by grouping contours which can be coated in one swath and sorting the order in which groups are coated. The grouping is done by formulating and solving a bin packing problem. Mixed Integer Linear Programming (MILP) and First Fit Decreasing (FFD) are compared and used to solve the problem, achieving more than 80% time reduction compared to no grouping on test models. A minimal movement time between all the groups is found by solving a Traveling Salesman Problem (TSP). The state of the art solver Concorde (CC) is compared to a Nearest Neighbor (NN) heuristic. A movement time reduction of 55% up to 85% is achieved compared to the standard 3D printing order on the test models. CC gives a maximum improvement of 2.3% over NN. The perpendicular coating method will be used to coat the filament. A significant total print time reduction is possible by grouping. MILP can have a hard time finding a solution for certain data sets, therefore FFD is preferred. For movement time reduction, a 2.3% performance increase by CC generally means that printing times are reduced by seconds. This is negligible considering 3D printing a layer generally takes minutes making NN a suitable heuristic for solving the TSP.","Traveling Salesman Problem; Nearest Neighbor; Variable Size Bin Packing Problem; First Fit Decreasing; Mixed Integer Linear Programming; 3D printing; Additive Manufacturing","en","master thesis","","","","","","","","2017-08-26","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control (DCSC)","","","",""
"uuid:5c67462f-d52e-4b61-adfd-b7d949ab5104","http://resolver.tudelft.nl/uuid:5c67462f-d52e-4b61-adfd-b7d949ab5104","The Effect of Shielding Gas Composition on Weld Bead Geometry during Short-circuit GMA Welding of Inconel625 Alloy","Zhao, Y.","van der Zwaag, S. (mentor)","2016","Recently, X65 carbon steel pipes internally clad with a thin Inconel625 alloy lining are highly used. However, during welding the Inconel625 lining in the root pass, the weld bead is uneven. To avoid weld defects in the successive passes, an additional grinding procedure is always required to flatten the weld bead. It is known that as an indispensable component in GMAW, the shielding gas can protect the liquid metal from oxidation and defects in the atmosphere. The shielding gas composition greatly determines the characteristics of the welding arc which plays a significant role in the heat, mass transfer during welding and flow motion in the weld pool. It is proposed that the uneven weld bead can possibly be improved by changing the shielding gas composition during welding the Inconel625 lining. Therefore, it is necessary to study the effect of shielding gas composition on the weld bead geometry during short-circuit gas metal arc welding (GMAW) of Inconel625 alloy. In this research, short-circuit GMAW of the Inconel625 lining was reproduced in a laboratory scale. Three kinds of gas, argon (Ar), helium (He) and carbon dioxide (CO2) were considered and two types of experimental configurations, bead-on-plate welding and U-shaped groove butt welding were performed. It was found that the shielding gas composition could make a great influence on the arc voltage, power of the heat source (welding arc), flow motion in the weld pool and the resultant geometry of the weld bead. For the Ar-He shielding gas mixtures, the arc voltage is increased as the He content is increased, which is attributed that the ionisation potential of He is higher than that of Ar. As a consequence, with an increase of the He content, the heat source (the welding arc) becomes more powerful and more heat is transferred to the workpiece. A larger fusion area is created. For the Ar-CO2 shielding gas mixtures, it was found that adding 3.3% CO2 to Ar could raise the arc voltage and lead to a more powerful welding arc. More importantly, the presence of oxygen which is dissociated from CO2 can largely reduce the surface tension of the liquid metal. The temperature gradient of the surface tension can be changed from negative to positive and the direction of the Marangoni convection which is driven by the surface tension force will be changed from outward and upward to inward and downward. The efficiency of the heat transfer from the surface of the weld pool to the bottom is greatly enhanced. Consequently, addition of 3.3% CO2 to Ar can deepen the penetration and enlarge the fusion area.","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures & Materials","","","",""
"uuid:d9fe2346-3dcf-40ba-8c91-e9be6e98b08b","http://resolver.tudelft.nl/uuid:d9fe2346-3dcf-40ba-8c91-e9be6e98b08b","User insight library: the development of an online platform where clients of Muzus can browse through rich user insights","","Mooij, S.C. (mentor); Van der Vorst, R.R.R. (mentor)","2016","This thesis focuses on the development of an online user insight database for Muzus. Muzus is a design company that is specialized in user-centered design research and concept development. The insights that Muzus collects can be implemented on a broader scale. Therefore wants to develop an online platform in which these results can be clustered based on life events and can thereby easily obtained by different clients.","","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","","",""
"uuid:fd9b21ff-439f-4551-96e3-7d3bd2560e94","http://resolver.tudelft.nl/uuid:fd9b21ff-439f-4551-96e3-7d3bd2560e94","Creating awareness around tyre conditions","Breukers, K.","Pasman, G.J. (mentor); Mooij, S.C. (mentor)","2016","""Transforming the current complex and technical context of car tyres into a more accessible and satisfying experience.” Currently many consumers are not fully aware of the safety issues of their tyres. 60% of all cars in the Netherlands are being driven with at least one under-inflated tyre. In addition to this, 42% of the cars with summer tyres had at least one worn tyre. This leads to dangerous situations How does this master thesis respond to this? By translating the complex information about tyre conditions into tangible information. Furthermore it adds value by explaining the benefits of proper tyre conditions to consumers. The final product is an application that consumers motivate to take care about their tyre conditions. Installed TPMS sensors in the tyres are connected with the application via Bluetooth. These sensors track the accurate tyre pressure and inform consumers if needed. Besides this application reminds and helps consumers to check their tread depth, by scanning the tyres with their phone.","Conditions; Awareness; Human centred design; Application; Consumer","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Design for Interaction (DFI)","",""
"uuid:55c225d5-5e75-4cd0-8097-715108eee172","http://resolver.tudelft.nl/uuid:55c225d5-5e75-4cd0-8097-715108eee172","The Role of Learning in Water Supply Systems: Analysing the Spread of Policies in Rural Uganda using Memetics","Knipschild, F.S.","Herder, P.M. (mentor)","2016","","Learning; Rural Water Supply; Memetics; Multi-Level Governance; Uganda; Agent Based Modelling","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","","",""
"uuid:fa6b06ca-29a4-4a3b-872a-b60f07b02684","http://resolver.tudelft.nl/uuid:fa6b06ca-29a4-4a3b-872a-b60f07b02684","Glass mining landmark","van Gelderen, E.","Asselbergs, M.F. (mentor); van der Zaag, E.J. (mentor); Nijsse, R. (mentor)","2016","Glass structure is often dismissed from usage due to cost and the fear of breaking. However glass is a material that more then any other is capable of unique aesthetic values. One big aesthetic value for glass is adding liquids to it. This liquid in the glass can then also be used as a heating system. Combining structure and climate design. In this research report information on the following topics will be gather by doing a literature study. Glass structure Glass and liquids Glass heating PCM/liquid These literature studies will provide information on what is possible with glass structure and how heating can work in glass. By researching these four main topics a conclusion can be made on how well a glass structure is able to hold the liquid as well as generate heating from it. The objective of this research is to create a glass structure system that has an integrated climate system, combined they create a new aesthetic value for glass structure and will make the choice for using a glass structure more negotiable.","glass structure; columns; beams; climate system; heating","en","master thesis","","","","","","","","","Architecture and The Built Environment","Architecture","","","",""
"uuid:9ecec1a5-e3fa-4468-9ac0-e6ffacaa1905","http://resolver.tudelft.nl/uuid:9ecec1a5-e3fa-4468-9ac0-e6ffacaa1905","Estimation and reduction of peak-locking errors in PIV measurements","Kislaya, A.","Sciacchitano, A. (mentor)","2016","In PIV, the systematic tendency of the measured sub-pixel displacement to be biased towards the integral pixel values is called peak-locking. This occurs when the particle image diameter is less than a pixel. The bias error causes inaccuracy in the measured PIV data which does not reduces with increase in the sample size. Recently developed LaVision’s optical diffuser was investigated to determine the reduction in peak-locking. The point-spread-function width of diffuser was examined to calculate the change in the shape and size of the point source under the influence of different parameters. Planar-PIV experiment were carried out in uniform, low-speed and high-speed flow conditions to analyse the effectiveness of the optical diffuser in reducing the bias error and change in the random error. Also, a comparative assessment was done between the use of conventional defocusing and optical diffusers during image acquisition. The use of optical diffusers reduces the bias error and random error by a factor of three. The reduction in the measurement error is similar to the best defocused optical position of the lens which is very difficult to determine. Additionally, an experimental analysis was done with three different camera-lens combination to determine the best relative aperture size for keeping the measurement error minimal for the large scale 2D PIV. With the help of optical diffusers, experimentalists can have more accurate PIV measurements which would lead to more realistic capturing of the flow phenomenon. With the use of optical diffusers, it would also help the CFD and theoretical experts to compare their predictions with better experimental benchmark results.","PIV; particle image Velocimetry; peak-locking errors; bias errors; random errors; LaVision Optical diffusers; Large scale PIV; Planar PIV","en","master thesis","","","","","","","","2017-08-26","Aerospace Engineering","Aerodynamics, Wind Energy & Propulsion","","","",""
"uuid:4ec866c9-3604-45cc-80f6-0e6ee174eb08","http://resolver.tudelft.nl/uuid:4ec866c9-3604-45cc-80f6-0e6ee174eb08","In vivo multicell inferior olivary recordings: alternative design methods for creating cheap and ﬂexible electrode structures","Kerpels, J.J.","Serdijn, W.A. (mentor)","2016","In order to allow neuroscientists to do in vivo recordings on hard to reach brain tissue, such as the inferior olivary complex, specially designed electrodes are required. Although a variety of electrodes are commercially available, they are usually expensive and it is hard to rapid prototype new designs. This thesis describes the design process of three electrode array designs, each improved based on the ﬁndings of the previous one. The ﬁrst design was made using a FlexPCB production technique, on which gold spots were added to create conducting measuring sites. The second design combined this technique with commercial microwire electrodes. The third design used 3D print technology combined with microwire electrodes to create an electrode array. All designs were tested in in vivo measurements on mice. Successful measurements were carried out with the electrodes and the new production methods were found to be eﬀective. Using a 3D printer to create electrodes makes for a very ﬂexible design process and can be used to create very speciﬁc electrode shapes very rapidly and cost eﬀective.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Microelectronics","","","",""
"uuid:41127090-1c14-45af-b1a9-6efeb667efe3","http://resolver.tudelft.nl/uuid:41127090-1c14-45af-b1a9-6efeb667efe3","Motion Planning in the State Space: Distance- and Steering-Functions","Spierenburg, W.W.F.","Wisse, M. (mentor)","2016","We introduce a motion planning infrastructure, a new set of distance functions and a steering function. Motion planners are used in robotics in order to plan realistic motions by allowing for variable velocities, as opposed to path planners which either neglect or consider velocities as constant. The main goal of this thesis is to design a planning infrastructure with minimal input, which can solve the motion planning problem for various robots. This all within a reasonable time frame and without preprocessing. Here the state space is the combination of all possible positions and velocities. To find this solution the Rapidly-exploring Random Tree (RRT) [1] motion planner has been adapted for use in the state space and was tested using a UR5 robot arm model. By itself it was shown to be unable to connect two states, even after adding 100000 states to its tree. Thus to verify the commonly used euclidean distance metric a new set of simple diIn conclusion the probabilistically complete motion planner RRT has been adapted for the state space, which in combination with the steering function can find a non optimal connection between two states on the gostance functions has been created, which have been verified by calculating their correlation with actual motions and considering their behavior in RRT. Both strong and weak correlations were observed for these distance functions, yet no connections were established between states. Finally a steering function has been created, PID-connect, which connects both the start and the goal state to a zero velocity state using a PID-controller. PID-connect is able to compute a motion between two states reliably in a short time (0.02 ± 0.01s, n = 1000). In conclusion a probabilistically complete variant o RRT has been adapted for use in the state space, which in combination with the steering function can find a non optimal motion between two states on the go. This research provides a solid basis from which further advancements in this field can be made.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","BioMechanical Design","","","",""
"uuid:094e2111-c3ea-489f-af62-46741f54b675","http://resolver.tudelft.nl/uuid:094e2111-c3ea-489f-af62-46741f54b675","Involving Patients in Data Decisions","Lionarons, M.","Sleeswijk Visser, F. (mentor); Giaccardi, E. (mentor)","2016","Data from health wearables promises to deliver value to all stakeholders by improving the quality of care, while making them more convenient and cost-friendly. However, many companies are collecting data now without a clear purpose, expecting it to pay out tomorrow. They handle the data as a currency, with similar risks and benefits, and often choose to leave their customers in the dark about data-related practices and focus on complying with security and privacy rules and regulations with large terms of agreements that nobody reads or understands. Meanwhile they leave their customers uneducated, often leading to a growing apprehension and reduction of trust. However, companies will achieve better results if they do not handle security and privacy as risks but as a brand differentiator. Gaining trust among consumers can become a market and product differentiation benefit, and an integral aspect of a company. Companies should not solely focus on complying with regulations with large terms of agreements but on complying with the trust frameworks inside the minds of their customers by actively engaging them through good communication. In order to do so they must: 1) Educate about data 2) Provide transparency in data use, collection, sharing and storage 3) Allow the user to control and intervene 4) Articulate a fair value and utility exchange for the data. In that way they will develop a long-term relationship with their customers based on digital trust.","Digital; Trust; Healthcare; Data; Decision-making; Strategy","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Design Conceptualization and Communication","",""
"uuid:d9afbb3e-ca3f-47f1-b701-1cc700fb75f3","http://resolver.tudelft.nl/uuid:d9afbb3e-ca3f-47f1-b701-1cc700fb75f3","A compact multi-electrode system to measure in vivo electrical activity in the olivocerebellar system: Measuring sub-threshold oscillations and action potentials spatially and over time","Weskin, M.E.","Serdijn, W.A. (mentor)","2016","In the Erasmus Medical Center (Erasmus MC) in Rotterdam a lot of research is done to investigate the olivocerebellar system, consisting of the cerebellum (motor behaviour) and the inferior olive (sensory input). The correlation and propagation of signals between both parts are of interest when investigating the influence of sensory input on the motor behaviour. To facilitate this research, a compact system to measure in vivo electrical neural activity in the olivocerebellar system spatially and over time is designed and described in this work. The designed system consists of three subdesigns: electrodes, electronics and a measurement set-up. It is small, compact and capable of measuring in vivo neural activity. The Delft Electrode Array proves its concept with four robustly mounted electrode tips, however the number of mounted electrodes should be increased. The Delft Analogue Front End, the electronics system with its custom measurement set-up, can - after only changing software parameters - measure up to 32 channels. Its bandwidth (-3 dB bandwidth from 0.3Hz to 30 kHz) is wide enough to measure sub-threshold oscillations (1 to 10Hz) as well as action potentials (up to 10 kHz).","","en","master thesis","","","","","","","","2021-08-26","Electrical Engineering, Mathematics and Computer Science","Electrical Engineering","","Bio-Electronics","",""
"uuid:165b36db-c4d2-417f-ac69-605f0610283e","http://resolver.tudelft.nl/uuid:165b36db-c4d2-417f-ac69-605f0610283e","The influence of AlSi and MagiZinc coatings on the susceptibility towards hydrogen embrittlement of press-hardening steel","van der Bij, N.K.","Gonzalez-Garcia, Y. (mentor)","2016","With the increasing use of high strength steels in automotive application, the risk of hydrogen embrittlement increases as well. Metallic coatings used for galvanic corrosion protection of these steels influence the absorption behavior of hydrogen in these materials. Two different press-hardening steels with equal mechanical properties, 22MnB5 and HQ1500, with MagiZinc and AlSi coatings respectively were investigated for hydrogen embrittlement properties. The materials were tested using immersion in 0.1M hydrochloric acid under mechanical stress. Corrosion properties of thematerials were investigated using OCP, Potentiodynamic and ZRA measurements. To understand difference in hydrogen embrittlement behavior between the two alloy, the hydrogen trapping densities and diffusion coefficients were determined using the Devanathan- Stachursky permeation method. Also an attempt was made to develop a new hydrogen embrittlement evaluation method through impressed hydrogen currents. The results showed an increase in hydrogen degradation in both materials because of the coating. Also the sensitivity of HQ1500 to hydrogen embrittlement is higher because of a 100x lower hydrogen trapping density. The testing method for hydrogen embrittlement using impressed current is able to reproduce immersion test results and is a viable testing method for hydrogen embrittlement.","","en","master thesis","","","","","","","","2021-08-26","Mechanical, Maritime and Materials Engineering","Material Science and Engineering","","","",""
"uuid:3313350a-3c85-40fd-a5c8-d8548393d4af","http://resolver.tudelft.nl/uuid:3313350a-3c85-40fd-a5c8-d8548393d4af","The design of an Attitude Control System for the SPS-2 satellite: For the Rate Damping, Sun Acquisition and Sun Pointing Modes","Oomen, M.M.","Mooij, E. (mentor)","2016","This thesis covers the design of the Attitude Control System for the SPS-2 configuration, developed at Airbus Defense and Space the Netherlands. The start of the SPS-2 mission consists of three phases: rate damping, Sun Acquisition and Sun Pointing. The SPS-2 satellite has five available mounting places for different payloads. In the first part of the thesis work, different configurations have been considered, of which three were chosen to be considered further. A choice has been made to develop a simulator in Matlab/Simulink, with inclusion of existing models of the GGNCSim repository. The correctness of the implementation of those models has been verified with ADS, where it has been discovered that there was a sign ambiguity in the aerodynamic model. At this stage, the assumption of modeling the complex SPS-2 geometry as a simple cylinder shape has been verified. The second part of the thesis describes the development of the sensor and actuator models, and the control algorithms which have been designed. The sensor and actuator model error parameters have been obtained from the data sheets of different manufacturers. The models have all been verified. The controllers which have been designed are a nominal B-dot detumble controller in the RD mode, a simple PI controller on the velocity in the SA mode and the quaternion-feedback controller in the SP mode. The gains for the different controllers have been selected such that these work for all configurations. The stability has been looked at briefly (gain and phase margins), but no requirements have been imposed on those. In the last part of the thesis, the simulation performance runs have been performed. Here the uncertainties in the error parameters, uncertainties in the satellite geometry, variations in initial conditions and the impact of different actuators have looked at. It has been found that the detumble time is within the requirement of 1.5 orbit, but that the maximum magnetic dipole moment shall be a minimum of 15 Am2. The SA and SP results show that the Sun is in all cases acquired within 1 orbit and that the time to be actual Sun pointing is less than 10 minutes. The pointing accuracy is within 1.92 deg for all cases. For both reaction wheels which have been considered, more than enough margin was found for the angular momentum build-up. As such, there is no need to offload the wheels during the first part of the mission. The SunSpace wheel will suffice. The simulation performance runs have shown that the requirements are met, even in presence of the maximum sensor errors. Critical may be the impact of the Earth-albedo error, which has been roughly estimated. More research shall be performed here. The current simulator structure and models which have been developed give a good basis for the further design of the ACS of the SPS-2, but also for other projects.","Attitude Control System; Airbus; SPS-2; satellite; rate damping; Sun acquisition; Sun pointing; detumble; B-dot; quaternion-feedback controller; magnetorquers; reaction wheels","en","master thesis","","","","","","","","2021-08-26","Aerospace Engineering","Astrodynamics and Space Missions","","","",""
"uuid:085eeb2a-1ce1-43c5-ad7c-1278c02c9851","http://resolver.tudelft.nl/uuid:085eeb2a-1ce1-43c5-ad7c-1278c02c9851","Resilient at work","Jain, R.","Desmet, P.M.A. (mentor); Siriaraya, P. (mentor); Schok, M. (mentor)","2016","This Master thesis report presents how design methodology founded on principles of positive psychology can be used to design for the prevention of stress. People who work in stressful professions are at increased risk of health problems. One of the key ways to maintain a positive outlook on life is by increasing the resilience of an individual, defined as the capacity of an individual to adapt to changing external circumstances or stressful/traumatic life events. Maintaining and strengthening resilience is expected to prevent sick-leave, burnout and other associated serious psychological problems (Rutter 1987). The project, which is a collaboration between Mindset and the Delft Institute of Positive Design focuses on an opportunity-driven approach wherein the emphasis is on eliciting positive emotions to achieve long-term happiness as a means to build resilience, in that, it’s not fixing what’s wrong(stress) as much as building what’s right (personal strengths) (Seligman 2002). Building upon this, the project explores how advancements in interactive technology could aid the process of strengthening the resilience of individuals who work in stressful professions. The report is structured into 3 phases. In the first phase of the report connections are sought between stress, resilience and positive psychology. Principles of building resilience paved the way for the generation of nine concept ideas, which were discussed with participants from the Dutch National Police, and a group of social workers dealing with patients suffering from post-traumatic stress disorders and other psychological conditions. The discussion led to selection of a concept idea for further elaboration in the subsequent phase. Furthermore, the discussion with the policemen and social workers led to the synthesis of interaction qualities of the product being simple and personal. The selected idea was inspired by Seligman’s empirically validated activity of recalling three good things on a daily basis with causal explanations which caused lasting happiness, and saw a decrease in depressive symptoms over a six month period. The concept is a button, and the user presses it every time he feels even the slightest of positive emotions, thereby providing a means to build a collection of positive moments. In the second phase of the report, a divergent approach was adopted in order to elaborate and explore the potential of the concept. During this phase, the concept was broken down to its basics: a positive journal, and various means like framework of emotional granularity at providing causal explanations to the moments were tested. Over the course of several iterative user tests, a division was made of the concept into two parts: moment of registering positive moments and moment of reflection of registered positive moments. In addition to being simple and personal, further interaction qualities for both moments were unearthed. The qualities for registering the positive moments were fixed as instantaneous and mindful while for the reflection of positive moments, the qualities were fixed as savouring and inviting. Concepts were generated and compared using a Harris profile analysis method. A final concept: the ebo band was synthesized to conclude this phase. In the third phase of the report, an experiential prototype is built and tested with 8 participants for a duration of 5 days per participant. The feedback on the experience was mostly positive, with participants claiming to have become more aware of their positive moments by registering them and finding happiness in the reflection of their registered moments. Finally, conclusions were drawn and recommendations for further product development have been presented.","stress; resilience; positive-psychology; Eudaimonia; well-being; ebo","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","","",""
"uuid:4f520286-ec98-49fb-a005-f18023c1ef9b","http://resolver.tudelft.nl/uuid:4f520286-ec98-49fb-a005-f18023c1ef9b","Identification of Room Boundaries for Sound Field Estimation","Coutiño Minguez, M.A.","Heusdens, R. (mentor); Møller, M.B. (mentor)","2016","Echoes generated by the sound reflected off the walls of a room carry information about the geometry of the enclosure. Capitalization of this acoustic property could lead to improvements in current state-of-the-art methods for sound field estimation, where prior information can be used to improve the conditioning of the problem. In this thesis, robust and computational efficient methods are developed for identifying first order reflections to estimate the room geometry using small microphone arrays. Furthermore, as the estimation of such reflections becomes even more challenging in actual audio reproduction systems, this work aims to develop methods capable to deal with complications that might arise due to the employed drivers. This is done by considering the estimation problem in two different scenarios. Firstly, the first order reflections estimation problem is posed as a sorting problem. For this case, a set of echoes, received at different microphones, must be grouped accordingly to the wall which originated them. This problem is solved by using a greedy subspace-based algorithm. The proposed approach provides similar performance compared with the state-of-the-art method at a reduced computational cost. For the second scenario, instead of echoes, only raw microphones measurements are available. This instance of the problem is posed under an estimation theory framework, and solved by sequential minimization of a non-linear cost function based on the propagation of waves. Experimental results, evaluated in simulated shoe-box shaped rooms, demonstrate the performance and applicability of the proposed methods for room geometry estimation.","source localization; acoustic echoes; room geometry; sorting reflections; greedy algorithm","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Circuits and Systems","","","",""
"uuid:ebed9a0a-0fce-4005-8c0b-17455b2fa10d","http://resolver.tudelft.nl/uuid:ebed9a0a-0fce-4005-8c0b-17455b2fa10d","An Experimental Investigation on the Rock Mechanical Behavior of Synthetic Layered Systems and Load-Cycling of its Individual Constituents","Janmahomed, F.R.","Barnhoorn, A. (mentor)","2016","Many authors already made an attempt to understand the effect of load-cycling on material strength and the evolution of elastic parameters. However, until now there was no study on the effect of load-cycling on the evolution of elastic parameters over the complete stress strain curve, i.e. the linear elastic regime, the fracturing regime, and the fractured regime. In addition, none of the authors focussed on the effect of load-cycling on fracture network improvement. Although previous studies already showed that elastic moduli of layered systems may be determined from properties and volume fractions of its individual constituents, there is no study done on the relation between rock mechanical properties, i.e. strains and yield or failure stresses, of (synthetic) layered systems and its constituents. Furthermore, in the literature no description is found on fracture characteristics of a (synthetic) layered system. Hence, an experimental investigation is conducted on the rock mechanical behavior of synthetic layered systems subjected to increased-loading and the effect of load-cycling on its individual constituents’ rock strength, elastic parameter evolution, and fracture network improvement. The increased-loading and load-cycling rock mechanical experiments are unconfined compression tests performed at room temperature. When comparing two of the same rock materials with a maximum deviation of 1% in porosity, load-cycling leads to failure at much lower stress levels when compared to increased-loading. Within the linear elastic regime, load-cycling returns a stabilized Young’s modulus which is always larger than its envelope value, while the Poisson’s ratio of the last load cycle coincides with its envelope value. Load-cycling generates an improved fracture network when compared to increased-loading. Characteristics of the improved fracture network are the increased fracture densities and the more uniform distribution of the fractures over the volume of the material. For vertically stacked synthetic layered systems, the elastic moduli and strains can be well-predicted by the Reuss Average which uses the average rock mechanical properties and volume fractions of the individual constituents. In contrast, for synthetic layered systems the stress level at failure point is independent of the volume fractions of its constituents and is observed to be in the vicinity of its weakest constituent. Despite this, fractures are still observed in the strongest constituents. The fracture propagation through the strongest constituent is ascribed to be due to amplified stress concentrations at the tip of the propagating fracture. However, from 2-layered systems it is observed that fracture propagation through the strongest constituent depends on the thickness of the weakest constituent. At the constituents’ boundary in synthetic layered systems, there is no offset in fracture path when going from one constituent to another nor is there a sudden change in aperture. However, there is a change in fracture inclination in a way such that the fracture inclinations of the individual constituents are respected. In addition, fractures in synthetics include cataclastics over the high porous intervals (±10-25%), while the same fracture is clean over the low porous intervals (±0,5-5%).","Rock Mechanics; Load-Cycling; Increased-Loading; Fracture Improvement & Characteristics","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering","",""
"uuid:7923ea86-8d83-4acc-95e2-530b18920ee2","http://resolver.tudelft.nl/uuid:7923ea86-8d83-4acc-95e2-530b18920ee2","The FA3D Hand: Design and evaluation of a Functional and Anthropomorphic hand prosthesis using the advantages of 3D-printing","Ten Kate, J.D.","Smit, G. (mentor)","2016","3D-printing has high potential in the field of prosthetic hands because of the ease of production and the possibility of individualization. All over the world people are designing and printing new devices. However, most of these devices lack functionality, do not have an anthropomorphic look and do not utilize the advantages of 3D-printing. The goal of this thesis was to design and evaluate a functional and anthropomorphic hand prosthesis using the advantages of 3D-printing. A prosthesis should meet basic user requirements and should have such a shape, size and color that it has a natural appearance comparable with the sound hand. The advantages of 3D-printing technology can be used to make a prosthesis of as little parts as possible to simplify the assembly, reduce costs and increase the anthropomorphic look. A body-powered hand prosthesis was designed which can be controlled using a shoulder harness. The fingers of the hand were designed out of one part with a flexible joint in the metacarpophalangeal (MCP) joint and one in the proximal interphalangeal (PIP) joint. For these joints the material, flexure length, wall thickness and the presence of an interphalangeal crease were determined to find an optimal joint geometry to fulfill the requirements; maximum actuation force and maximum cable displacement delivered by a shoulder harness. This resulted in the FA3D Hand; The Functional and Anthropomorphic 3D-printed hand prosthesis. It has fingers fabricated out of one part with flexible joints and an actuation cable hidden inside the fingers. The FA3D Hand was made of the flexible material NinjaFlex using fused deposition modeling (FDM) 3D-printing and consisted of only seven 3D-printed parts; four fingers, a thumb and a palm made of two parts. Everything was assembled with the use of standard nuts and bolts. The total material cost of the FA3D Hand is just ¤30,-. The FA3D Hand has a weight of 307 gram and can be connected to a socket using a standard prosthesis socket connector. With the use of a mechanical test bench the actuation force needed to operate the hand, the fingertip forces the hand can deliver and the required cable displacement were measured. A clinical test was done to show the functionality of the hand and to get a better insight on the opinion of the appearance. Results show that even though the fingers have flexible joints and can be operated with a cable displacement of 34 mm, which is lower than the maximum allowable cable displacement, a considerable high actuation force (266 N) is needed resulting in a low pinch force (3,5 N per finger). Using the Southampton Hand Assessment Procedure (SHAP), subjects (10 able-bodied and one amputee) where instructed to manipulate both lightweight and heavyweight abstract objects and activities of daily living (ADLs). Results show that a large variety of objects can be manipulated in a quick way and the opening width of 80 mm enables the FA3D Hand to grasp a broad range of objects (from a coin to a large cup). The test also showed that a large part of the people appreciated the anthropomorphic look of the FA3D Hand. The FA3D Hand is a functional and anthropomorphic hand prosthesis designed using the advantages of 3D-printing. Although the delivered grip forces are lower than needed, the FA3D Hand is highly functional, has the shape, size and color comparable to the sound hand and consists of considerably less parts than comparable 3D-printed hand prostheses.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","BioMechanical Engineering","","","",""
"uuid:aa87e04d-83eb-486f-87c4-a5a27aeacc99","http://resolver.tudelft.nl/uuid:aa87e04d-83eb-486f-87c4-a5a27aeacc99","Designing a bio actives application system for the antimalarial eave tube project in Ivory Coast","Polman, H.H.","Diehl, J.C. (mentor); Kuipers, H. (mentor)","2016","The eave tube project in Ivory Coast is set up by In2Care and is currently funded by a grant from the European Union and the Bill & Melinda Gates foundation. In2Care developed a new method to make housing mosquito-proof and simultaneously target mosquitoes with insecticide as they try to enter the house. The test study in Ivory Coast should show whether or not their product is efficient in eliminating mosquitoes from the targeted areas. With this launch in Ivory Coast there is need for an insecticide powder application machine which is able to apply insecticide to the product within the defined local context. In this graduation project a research and design process is conducted to reach three main results. 1. The first result is an insecticide application machine which is scalable and adjustable to preferences in multiple contexts. 2. The second result is an user interaction between the operator and the application machine. Key in this interaction is the health safety from the operator to reduce the amount of contact with insecticides. 3. The third result is an overview map to indicate regulations, material flows and storage conditions to ensure a consistent approach in different environments. In order to get a full understanding of capable methods to apply an insecticide powder to the eave tube, extensive testing has been conducted. Before the graduation project started there was no to little information available about binding properties between the netting of the eave tube and the insecticide powder. Research and testing showed that it is recommended to bind the particles to the netting by means of friction with a brush. The developed product is inventive by its simple means of printing a powder to the netting with adjustable preferences according to locally changing infield demands. It is handled by a locally trained operator which follows simple steps and guidelines. The excess of insecticide powder is collected in the filter attached to the application machine and offers the opportunity to reuse this powder again in the process. To evaluate this principle of printing insecticide powder onto the eave tube netting a one-to-one functional prototype is built. This prototype is solely focussed on printing a layer of insecticide powder to the surface, and in future development the brush will be added. Since the prototype proofed to be efficient, In2Care plans to develop this prototype even further so that it can be used in the field.","BoP; malaria; powder application machine; printing; bio actives","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","","",""
"uuid:70e74b61-9bcf-4941-919b-19be5151ae80","http://resolver.tudelft.nl/uuid:70e74b61-9bcf-4941-919b-19be5151ae80","Applying the epidemic spreading model on the brain network to explain effective connectivity","Zhou, X.","Meier, J. (mentor); Van Mieghem, P. (mentor)","2016","Network science studies a complex system as a network to capture the connectivity patterns and topological features. Different network topologies have been observed to shape dynamic spread- ing processes on the network in various ways, while the exact relationship is complicated and not yet fully understood. Epidemic models are often applied to describe the spreading process on a network and to facilitate studying the dynamic interactions. We apply a simple epidemic model, the Susceptible-Infected-Susceptible (SIS) model, on the structural brain network to explore the topological properties that drive the dynamic processes. A recent study examined the transfer entropy of empirical data and observed a dominant posterior-anterior spreading pattern in the brain. In both transfer entropy and delayed correlation measures, we show that hubs are more sending information to the network than lower degree nodes. With our continuous-time simu- lations, we also found the empirically-observed posterior-anterior global pattern. Based on our results, the brain topology of hubs mainly located at the back of the brain seems to be responsible for the emergence of the global pattern.","global pattern; SIS epidemic spreading model; brain structural network; functional connectivity; effective connectivity; delayed correlations; transfer entropy; directionality","en","master thesis","","","","","","","","2017-01-01","Electrical Engineering, Mathematics and Computer Science","Intelligent Systems","","","",""
"uuid:82f0bbae-b088-4fe6-987e-149647ce16ab","http://resolver.tudelft.nl/uuid:82f0bbae-b088-4fe6-987e-149647ce16ab","Uncertainty in electricity generation mixes","Wittmann, D.","Mutel, C. (mentor); Burgherr, P. (mentor)","2016","Introduction to Life Cycle Assessment and Ecoinvent: Life Cycle Assessment (LCA) models the complex interaction between a product and the environment form cradle to grave. The aim of the LCA is to capture the environmental impact of a product and helps with decision making. One of the background databases for LCA is Ecoinvent. Studied Problem: In Ecoinvent, the data for the electricity generation mixes in a geographical area is generated by a log-normal distribution. The aim of this thesis is to provide an analysis of the uncertainty (a total range a value can take with a corresponding probability – probability density function) of the log-normal distribution in Ecoinvent and to promote a new approach for sampling data. The reason for studying the uncertainties for the electricity generation mixes is to give a better approximation how much electricity contributes to overall emissions in a LCA. The Methodology and Approach: The thesis covers two interlinked parts. The first part evaluates the fit of the log-normal distribution to real data. For this, the different histograms are compared, the ANOVA test is performed for a difference in the seasons, the hartigans dip test is performed, and to calculate the absolute percentage difference the Method of Moments (MoM) of the real data is compared to Ecoinvent. This is accompanied by the goodness of fit tests. Additionally, a linear correlation and cross correlation is applied. The linear correlation is applied in the second part and the cross correlation should provide as an additional evidence that the energy types are correlated. In the second part the correlations are applied in Brightway v 2, and we perform the correlated sampling. The results from correlated sampling, sampling from the parent data and the data in Ecoinvent v 2.2 and Ecoinvent v 3.2. are compared. (for a better overview consult chapter 9.2) Findings and Results Results of the statistical analysis showed that the log-normal is not a good fit for the energy mixes, since 35/56 distribution were at least bimodal. Additionally, the uncertainty for the different distribution mixes is underestimated. The standard deviation of Ecoinvent is on average 1100% off compared to the MoM. Correlated sampling was not possible, due to programming limitations and time. The sampling from the parent data showed that the uncertainty for the impact category of the global warming potential was underestimated. For the impact category ReCiPE and Ecotoxcicity, this was not the case, because they are more prone to other substances. Conclusions • The uncertainty in Ecoinvent is underestimated and could be improved at low cost. • A multimodal distribution fits better (in most cases) than the log-normal distribution. • There are some significant correlations between different fuel types. Recommendations • Improving the uncertainty in Ecoinvent by increasing the standard deviation. • Further research in correlated sampling for Ecoinvent. • Research in implementing a bimodal or multi-modal distribution into Ecoinvent. Limitations of this report • Active forecasting (modelling) was not investigated for Ecoinvent. • Correlation analysis did not consider partial correlation and multi-colinearity. • Implementing a standardized bimodal distribution (which is difficult) has not been covered.","","en","master thesis","","","","","","","","2016-07-05","Technology, Policy and Management","Engineering and Policy Analysis","","EMIN Economics and Management in Network Industries","",""
"uuid:4aee5956-50a4-4ed4-856e-4b2350433a67","http://resolver.tudelft.nl/uuid:4aee5956-50a4-4ed4-856e-4b2350433a67","Adapting urban areas to climate change: a necessary evil or a world of opportunities?","Wouters, K.","Van de Ven, F.H.M. (mentor)","2016","","Climate change; urban developement; stakeholders; planning support system; adaption support tool","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","","",""
"uuid:3dd65f1b-dc0f-4b52-8e61-1edcda64c69d","http://resolver.tudelft.nl/uuid:3dd65f1b-dc0f-4b52-8e61-1edcda64c69d","Use of uncertain production data in nodal analysis","Alderete Berzabá, A.","Jansen, D.J. (mentor)","2016","The objective of this thesis is to implement a data assimilation algorithm for production data as a first step in dealing with uncertainties in nodal analysis. The methodology is based on the Ensemble Kalman Filter (EnKF), a data assimilation technique that aims to optimally combine uncertain ‘prior’ model predictions and uncertain measured data to obtain a ‘posterior model’ with improved predictive power. The implementation of stochastic nodal analysis models, to generate the uncertain predictions, and the data assimilation procedure were performed with Matlab. A simple production system with uncertain parameters was considered, consisting of a well head choke, a tubing and a near-well reservoir region. The uncertain parameters (random variables) were (log)permeability, skin, choke discharge coefficient and tubing roughness. The predicted and measured variables were the flowing bottom hole pressure and flowing tubing head pressure. The measured data in the EnKF should come from field measurements. To test the method, we performed ‘twin experiments’ in which we used synthetic data with added noise. In the examples considered we achieved a reduction in the uncertainty of the model parameters.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering","",""
"uuid:484e8250-7f38-46de-8b5b-7637c6ab77c9","http://resolver.tudelft.nl/uuid:484e8250-7f38-46de-8b5b-7637c6ab77c9","Guidance and Control for Re-entry Vehicles in the Terminal Area: Application of Pseudospectral Methods","Huneker, L.J.","Mooij, E. (mentor)","2016","One part of spaceflight technology that has always been in demand is to go to space using a vehicle that can be re-used without much additional costs. It is roughly five years ago that the last Space Shuttle mis- sion took place, and replacements are currently in development. Yet, much of the available literature is still based on architectures created during the development of the Space Shuttle. The goal of this thesis is to expand the current knowledge with modern computational methods. This thesis demonstrates if pseudospectral methods can be used as the basis of a guidance and control architecture in the terminal area. In the past, these architectures were based by decoupling the longitu- dinal and lateral dynamics, using the energy available to calculate the current state without knowing what is to come, and calculating many variables and possible scenarios before the flight has even commenced. The architecture presented here does not require this, and it demonstrates an integrated package that can do this on the fly, by deploying a type of non-linear optimization method called pseudospectral methods. The X-38 is chosen as the reference vehicle due to the compatibility of the aerodynamic data set with non- linear optimizers. A 3-DOF simulation is created to see what set of feasible trajectories can be created un- der nominal conditions. Subsequently, a single reference trajectory is used as a basis to determine if the guidance subsystem is able to cope with a wide variety of initial conditions. The guidance system proved to be quite robust. A set of winds and gusts have been simulated and it would return to the desired state as long as there is an approximate predictive ability. The research is later continued by extending to a 6-DOF simulation. A control subsystem based on LQR is implemented with good results. A PSM implementation is introduced after and the results are promising, but the computation time is too high to be feasible. The upside is that a result is found without calculating the deflection of the trimmed state and gains, which was required with LQR. The conclusion is that pseudospectral methods can be used as a robust real-time guidance method, but that using it as a control method is currently not feasible. It is thus proven that pseudospectral method is able to deal without calculating predictive trajectories and gains as was done with the Space Shuttle. This reduces the complexity of re-entry problems by a sub- stantial amount. The time it takes to solve problems with new conditions and spacecraft is reduced, which might encourage other people to continue in this direction. There is still much more possible than what is presented in this thesis.","","en","master thesis","","","","","","","","","Aerospace Engineering","Space Engineering","","Space Exploration","",""
"uuid:12f0c3b3-5bac-4167-999e-6adb3eb73eea","http://resolver.tudelft.nl/uuid:12f0c3b3-5bac-4167-999e-6adb3eb73eea","Follow-The-Leader Control: A mechanical control mechanism for path following deployment of surgical instruments","Gottenbos, S.","Breedveld, P. (mentor); Henselmans, P.W.J. (mentor)","2016","Surgical procedures are shifting towards Minimal Invasive Surgery (MIS). Surgeons are currently limited in their actions by the straight rigid instruments available to them. Devices capable of navigating along curved trajectories increase a surgeon's workspace, which can benefit surgical performance. This paper shortly goes over the state of the art of Highly Redundant Devices (HRD) capable of Follow-The-Leader (FTL) behaviour. In literature virtual and physical control methods are used. Virtual control relies on computer control in combination with actuators. The actuators control the tip segments via cables running the length of the device. Physical control stores and transfers shape information in a mechanical configuration. From the state of the art it was found that physical control has the potential of reducing the amount of actuators severly. However, it was also found that these mechanisms are, up to now, placed inside the tip. The integration of the physical control inside the tip leads to problems with stiffness, accumulating errors and minimisation. A combination of attributes from both virtual and physical control resulted in a novel mechanical FTL control principle: MemoSlide. This principle essentially captures a software algorithm in a mechanism. The suggested control principle reduces the number of actuators needed for FTL control. A Proof of Concept (PoC) demonstrator model, showcasing the MemoSlide principle in a plane, is designed and fabricated. Friction, Jamming and Locking were identified as crucial aspects to consider while implementing the MemoSlide principle. Experience gained during fabrication and evaluation resulted in a set of design guidelines and a simplification of the MemoSlide principle. Application of the simplified principle and the design guidelines resulted in a conceptual design of a control module. The control module is intended to control a steerable surgical tool tip as inspiration for future applications of the MemoSlide principle.","Follow-The-Leader; FTL; highly redundant devices; HRD; surgical instrumentation; pathway surgery; minimal invasive surgery; MIS; prototyping","en","master thesis","","","","","","","","2017-09-01","Mechanical, Maritime and Materials Engineering","Biomechanical Engineering","","Medical Instruments & Bio-Inspired Technology","",""
"uuid:c722d034-82fd-4d47-a7c5-144481408d89","http://resolver.tudelft.nl/uuid:c722d034-82fd-4d47-a7c5-144481408d89","Human Machine Interface in a Highly Automatic Diving phase","Heijboer, S.","Tempelman, E. (mentor)","2016","In response to current developments in the automotive industry to apply more diverse, adaptable and multifunctional materials in car interiors, this graduation thesis researches the possible role of two smart materials: on LTM and Aito HapticTouch. The fundamental developments in reduction of accidents, ownership and emission (the 3 Zero’s) all will have an effect on the way dashboards and HMIs will be used and designed. In order to find out how the LTM material and Aito HapticTouch can contribute to these developments in the coming approximately 5 years, and can be applied in automotive dashboards and in that way bring value to end users, it is essential to understand their respective working principles. The LTM material is comprised of a luminescent OLED layer and a touch-sensing polymer piezo layer. Aito’s material combines touch with active haptic technology by using piezo disks that create a haptic feedback feeling in combination with any kind of material. Several meaningful tangents can be found with the specific combination of characteristics from Aito’s technology. On the other hand, the the LTM material’s affordances do not suit the developmental key points as well as Aito’s do. From all UI components, the steering wheel will be affected most by the development towards autonomous driving and Aito’s technology is particularly suitable to accommodate these developments in the steering wheel because it is the primary haptic control element in a car. Therefore, the design brief is to design and build a demonstrator for Aito. In short, the concept of highly automated driving (HAD) calls for a more minimalistic approach in aesthetics and ergonomics, in particular the steering wheel that will have to blend in with the interior when not in use. Also, the communication between driver and car in the event of an authority transition in between manual driving and HAD has to be defined in a very clear way. In response to the development of shared car systems, Aito could be very relevant by contributing to adaptable interfaces that have disappearing, changing or emerging functions, and that have a seamless integration with a smartphone UI. Moreover, Aito’s technology could replace traditional relatively heavy components by lighter components and in this way play a part in the shift towards zero emission. The demonstrator has been validated at several suppliers/OEMs, and generally it can be concluded that it illustrates clearly how this new technology can be implemented in an application with a new user interface.","materials; automotive industry","en","master thesis","","","","","","","Campus only","2018-08-25","Industrial Design Engineering","Design Engineering","","","",""
"uuid:6219f4a4-c3cc-4a73-91a9-f80471a9c1b8","http://resolver.tudelft.nl/uuid:6219f4a4-c3cc-4a73-91a9-f80471a9c1b8","Design of a modular bodywork set for a new generation motorcycles for the East-African market","Toet, R.","Kuipers, H. (mentor)","2016","Koneksie is an Amsterdam-based company and has designed a motorcycle especially for the purpose of being used as a taxi in eastern-Africa, the Kibo K150. The design of the Kibo K150 features four main characteristics; Durability, Ease of Maintenance, Availability of parts and Quality. The design of the K150 is not innovative but uses proved technologies that can be applied, used and maintained in the target market. In order to expand the market share of Koneksie in the East-African motorcycle market, Koneksie plans to develop new motorcycles. Since the K150 caters to the taxi market, Koneksie aims to the second largest motorcycle market in East-Africa: Personal transport and commuting.","modular bodywork set; motorcycles","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","","",""
"uuid:41ccb906-dab7-4a6e-a40f-2e9d12f26a02","http://resolver.tudelft.nl/uuid:41ccb906-dab7-4a6e-a40f-2e9d12f26a02","Extension of the Use of a Single Degree of Freedom Model for Non-Uniform Load Distributions","Ferretti, D.L.","Van Doormaal, A. (mentor); Sluys, L.J. (mentor); Van Dalen, K.N. (mentor); Weerheijm, J. (mentor); Houben, L.J.M. (mentor)","2016","In this research the translation of non-uniform load distributions to equivalent uniform load distributions has been investigated. The equivalent uniform load can be used as input for a single degree of freedom model to compute vulnerability analyses in which hundreds of scenarios need to be analysed.","single-degree-of-freedom; SDOF; Blast; explosions; approximation; approach; dynamic load; dynamic response","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","","",""
"uuid:cc7df2d1-ef45-4cd6-a4dd-b927604b5fa0","http://resolver.tudelft.nl/uuid:cc7df2d1-ef45-4cd6-a4dd-b927604b5fa0","Influence of operational conditions and sludge characteristics on anaerobic digestion process of a plug flow system using a series of complete-mix reactors in a series","Evangelou, M.","de Kreuk, M.K. (mentor)","2016","","","en","master thesis","","","","","","","","2018-08-25","Civil Engineering and Geosciences","Water Management","","Sanitary Engineering","",""
"uuid:c049c3c2-c8d8-4a3c-86ef-222846e91291","http://resolver.tudelft.nl/uuid:c049c3c2-c8d8-4a3c-86ef-222846e91291","The impact of event-triggered control on the energy consumption of a legged robot","Oosterhuis, B.F.","Mazo Espinosa, M. (mentor)","2016","In event-triggered control (ETC) the control task is only executed when needed to ensure performance or stability. A well known property of ETC is that it can reduce the number of control task executions while retaining similar performance. A large amount of research is focused on how ETC can be used to achieve energy savings in wireless networked control systems. However, the effect of ETC on the actuation energy consumption and whether it could be used as a strategy to achieve actuation energy savings has been unexplored. This thesis presents a study of the impact of ETC on the energy consumption of a legged robot. The focus is on the control task of balancing the robot on top of its legs. To perform this study, a system allowing real-time control and energy monitoring of the robot is developed. This system allowed an experimental comparison between a standard periodic time-triggered controller and a periodic event-triggered controller. Three experiments were performed that enabled time-triggered control and ETC to be compared during transients. The results show that ETC is capable of affecting the energy consumption while reducing the number of control updates. During the first two experiments the system was brought into a transient state by starting the system from nonzero initial conditions. During the third experiment a disturbance was applied on the control input of one leg pair. In the first experiment the energy consumption during ETC increased with 3.0% while in the second and third experiment the energy consumption decreased with 3.2% and 7.8% respectively. These differences occurred while reducing the number of control updates by at least 35.9%. Analysis of the results suggest that the differences in energy consumption are due to the different control input during ETC combined with the effect of unmodeled dynamics.","event-triggered control; PETC; legged robots; energy consumption","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Center for Systems and Control (DCSC)","","Embedded Systems","",""
"uuid:f44a0e5f-a9bb-43e1-a09f-b63fe716f738","http://resolver.tudelft.nl/uuid:f44a0e5f-a9bb-43e1-a09f-b63fe716f738","Thing Different: The roles of Connected Objects in knowledge work practices of the future","Bitter, F.K.W.","Giaccardi, E. (mentor); Rozendaal, M.C. (mentor)","2016","Emerging technologies not only shape how people live, entertain themselves, move forward, relate to each other, but also how they work. The Internet of Things is a technological development which is by many experts, expected to change all of these aspects tremendously. However, it is not yet known which role this technology will actually play and what it would mean for us. In the domain of work IoT technology is mainly deployed in production work and logistics, to increase efficiency, accuracy, and eventually productivity by connecting objects or by giving them the ability to collaborate with the workers. In the meantime knowledge work is becoming increasingly important, and due to this different context IoT technology as well as Connected Objects have to be approached differently, if they should add a meaningful value in this field of work. In a domain where the main activity is “thinking” rather than manufacturing; social dynamics, collaboration and creativity are aspects to stimulate and support. How knowledge work is practiced is also expected to change in the future. At present not much is known how to achieve these goals and what value Connected Object can have in the knowledge work context. This thesis therefore addressed the main question: “What are the roles Connected Objects have in knowledge work practices of the future.” The question about the role of Connected Objects in knowledge work has two implications: first if they would play a role at all, and second, which role would they take on? The first implication explores the value, relevance and the application of Connected Objects in knowledge work: where could Connected Objects support knowledge work and what is desirable for knowledge workers? The other implication is to understand role as an expected behavioural pattern of the objects, which also explores different kinds of agencies: are Connected Objects perceived as colleagues or as obedient slaves, how can they influence work, and how does it feel to interact with them? A Research through Design approach (Zimmermann, Forlizzi & Evensons, 2007) was undertaken for this thesis. “Design” in this approach acts not as product propositions, but as a genuine way of generating knowledge. A designed artefact serves as a research tool, and also embodies this new knowledge, compared to conventional research approaches, where research is done mainly text based. Furthermore a concept-driven approach (Stolterman & Wiberg, 2010) was pursued. Here a strong concept is seen as embodied theory and can also serve as an intellectual tool to explore futures and possible problems linked to the concepts. By following these approaches instead of creating affirmative design proposals to be tested with users, a broad and diverse spectrum of statements regarding technology was achieved. This led to more meaningful aspects, rather than only revealing usability issues. Phase 1: Review and Interviews An extensive literature study and expert interviews were done at the beginning. Here it was concluded that, for IoT to be deployed, different aspects have to be taken in consideration to the knowledge work context compared to the usual context. Compared to Software solutions, IT technology or inert tools, Connected Objects hold unique qualities which change the knowledge work environment: they exploit physical affordances, can be autonomous and utilize data. The shift from IT to IoT technology can be promising: it can connect formerly separated people, trigger new ways of thinking, constitute new work practices, or involve new forms of knowledge, which could be generated from Connected Objects. Four main opportunity fields are identified: Influencing Processes, complementing humans, acting as (neutral) mediators, as well as embedding and conveying values. Phase 2: Creating embodied hyptheses Three concepts were developed to act as embodied hypotheses, which were then used as research as triggering artefacts (Mogensen & Trigg 1992) in the final study with knowledge workers. To create these concepts an iterative design process was undertaken. This process was informed by insights from the research phase, current social as well as technological developments, and later on by the different factors, such as the embodiment of a certain knowledge worker role and different kind of agencies for each concept. Overall three main hypotheses were created to be checked in the study which were constructed around the questions: 1. What if Connected Objects participate in work processes and have their own behaviour? 2. What if work and life continues to intertwine and Connected Objects try to support that? and 3. What if Connected Objects are approached as independent colleagues with special skills? Phase 3: Evaluating the future Models were built to serve as plausible triggering artefacts and scenarios were formulated to support the artefacts. The interviews with the participants were undertaken in their own everyday work contexts. By using open coding, which is a method for qualitative data analysis, and by using the software tool MAXQDA, the interviews were then analyzed. Phase 4: Drawing conclusions Creating Connected Objects for knowledge workers needs to take their use contexts and practices into consideration. The value for creative context is particularly seen where Connected object can serve as disruptive elements. The social dynamics and expected roles of different human and non-human actors within this context has to be considered. Assigning roles to Connected Objects and giving them a unique character can create interesting new situations. Grounding work decisions on algorithms and data, serving as hard facts, can also lead to blind spots and exclude humans and their unique qualities. Nevertheless technology is able to extend human capabilities and also complement humans by providing genuine new skills and thing-perspectives. However a full reliance on Connected Objects cannot be expected – here it is about fostering smart collaborations, instead of creating smart products. Connected Object and human have to be envisioned as a team. However looking at all the advantages this technology can entail, it should never be just a means to it’s own end, but should help to achieve a preferred future. What this preferred future is, has to be constantly explored and discussed – possibly with the help of design.","connected objects; knowledge work practices","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","Human Information Communication Design","",""
"uuid:b1aa758f-ba26-48cd-a15a-6955a05b0655","http://resolver.tudelft.nl/uuid:b1aa758f-ba26-48cd-a15a-6955a05b0655","Has Finance Grown Too Big?: On The Social Efficiency of the Shadow Banking System","Karagiannis, V.","Storm, S.T.H. (mentor)","2016","Over the previous thirty-five years the financial sector of the high-income (OECD) countries has considerably increased in size, importance and sheer complexity. A social efficient financial system is crucial to a stable, productive and innovative market economy. In such a system, the economic welfare is enhanced as banks and financial institutions provide funding for productive investment, for research, design and development RD&D and for high-tech innovators. They also provide insurance that results in risk reduction, create sufficient amounts of useful liquidity, run an efficient payments mechanism and generate financial innovations to do all these useful things more cheaply and effectively. But decades of financial sector’s rapid growth have been taking place mostly in (what is officially called) the “shadow banking system” (SBS) – the unregulated and opaque part of our banking system where most of the (often) speculative financial engineering has taken place and where the financial imbalances were built up which have led to the financial crisis of 2008. More specifically, the SBS is defined as the credit intermediation outside the traditional regulated banking system, involving all sources of funding apart from the, traditionally used, regular deposits. The spectacular rise of the SBS raises the important but so far under-researched question about whether and the extent to which the SBS is serving and furthering the real economy by providing adequate finance to investment and (high-tech) innovation, offering insurance and creating useful liquidity (i.e. the question about the SBS’s social efficiency). This is the main research question of this thesis project, which will use novel data sets for the OECD countries in order to empirically evaluate the social efficiency of the SBS. In particular, the contribution of the SBS to the growth of real GDP per capita and hourly labour productivity (which are two indicators of “social efficiency” often used in economic research) is assessed.","","en","master thesis","","","","","","","","","Technology, Policy and Management","Values Technology and Innovation","","","",""
"uuid:fa826ec7-f53f-409d-b4bb-d037e674be3e","http://resolver.tudelft.nl/uuid:fa826ec7-f53f-409d-b4bb-d037e674be3e","Taxi ride scheduling and pricing using historical data","Den Hoedt, M.","De Weerdt, M.M. (mentor)","2016","Taxis often bring people to airports and drive back to their area of operation without a passenger. This is considered as a gap in a taxi driver's schedule and could be filled up by giving customers extra incentive to book a taxi ride by asking a reduced price. A reduced price is justified when the costs of when the rides are combined is lower than when the rides are carried out separately. This thesis work describes two steps towards computing the cost of performing a taxi ride while taking into account the time and fuel needed for driving empty between rides. The first method responds to a request of a customer and composes offers that are related to the customer's taxi ride request from different companies, while taking the already booked rides into account. A good offer has both the price and the offset to the requested departure time minimized. The second method learns the cost for driving empty by taking into account the probability of taxi rides that will be booked in the future.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Software Technology","","Algorithmics","",""
"uuid:505300b0-f421-4470-b63e-cdb9154f7d54","http://resolver.tudelft.nl/uuid:505300b0-f421-4470-b63e-cdb9154f7d54","Performance Analysis of WebRTC-based Video Conferencing","Jansen, B.A.","Kuipers, F.A. (mentor)","2016","Video Conferencing has been rapidly gaining attention and is becoming a more popular way to communicate due to better internet connectivity and increasing computation power. Because of this increase in demand, the dying out of proprietary plugins like Adobe Flash, and the need for easy and accessible video conferencing, a new browser technology emerged: WebRTC. WebRTC allows internet browsers to perform a peer-to-peer video conference without the need to install additional plugins or applications. WebRTC is widely adopted for video conferencing solutions and getting more support across all browsers. WebRTC is easy to use by application developers, but unfortunately does not allow a developer to easily have insight into the video call characteristics made on the platform. This is because the way WebRTC handles network congestion is completely hidden away in the code of the browser. This work describes the media pipeline of WebRTC including all different network protocols used by WebRTC. To have a better understanding of how WebRTC handles network congestion, the Google Congestion Control (GCC) algorithm used by WebRTC is analyzed. Since WebRTC works over UDP which does not support congestion control like e.g. TCP does, a custom made congestion control is used. GCC changes the data rate based on packet loss and latency measured at respectively the sender and the receiver side during a WebRTC video conference. Based on the thorough WebRTC analysis including related work which some of WebRTC’s pitfalls, a test bed is set up to conduct a comprehensive performance analysis of WebRTC using the latest browsers, for different scenarios, while being subject to different network effects. The effects of additional latency, packet loss and limited bandwidth are tested. The effects of cross traffic, multi-party, WebRTC on mobile devices, different browsers and different video codecs are furthermore analyzed. This performance evaluation verifies that results shown in earlier research no longer hold due to the improved GCC algorithm. Results however do show that there is room for improvement since inter-protocol fairness is not ideal and WebRTC streams have a higher priority than competing TCP flows. It also shows the difference in WebRTC’s performance for different internet browsers and mobile devices. Also the newly added support for the video codecs VP9 and H.264 are analyzed which do not perform as expected and need to improve.","WebRTC; video conferencing; real time communications; performance analysis","en","master thesis","","","","","","","","2017-08-25","Electrical Engineering, Mathematics and Computer Science","Network Architectures and Services","","","",""
"uuid:e31a8e50-8f7c-4601-ad97-27a292092554","http://resolver.tudelft.nl/uuid:e31a8e50-8f7c-4601-ad97-27a292092554","Collaboration for innovation: Designing an ICT platform for matchmaking of companies in the field of Internet of Things","Frijns, M.","Bouwman, W.A.G.A. (mentor)","2016","","IoT; Matchmaking; Ecosystem; Multi-sided platform; ADR; Design","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","Management of Technology","",""
"uuid:3eaecc9f-bfe3-4ff8-a13c-87fc5c03d785","http://resolver.tudelft.nl/uuid:3eaecc9f-bfe3-4ff8-a13c-87fc5c03d785","City Government’s Pursuit of Circular Economy from an Industrial Park: Taiwan’s Largest Feasible Scope of Industrial Symbiosis in Reality","Gao, Y.","Korevaar, G. (mentor); Hu, M. (mentor)","2016","Circular economy is an economic model aiming at decoupling economic growth from the consumption of finite resources. It attracts attention to system changes at all levels, and Taoyuan City Government devotes itself to the circular economy development as well. Nevertheless, the Taoyuan environmental protection park, with the early ambition of developing an eco-industrial park, currently consists of only homogenous waste treatment enterprises. This research aims to explore the possible future of the park and the largest scope of symbiosis from the city government’s perspective. It also attempts to enhance the understanding of the relationship between circular economy and industrial symbiosis. Previous literature shows that no single, universal theory can be expected to predict the emergence of industrial symbiosis. Regional difference together with contextual factors play a huge role in the industrial symbiosis and circular economy development. Several interviews have been conducted with city government officials, business representatives from the park, public administrators, and the non-government advocate. This research adopts the integrated mixed-level analysis, comprising the SWOT analysis and the context analysis, to present the holistic case knowledge and to structure the key factors. The sway between a globalized neo-liberal economy and a localized circular economy is addressed. Mainly due to the small scope, the industry type, and the inherently weak geographical location, the conclusion is to maintain the achieved sustainability of the park and not to develop into an eco-industrial park. Considering the political and economic reality at the macro-global level, six other recommendations are formulated to facilitate a circular economy from the Taoyuan city government’s perspective. The key findings have been validated by all interviewees and are presented to the city government as policy advice.","Taiwan; industrial symbiosis; circular economy; industrial park; city government; context analysis","en","master thesis","","","","","","","","2017-08-25","Technology, Policy and Management","Engineering, Systems and Services","","","",""
"uuid:deb7f375-e5ae-4711-8b1f-41153444d5b5","http://resolver.tudelft.nl/uuid:deb7f375-e5ae-4711-8b1f-41153444d5b5","Semi-analytical solutions for buckling and post-buckling of composite plates: Application to stiffened Panels","Cabot Talens, E.","Kassapoglou, K. (mentor)","2016","This MSc thesis addresses the study of buckling and post-buckling of composite plates with elastic restraints at the edges and under any combination of in-plane loading, aiming to solve the plate response in stiffened panel structures. The implemented solutions are based on thin plate theory for mid-plane symmetric plates. The governing equations are solved using a semi-analytical formulation (not closed form) to combine advantages from analytical and numerical analysis. This approach allows to solve most of the typical laminates used in aerospace applications while allowing an improved performance when compared to FE models. The developed formulation relies on eigenbeam functions to approximate the plate behaviour for any combination of arbitrary elastic restrains, with a minimum number of degrees of freedom. This approach has proved to be able to reproduce the buckling mode and load for buckling and the out-of-plane displacement for post-buckling. The results obtained have been verified against FE commercial software package Abaqus and good to excellent agreement has been achieved using a fraction of the computational power. The relation between the ideal torsional springs and stiffeners’ restrain is approximated in order to apply the developed formulations to more practical problems involving stiffened panels. Preliminary verifications show the validity of the proposed approaches and encourages the further development of the solution to achieve a more powerful stiffened panel formulation. Moreover, the developed approaches can be extended to solve other relevant stability phenomena such as global buckling or stiffener crippling. This work is part of a Fokker Aerostructures project to develop an analytical framework for analysis and design of composite stiffened panels with post-buckling capabilities. This framework will facilitate the preliminary design of composite stiffened structures and allow further optimization without requiring the prohibitive computational cost and complexity of finite element models.","buckling; post-buckling; plates; semi-analytical; composite laminates; combined loading; rotationally-restrained; stiffened panels","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures and Computational Mechanics","","","",""
"uuid:e113c954-dfb0-4da0-8f96-e8bc45c75a37","http://resolver.tudelft.nl/uuid:e113c954-dfb0-4da0-8f96-e8bc45c75a37","Circulating fluidized bed of biomass gasification: Product compound prediction by application of a constrained equilibrium model","Wibisono, A.M.","Yakaboylu, Y. (mentor); Tsalidis, G.A. (mentor); de Jong, W. (mentor)","2016","Biomass as a renewable energy resource and a carbon neutral feedstock has a big potential to replace fossil fuel. Gasification as one of thermochemical process routes in biomass utilization draws attention with regards to its broad product range. The circulating fluidized bed with the advantages of higher and rapid conversion performs as the favorable type of reactor for the gasification process. Research of either model development or experimental study has been extensively carried out in characterizing the circulating fluidized bed biomass gasification to obtain an optimum set up. The two most commonly known classes of models for the circulating fluidized bed biomass gasification are equilibrium model and kinetic model. Despite its well-known advantage in providing the accuracy by the mechanistic approach, the model complexity should be paid off to compensate the proper result. Certain strategies are commonly elaborated in simplifying the model by introducing the empirical relation for certain conversion step or neglecting the less affecting phenomena of the model. Such simplification unavoidably limits the model flexibility into the certain application only. Regardless of its accuracy, the equilibrium modeling approach is famous with its flexibility application and is considered simpler than the kinetic one. Making use of the Gibbs free energy minimization as the thermodynamic equilibrium condition the model predicts the product compounds in response to the operating conditions. The global equilibrium consideration causes the poor accuracy of the model since there is a limitation in achieving such condition for the actual conversion process conditions. Introducing the experimentally based constraints is noticed in several published papers as an improvement in correcting the model global equilibrium condition for the better representation of the actual process. The improvement opens the possibility to upgrade the model accuracy. In this master thesis, the extent of the constrained equilibrium model is examined among developed models and experimental data by developing the most robust constraint cases in predicting the product compounds. Several methods were performed in determining such constraints in representing the actual process. The carbon conversion together with the CH4 amount from related gasification and devolatilization experiment were chosen to be the constraints for the major gasses prediction. Meanwhile, carbon conversion and 4 major gasses amount (H2, CH4, CO, CO2) were selected to correct the model global equilibrium in predicting the formed tar species. The selected constraint cases were introduced in the equilibrium model in performing the prediction for the case study on torrefied and non-torrefied circulating fluidized bed biomass gasification campaign 100 kWth test rig at TU Delft. The major gasses prediction among cases varies from the good agreement of H2/CO ratio and Lower Heating Value of the product gas to the roughly approximated 30% mole fraction average difference to the experimental data. The model predicted 13 species of tar from 25 considered species. From the 15 species analyzed in the experimental data, 10 predicted species are in qualitatively good agreement. The total tar and BTX tar prediction are in quantitative agreement with the experimental data.","fluidized; biomass; gasification; constraint; equilibrium; model; tar","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Process and Energy","","","",""
"uuid:3fcd92cf-c39c-4d21-83e8-09da20d61fd7","http://resolver.tudelft.nl/uuid:3fcd92cf-c39c-4d21-83e8-09da20d61fd7","The socio-hydrological situation of smallholder in Marathwada, Maharashtra state (India)","den Besten, N.I.","Pande, S. (mentor)","2016","India’s rural population accounts for sixty percent of the total, where agriculture provides the main source of income. This is no different for the mid-southern state Maharashtra. A state where the monsoon drives the climate, hence determines crop yields of abundant rain-fed farmers. The state also witnessed high suicide rates among farmers in the last decade, despite high growth of the Indian economy A socio-hydrological modelling is used as a tool to interpret the crisis. A recently developed smallholder socio-hydrological modelling framework is deployed that conceptualizes the system dynamics of a farmer. First a comparative assessment is done by applying the model to two adjoining divisions of Maharashtra: Maratwada and Desh. To obtain insight into the dominant factors behind the crisis in Marathwada, which witnessed higher farmer suicide rates than Desh. It reveals that the difference in farmer distress can be attributed to differences in soil characteristics, hydro-climatic variability and cropping pattern. The role of unsuitable cropping patterns in triggering farmer distress is then assessed historically (over a 30 year period of 1981-2011) simulating 3 types of smallholders: a food grain producer, a farmer who changed his cropping to pulses (after 1992), and a farmer who switches to producing cash crops (after 1992). These assessments are based on observed changes in cropping patterns in Marathwada and support the argument that marginal farmers switching to risky cash crops, that are not appropriate for the local conditions, may be behind elevated farmer distress. Finally, various on-farm socio-hydrological processes, that were inspired by farm surveys conducted in March and April 2016 in Parbhani, were improved in order to enhance the realism of the developed model for Marathwada. These included the effect of high intra-seasonal variability on crop yields and the on-farm characteristics. All led to improved correlation with observed suicide rates. Results demonstrate that socio-hydrological modelling provides an explanation behind regional differences in suicides rates. Cash crops increase the vulnerability of small farmers to crop failure, yet farmers take more risk hoping for high returns from selling cash crops. Weak credit and crop insurance systems are accelerating the distress amongst farmers in the region. In fact, the effect of droughts is accentuated by the choice of growing cash crops. In conclusion, finding on-farm water storage solutions are not sufficient to alleviate farmer distress. Policy intervention should focus on promotion of watershed development and financial incentives may be needed to motivate a change in cropping pattern or off-farmlabour alternatives.","","en","master thesis","","","","","","","","2016-08-21","Civil Engineering and Geosciences","Water Management","","Water Resources & Hydrology","",""
"uuid:8d5ead0a-e486-47ad-9735-44593307504d","http://resolver.tudelft.nl/uuid:8d5ead0a-e486-47ad-9735-44593307504d","Q: A communication platform establishing customer loyalty, using gratitude","Ton, R.","Schifferstein, H.N.J. (mentor); Bodenstaff, M. (mentor); Pasman, G.J. (mentor)","2016","Marqt is a supermarket located in the conurbation of the Netherlands. In fifteen stores they sell over three thousand products that fit their ideology; ‘making choices together with respect to our environment, the animals and our health’. Marqt enterprises the conventional grocery sector and tries to convey the above stated message by offering sustainable groceries. However, Marqt’s group of loyal customers is not sufficient enough to make the enterprise grow. The specific challenge of this thesis is therefore; How to make Marqt’s (loyal) customers more loyal in their attitude? As retail developed, our consumptive society developed as well. In fifty years doing groceries evolved from having direct contact with grocers and specialists to big supermarkets where people became disconnected with the grocer. Therefore, value and meaning of products in the sector got lost, whereby consumers in general strive to get the most goods for the least amount of money. Since media attention of food and what we eat increased, consumers strive to retrieve meaningfulness, authenticity and value a personal relationship with the retailer (Van Tongeren, 2013). This development results in a new shift in consumer demands and power where retailers have to react on. Reflecting loyalty in different disciplines showed that being loyal does not have anything to do with the stimulation of repetitive buying behavior, since features and prices are considered distant (Shedroff, 2010). Loyalty contains multiple psychological processes where connections are built that form a specific attitude towards someone or something (Chaudhuri & Holbrook, 2001). This attitude can only be built during meaningful experiences where active customer involvement turns attracted to attached customers. Before building meaningful experiences, values were found during an extensive customer analysis in the form of interviews with the customers of Marqt. These interviews generated empathy with and insights in the customers. The research showed that Marqt deals with four different types of customers. Four each customer role, several customers values were determined that shape the customers’ grocery buying behavior. With regard to the context, Marqt’s loyal customers addresses every customer role. Therefore the ideal vision includes all customer roles respecting all values. A pro-active design approach was set explicitly, in order to increase active customer involvement wherefrom customer attitudinal loyalty evolves. The interaction of the design is established through gratitude. Using the analogy of a soccer team thanking the audience after a match. Gratitude combines a behavior and emotion and is about expressing appreciation and acknowledgement that can only arise between two (living) things. Gratitude correlates to loyalty, because loyalty is mutual too and occurs between two parties. Multiple iterations were done in order to converge into one specific design; an open-platform ‘Q’. This platform connects the customers of Marqt with the partners that produce the products Marqt sells by sending messages; by letting the customers express their gratitude for the products. The product is, in this sense, the connecting factor. The content of the message could contain product reviews or direct questions to the partner. The platform exists of a customer mobile app and a partner website where both parties communicate with each other facilitated by Marqt. Customer’s attitudinal loyalty is increased by introducing the customers of Marqt to the backstories of the products of Marqt’s assortment. The stories are valuable and show what Marqt tries to convey in their vision. Enabling a connection with the person behind a product elicits a form of recognition and acknowledgement of the people behind the product and makes the backstory linked to a living person. Moreover, Marqt is planning to introduce a new brand strategy that focuses on making choices together. Part of the strategy is to refund Marqt’s upper three percent of the excessive profit to its customers. The open-platform includes a section where Marqt and the partners launch a blog and start the conversation with the customers. The more members actively participate in the platform, the more these members are rewarded. Evaluative interviews with different stakeholders of the concept complemented that customers using the platform are likely to create a better attitudinal connection towards Marqt. However, the potential could only be estimated, since loyalty can only be measured over a longer period of time. Moreover, the platform was considered idealistic, evoking a beautiful gesture. The essence of the design fits Marqt’s future plans to implement for of the new brand strategy. However, Marqt’s assortment is not yet transparent enough to trace the origin of every product, so customers cannot connect with every partner of Marqt. Suggestions for further development and improvement for design arose as well. Customers stressed the importance of functionality in the platform. The connection with the partner considered welcome, however more functionality in a supermarket-app is preferred; searching for recipes and ordering groceries with the app are mentioned as desirable additions to an app for Marqt. Last important suggestion for further research is how to prevent; that customers get bored over time. Creating loyalty at customers is one study, remaining loyal is something different; increasing the importance of the opinion of a loyal customer, could be a means by which boredom could be prevented.","design; customer loyalty; attitude; gratitude; communication platform","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design for Interaction","","","",""
"uuid:4cc46b9e-9552-442f-9628-1706250dcd7b","http://resolver.tudelft.nl/uuid:4cc46b9e-9552-442f-9628-1706250dcd7b","Strengthening the integrality gap for the capacitated facility location problem with LP-based rounding algorithms","De Koster, P.B.J.","Aardal, K.I. (mentor)","2016","This thesis studies the capacitated facility location problem, in which all clients have unit demand and all facilities have integral capacity. A linear relaxation is researched, with corresponding integrality gap bounded by a constant. Recently, such a linear relaxation has been found and proven using an LP-bounding algorithm. The formulation of the relaxation and the proof were very complex and intuitively hard to understand, however. Therefore, this thesis provides a simpler, more formulation and proof. This thesis has two main contributions. First, a structured overview of all the theory prior to the construction of the relaxation is provided. To do so, the minimum knapsack problem is treated, which is a simplied version of the capacitated facility location problem. An LP-based rounding algorithm is presented to illustrate general ow-network techniques for facility location problems. Second, the rounding algorithm for the capacitated facility location problem is illustrated and explained more accessible to readers less familiar with LP-based rounding algorithms. The existing rounding algorithm for the capacitated facility location problem is treated, illustrated and extended with Matlab code. The rounding algorithm proves an integral solution for the capacitated facility location can be constructed from the linear optimal solution, with cost no more than 288 times the cost of the fractional optimal solution. This proves that the integrality gap of the proposed relaxation is bounded by 288.","capacitate facility location problem; Knapsack problem; rounding algortihm; LP; rounding algortihm; integrality gap","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","","",""
"uuid:7e5a1ab2-28b1-44ef-9522-119e8ee8613e","http://resolver.tudelft.nl/uuid:7e5a1ab2-28b1-44ef-9522-119e8ee8613e","Has Finance Grown Too Big?: On The Social Efficiency of the Shadow Banking System","Karagiannis, V.","Storm, S.T.H. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Values Technology and Innovation","","","",""
"uuid:c1becb62-7c11-4c4d-82a7-c12c5d23a4b5","http://resolver.tudelft.nl/uuid:c1becb62-7c11-4c4d-82a7-c12c5d23a4b5","A viral spreading model in signed networks","Li, B.","Wang, H. (mentor)","2016","In a business network, a company may be encouraged to utilise a new technique when either its cooperators or its competitors have made their adoption. Nevertheless, its willingness can be reduced as more of its competitors start applying the new technique. Inspired by the this scenario, in this thesis we proposed a viral spreading model in signed networks, in which each link is allocated with a positive or negative sign, representing, e.g. the cooperative or competitive relation between two companies. We introduce a dynamic infection rate to capture the influence of negative links into viral spreadings and explore the performance of our proposed model with respect to the following influential factors: the relative infection rate of negative links with respect to positive links (relative negative-link infection rate), the correlation between positive and negative degrees, and the degree distributions of the positive and negative links respectively. To provide analytical explanations, we develop an Individual-Based Mean Field Approximation (IBMFA) method. We show that IBMFA is a feasible theoretical tool that can well approximate the observations in Monte-Carol simulations. Our results show that, contrary to our intuition, when positive degree and negative degree of nodes are less correlated in scale-free networks, a larger relative negative-link infection rate does not always results in more nodes being infected. In addition, compared to networks with only positive links, viral propagation via negative links can lead to a higher fraction of infected nodes at small infection rate; whereas the overall spreading starts being suppressed in signed networks as the infection rate becomes larger than a certain value. We find that this certain infection rate, at which signed and unsigned networks share the same fraction of infected nodes, is approximately linearly correlated to the relative negative-link infection rate. Corresponding to the scenario mentioned at the beginning, our findings indicate that: (1) Adopting a new technique at a high rate from competitors does not always lead to a larger percentage of adoption among companies; (2) Compared to networks with only cooperative relations, the competitive relations between companies may sometimes facilitate a higher percentage of adoption, especially when every company accepts a new technique at a low rate.","viral spreading model; signed network; Individual-Based Mean Field Approximation (IBMFA)","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Intelligent Systems","","Multimedia Computing","",""
"uuid:c4bf246e-3932-4d4c-bc2a-8e7a9b7fd708","http://resolver.tudelft.nl/uuid:c4bf246e-3932-4d4c-bc2a-8e7a9b7fd708","Can ICT Contribute to Achieve Independent Living?: Exploring Capabilities of the Health and Wellbeing Platform","Agahari, W.","Enserink, B. (mentor); de Reuver, M. (mentor); Kroesen, M. (mentor); Keijzer-Broers, W. (mentor)","2016","As the world population is getting older, healthcare expenditure in many countries are increased due to the fact that more people become vulnerable to various health problems. Because of this, elderly people are encouraged to live independently in their own home as long as possible. Promoting independence to elderly people will not be possible without support from society, including voluntary caretakers who took care of elderly people. Hence, it is believed that the health and wellbeing platform as an example of Information and Communication Technologies (ICT) could be beneficial to achieving independent living. Nevertheless, the potential impact of such a platform is relatively unknown since this type of platform is still lacking in the market. In addition, availability of such a platform does not guarantee that elderly people will gain any value from it. Therefore, this research aimed to apply the concept of the capability approach to examine why and how ICT, in particular the health and wellbeing platform, can contribute to achieve independent living of elderly people. To do so, we conducted a case study on a health and wellbeing platform in the Netherlands through secondary data analysis and interviews with potential end-users. Our findings showed that such a platform could contribute in achieveing independent living by enabling certain capabilities (find products & services, find activities, manage daily activities, stay connected with others, monitor conditions, arrange help for others) for either elderly people or voluntary caretakers. Enablements of these capabilities are influenced by variety of conversion factors, namely individual characteristics (age, health condition, technological knowledge), individual perception (perceived ease of use, expected benefits, need for technology, satisfaction level), and social contexts (recommendation from closest people & healthcare stakeholders). Our findings provide insights on how this platform could become an intervention tool to support government policies in encouraging elderly people to live independently at home. Moreover, our study also adds another perspective on how to operationalize the capability approach in the comprehensive view of elderly, ICT, and healthcare.","Independent Living; Information and Communication Technology; Capability Approach; Smart Living; Elderly; Healthcare","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","Engineering and Policy Analysis","",""
"uuid:8ad32819-2508-4b94-a9a7-245292184099","http://resolver.tudelft.nl/uuid:8ad32819-2508-4b94-a9a7-245292184099","Steering Algorithm of Single Gimbal Control Moment Gyroscopes: A Practical Application for Agile Spacecraft","Schallig, S.A.V.","Chu, Q.P. (mentor)","2016","Future space platforms, such as next-generation Earth imaging and missile-tracking satellites, require rapid rotational maneuverability. Currently, most European satellites use reaction wheels as momentum exchange device for their Attitude Control System. However, reaction wheels do not produce enough torque to meet the high slew rate requirement of agile satellites. An alternative is to use Control Moment Gyroscopes, because of their torque amplifying properties. This thesis presents a new maximum null motion algorithm for single gimbal control moment gyroscopes. This algorithm limits the amount of null motion to the gimbal rate limit and switches direction of the null motion if the limit is reached. Numerical simulations show that using the complete equations of motion, gimbal dynamics, gimbal rate limit, and including system and sensor noise, the maximum null motion algorithm is better in generating a constant torque command than the traditional nondirectional null motion algorithm. This is especially relevant for spacecraft during station keeping tasks under a constant disturbance torque. However, no improvement was observed between the nondirectional and maximum null motion algorithm for a reorientation maneuver","Single Gimbal Control Moment Gyroscopes; Spacecraft attitude control; Singularity avoidance","en","master thesis","","","","","","","","2021-08-25","Aerospace Engineering","Control & Operations","","Control & Simulation","",""
"uuid:12d5bae4-b061-4acf-a8a4-3a06cd3ce33f","http://resolver.tudelft.nl/uuid:12d5bae4-b061-4acf-a8a4-3a06cd3ce33f","Human operator identification in the LPV system framework","Duarte, R.F.M.","Pool, D.M. (mentor)","2016","Implementation of predictor-based subspace methodology for human operator identification in the LPV system framework.","","en","master thesis","","","","","","","","2021-08-25","Aerospace Engineering","Control and Operations","","Control and Simulation","",""
"uuid:e1ff3fe9-d32b-4335-aa5c-b080b24bcdb0","http://resolver.tudelft.nl/uuid:e1ff3fe9-d32b-4335-aa5c-b080b24bcdb0","On the Rain-Wind Induced Vibrations of a Mass-Spring System","Kole, Sjors (TU Delft Electrical Engineering, Mathematics and Computer Science)","van Horssen, Wim (mentor); van Elderen, Emiel (graduation committee); Vermolen, Fred (graduation committee); Delft University of Technology (degree granting institution)","2016","In this report, the rain-wind induced vibrations of cables are studied. This is done by modeling the cable cross-section as a mass-spring system with two time-varying masses. Thereafter, the solution of this model is approximated using a multiple timescale perturbation method. Lastly, for some choices<br/>of the time-varying masses the eigenfrequencies are analyzed, stability properties are derived, and approximations of the solutions are given.","Rain-Wind Induced Vibrations; Differential Equations; Perturbation Analysis; Mass-Spring System","en","bachelor thesis","","","","","","","","","","","","","",""
"uuid:b0eea7c0-b4a7-47c0-8fcf-af597bc9ba3e","http://resolver.tudelft.nl/uuid:b0eea7c0-b4a7-47c0-8fcf-af597bc9ba3e","Social learning for sustainable food: Recommendations to facilitate social learning in practice within the governmental programme Knowledge and Education Deal Food","Voorwinden, E.","Wehrmann, C. (mentor); van der Sanden, M.C.A. (mentor); Smulders, F.E.H.M. (mentor)","2016","","","en","master thesis","","","","","","","","","Applied Sciences","Science Communication & Education","","Science Communication Track","",""
"uuid:5148aafa-91ed-485d-ac7a-2883394ec0a4","http://resolver.tudelft.nl/uuid:5148aafa-91ed-485d-ac7a-2883394ec0a4","Hybrid Electric Propulsion Systems: Integrated performance analysis applied on short-range aircraft","Ang, A.W.X.","Gangoli Rao, A. (mentor)","2016","The growing level of environmental implications of aviation and a significant increase in air traffic have been driving technological advancements in the aerospace industry. The rising developments in battery technology allowed the automotive industry to build hybrid and fully electric cars. However, the limited power-to-weight ratio components impede the development of fully electric aircrafts. In this regard, Hybrid Electric Propulsion Systems (HEPS) are more viable. With HEPS, the aero-engine is combined with an electric system. This brings several technological challenges, such as the increased complexity in the performance and sizing methodology of the propulsion system and the necessary offset of the weight increment. Because of these issues, the different state of the art HEPS concepts largely depend on the development of new technologies, as well as maintaining efficient power management during the different phases of the flight envelope. The power management system adjusts the supplied power ratio between the energy sources during operation, as power can come from fuel and/or electrical energy. The objective of this study is to analyse the power management of HEPS, focusing on passenger transport aircraft. The application of HEPS in the mid/long-term is heavily dependent on the technology maturity level of electric components. To examine the performance of HEPS, a simulation model is developed after determining the power requirements of a flight mission. The effects of different power management control strategies are analysed by combining different take-off and climb power splits. The power management control strategy with a take-off and climb power split of approximately 25% and 14% respectively, while scaling down the engine to 90%, is the most optimal and feasible strategy. This will reduce the fuel burn by 7.5% and total energy consumption by 2%.","power management strategy; performance sizing; Hybrid Electric Propulsion System; HEPS","en","master thesis","","","","","","","","","Aerospace Engineering","Flight Performance and Propulsion","","","",""
"uuid:58a4c285-e3b6-4bf0-b885-2908077e9b02","http://resolver.tudelft.nl/uuid:58a4c285-e3b6-4bf0-b885-2908077e9b02","Tracking and Following a Moving Person Onboard a Small Pocket Drone","Duro, T.","De Croon, G. (mentor); De Wagter, C (mentor); Meertens, R. (mentor)","2016","This paper presents a vision based strategy, designed to work fully onboard a small pocket drone, for autonomously tracking and following a person. Flying a drone is not an easy task, usually requiring a trained pilot, with the presented system it is possible to use a drone for filming or taking pictures from previously inaccessible places without the need for a person controlling the aircraft. Such framework is comprised by two main components, a tracker and a control system. The tracker has the function of estimating the position of the person that is being followed, while the control system gets the drone near that person. Limited by payload weight, power consumption and processing power the system results in a delicate balance between these constraints. The main contributions of this paper are the comparison between two state-of-the-art visual trackers running on paparazzi, Struck and KCF, as well as the control system that uses the tracker’s output location to perform the person following task. Then a new tracker is developed to be as computationally light as possible so that it can run onboard a small pocket drone, based on HOG feature extraction, it uses logistic regression to train a detector on the appearance of a person.","","en","master thesis","","","","","","","","","Aerospace Engineering","","","","",""
"uuid:2afcede4-f818-4f5b-93e1-c2864ecafef3","http://resolver.tudelft.nl/uuid:2afcede4-f818-4f5b-93e1-c2864ecafef3","A design-oriented numerical investigation seismic soil-structure interaction","Toma, R.F.","Pisano, F. (mentor)","2016","Presented here is the numerical investigation on seismic soil-structure interaction applying features of a rather non-conventional earthquake design. The examination focuses on the Groningen site conditions which lately experienced an induced-type of earthquake. A unique situation calls for a suitable seismic design for both existing and new buildings. Instead of adopting the conventional capacity design, the current paper explores a novel dynamic approach which gains traction within the research community. For validation and veri?cation purposes, the thesis invokes the scientifc literature findings of situations which resemble to the Groningen circumstances.","clay; non-linear analysis; seismic; Groningen; earthquake; soil","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Enginering","","","",""
"uuid:5ddf84e7-5142-45b5-8e63-a9679e596487","http://resolver.tudelft.nl/uuid:5ddf84e7-5142-45b5-8e63-a9679e596487","A road map for constructive collaboration among stakeholders: An improvement to stakeholder participation in virtual design and construction engineering design projects","van Schie, X.D.","van Nederveen, G.A. (mentor); Enserink, B. (mentor); Bakker, H.L.M. (mentor)","2016","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Construction Management and Engineering","",""
"uuid:06af584d-c090-4300-83b8-61c43173d763","http://resolver.tudelft.nl/uuid:06af584d-c090-4300-83b8-61c43173d763","Visualizing the DJ performance","Groenendaal, P.H.T.","Wijntjes, M.W.A. (mentor)","2016","Electronic dance music events are gatherings where people come to dance and amuse themselves while enjoying an audiovisual show. To attract larger audiences, the stages at these events have become larger in size and in number of show elements. This increases the number of people who can attend the event but at the same time it decreases the size of the artist, the DJ, in relation to the stage. Moreover, in comparison to other performance artists such as rock bands, DJs have the disadvantage of being stuck behind a booth. The booth creates a barrier that prevents them from interacting with the audience during operation of their instruments. This project is set out to increase the interaction between the DJ and the audience and the aim of the outcome is to communicate the actions of the DJ to the audience. The actions of the DJ on the audio players are converted into visual effects on the stage, which magnifies the actions to become visible for the audience. This increases the impact of any activity behind the booth through which the artist regains a more prominent role in the show. Visualizing the actions of the DJ creates insight in the level of activeness and provides a component that can be spectated, engaging the crowd in the process. This has significant implications for DJs and events as the essence of entertainment is to retain the attention of an audience. To realize this communication to the audience a software application reads the live status of the mixer and players that the DJ is operating. The application translates the input signals and sends triggers over a network to the systems of the VJ and lighting designer in order to trigger and generate visual effects. In this way, the DJ gains control over visual elements in the show by interacting with the decks, allowing for communication with the audience without obstruction during deejaying. In advance of the show, the VJ and lighting designer can assign triggers to specific actions of the DJ. The application is designed to fit in the workflow of the crew and as a result they can use their own systems to design and setup visual effects to accompany the actions of the DJ. With the application the crew can monitor exactly what the DJ is doing live. The information can be recorded and replayed after the show to evaluate how the live visual effects affected the atmosphere among the audience. This feature enables a swift and iterative approach in designing and road-testing the effects. A vital factor for the concept to work is the balance between the DJ- and VJ-triggered effects. Synchrony in the audiovisual experience is key in order to stimulate, and not disturb, the senses of the festivalgoer. To prevent becoming an obtrusion the DJ-triggered effects should not only be in sync with rhythm of the music but also aesthetically consistent with the VJ-triggered visuals. The DJ-triggered effects however do need to be discernable for the audience in order to perceive the link between the effect and the action of the DJ. Therefore, finding the right balance is key for the concept to work. Three versions of video clips were created to test how the DJ-triggered effects influence the engagement of festivalgoers. The clips display a DJ surrounded by visuals to simulate the stage of a festival. The results of the test show a decrease in engagement of the festivalgoers with the video clips that do contain DJ-triggered effects. Participants also thought the control version fit the performance of the DJ better. This iteration of visual effects has thus not been successful at reaching the goal of increasing engagement of the audience. However, the concept does not have to be discarded yet. The aesthetic differences between the two visual layers, the VJ- and DJ-triggered visuals, appear to be too different to be perceived as a whole. Iterating with visual styles that are more coherent might lead to a better balance between the two visual layers and eventually to more satisfying results. Finally, a redesign has been made that suits these observations.","entertainment; interaction design; electronic dance music","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","","",""
"uuid:53aadf80-2d52-4b31-9628-6643e39cf380","http://resolver.tudelft.nl/uuid:53aadf80-2d52-4b31-9628-6643e39cf380","Exploring the Value Enabled by Big Open and Linked Data in Smart Cities: A foresight study for the smart cities Rotterdam and The Hague","Nenciu, G.","van der Duin, P.A. (mentor); Janssen, M.F.W.H.A. (mentor); Hulstijn, J. (mentor); Snijders, D. (mentor)","2016","Despite the high expectations regarding the potential value that Big Open and Linked Data (BOLD) can enable in smart cities, the research on the topic is scarce. The present study responds to this need by conducting a foresight study with core interest the long-term impact of BOLD on smart cities. Moreover, the research develops a model for characterizing smart cities based on two defining elements: data governance (top-down or bottom-up) and openness of data (open or closed). The foresight study has an outside-in perspective, starting from the case of Rotterdam and The Hague as smart cities. Societal developments were identified by conducting nine interviews with smart city stakeholders from public sector, business, and academia. The trends served as input for a scenario development workshop, which was organized in collaboration with The Netherlands Study Centre for Technological Trends (STT). Four future data- driven smart city scenarios emerged from the workshop, and they were used for two purposes: (1) to develop smart city types, namely See-through City, Data Masters, Grass-roots Town, and Data Bazaar; and (2) to explore the potential value enabled by BOLD in smart cities, by applying a developed value network model. The research offers practical recommendations for policy-makers involved in urban data and smart city projects in regard with interoperability of data systems in a city, development of public-private partnerships, creation of a long- term BOLD strategy, and others. Next to that, the study has the following scientific relevance: (1) development of a smart city model; (2) development of scenarios for data-driven smart cities; (3) exploration of BOLD enabled value in smart cities; (4) overview of the smart cities Rotterdam and The Hague; and (5) a new role in smart city networks – the catalyser role.","big data; open data; smart city; value network; value creation; foresight; future scenarios","en","master thesis","","","","","","","","","Technology, Policy and Management","Values Technology and Innovation","","Management of Technology","",""
"uuid:5de1751d-8ab3-4eec-9ade-c506f6d3e4d0","http://resolver.tudelft.nl/uuid:5de1751d-8ab3-4eec-9ade-c506f6d3e4d0","Implementation of XBRL Based Reporting System: Developing a framework for the process of XBRL adoption and implementation by using the case study in the Netherlands and in Indonesia","Sulastri, R.","Janssen, M.F.W.H.A. (mentor); Ubacht, J. (mentor); Rook, L. (mentor)","2016","Numerous studies on XBRL (eXtensible Business Reporting Language) have been conducted in the past two decades. However, the research on the dynamic issues during XBRL adoption and implementation stages are still left unaddressed. The aim of this study is to develop a framework of factors that influence the process of XBRL adoption and implementation by conducting case studies in Indonesia and in the Netherlands. The Netherlands' effort to implement XBRL based reporting system has been recognized as one of the first nation-wide implementation and has been awarded as the European best practice. On the other hand, XBRL implementations in Indonesia are conducted independently by different institutions to address reporting issues in their respective authorities. This study adopts the Technical, Organizational, and Environmental (TOE) model of IT adoption to classify the factors influencing XBRL adoption and implementation. The framework developed in this research is beneficial for further XBRL study as part of the socio-technical aspects of inter-organizational system implementation; it also can be used by XBRL practitioners as a guideline in designing XBRL implementation approach. Room for further research of qualitative and quantitative studies to validate the generalizability of the developed framework is also delineated.","inter-organizational information system sharing; XBRL; TOE; business reporting platform","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering Systems and Services","","Management of Technology","",""
"uuid:e76137e9-325d-4197-abf3-fc51d2c67a39","http://resolver.tudelft.nl/uuid:e76137e9-325d-4197-abf3-fc51d2c67a39","Developing Inland Logistics Hub in North Sulawesi Province to Reduce Transport Cost to/from Bitung Port","Fahmiasari, H., H.","Tavasszy, L.A. (mentor); van Dorsser, J.C.M. (mentor); van Ham, J.C. (mentor); Burgess, A. (mentor)","2016","Indonesia has planned to develop 24 main ports to support inter-island maritime highway concept. It has envisioned to balancing the trading activity between the eastern and western part of Indonesia. Nevertheless, trade problem in Indonesia also is caused by inefficient hinterland transportation to/from the port, thus increase transport cost. Bitung Port was the focus of this study by its international hub port role in the future. It also experiences the high transportation cost in the hinterland. This research focused on developing inland logistics hub in North Sulawesi Province, the hinterland of Bitung Port. Discrete cost simulation method was used to analyze the location and cost reduction after logistics hub applied. It was intended to overview the highest transport cost saving from each node when it was implemented to be logistics hub. There are 15 candidates, which represent the district in North Sulawesi Province. The result indicated that Airmadidi was the most optimum location for a logistics hub. It can save € 20/ton from other regional nodes. Social Cost Benefit Analysis was used to assess the economic feasibility of the project. This project is considered as an economically feasible by result IRR of 9%, BCR of 2.3, and NPV of € 38,230,669.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","Transport, Infrastructure & Logistics","",""
"uuid:a93653f7-584e-4d0b-ba4f-661d89f2101d","http://resolver.tudelft.nl/uuid:a93653f7-584e-4d0b-ba4f-661d89f2101d","De afstand tussen ongewortelde fylogenetische bomen","van Aken, M.","van Iersel, L.J.J. (mentor)","2016","Een ongewortelde, binaire, fylogenetische boom is een boom waarvan de bladeren gelabeld zijn en iedere niet-gelabelde knoop graad 3 heeft. Fylogenetische bomen worden gebruikt om de ontstaansgeschiedenis van organismen te begrijpen. Twee fylogenetische bomen T1 en T2 met dezelfde gelabelde blad-verzameling L kunnen namelijk vergeleken worden. Kan het ene organisme ontstaan zijn uit het andere organisme? Zo ja, hoe groot is de vergelijkenis? Door middel van zogenoemde Subtree Transfer-operaties kan T1 omgevormd worden tot T2. De twee belangrijkste Subtree Transfer-operaties in dit verslag zijn SPR-operaties en TBRoperaties. Bij beide operaties wordt er eerst een tak verwijderd van de boom. Bij een SPR-operatie wordt een tak geknipt en aan een andere tak vastgemaakt, terwijl bij een TBR-operatie een tak volledig verwijderd wordt, waarna een nieuwe tak de verbinding zal zijn tussen de ontstane deelbomen. Hoe vaak een operatie moet worden toegepast om T1 om te vormen tot T2 heet de afstand tussen de bomen. De probleemstelling van dit verslag luidt: hoe kan de TBR-afstand tussen twee fylogenetische bomen bepaald worden? Het blijkt dat de exacte afstand tussen T1 en T2 verbonden is met het aantal componenten in een Maximum Agreement Forest van T1 en T2 [1]. EenMAF kan omschreven worden als het minimale aantal bomen, zó dat iedere boom een deelboom van T1 en T2 is; de bomen disjunct zijn; en deze bomen heelL opspannen. Aangezien voor de probleemstelling een MAF gevonden moet worden tussen twee fylogenetische bomen, wordt in hoofdstuk 2 een handig uitstapje gemaakt. Wanneer twee bomen topologisch verwant zijn, kunnen ze gereduceerd worden, zonder dat de TBR-afstand verandert [1]. In hoofdstuk 3 wordt laten zien dat bij gewortelde, binaire bomen gekeken wordt naar onverenigbare trippels om een agreement forest te vinden. Twee gewortelde, binaire bomen zijn namelijk equivalent dan en slechts dan als iedere trippel in de twee bomen topologisch equivalent is [2]. Bij twee fylogenetische bomen T1 en T2 wordt er gekeken naar onverenigbare kwartetten. In hoofdstuk 4 wordt een algoritme beschreven die de TBR-afstand berekent [3]. Deze berekening bestaat uit twee fasen. In de eerste fase worden van bepaalde onverenigbare kwartetten in T2 een tak verwijderd, zodat er een bos F0 overblijft. In de tweede fase wordt gekeken naar paden die disjunct zijn in F0, maar overlappen in T2. Weer worden er takken verwijderd, zodat na de tweede fase een agreement forest overblijft. Tot slot wordt in het laatste hoofdstuk de theorie van hoofdstuk 3 en 4 gecombineerd. Deze combinatie resulteert in een nieuwe theorie. Dit geeft een ILP-formulering waarin de grootte van een MAF tussen twee fylogenetische bomen gevonden kan worden. Er is in dit verslag gekozen om alleen de belangrijkste lemma’s en stellingen te bewijzen. De kern van dit verslag ligt in de laatste drie hoofdstukken, dus daar zullen meer bewijzen te vinden zijn dan in de eerste twee.","","nl","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","","",""
"uuid:919fdb70-55a1-4be9-8795-fb7ef87d3382","http://resolver.tudelft.nl/uuid:919fdb70-55a1-4be9-8795-fb7ef87d3382","Design of a composite guitar","Roest, M.G.","Bergsma, O.K. (mentor)","2016","Today’s acoustic guitars are getting increasingly expensive due to the worsening availability of the highest quality woods. Composites show great promise in replacing wood in acoustic guitars as they are lightweight, are not as sensitive to environmental effects and are much stronger. Current composite guitars however do not sound as good as their wooden counterparts. Therefore this thesis research has been set-up to create a composite material that can match wood acoustically and therefore have all the benefits in terms of environmental sensitivity and strength while not compromising the sound quality of the instrument. A new composite is developed and extensively tested. This composite consists of a carbon fibre reinforced polyurethane foam and has a comparable acoustic response to high quality spruce used for guitar soundboards. With this new composite a complete composite guitar is designed, manufactured and tested. The psychoacoustic analysis performed showed that the new composite guitar is considerably more wood like in its sound compared to current carbon fibre acoustic guitars.","Composite; Guitar; Acoutics; Carbon fibre; Laser Scanning Vibrometry (LSV)","en","master thesis","","","","","","","","","Aerospace Engineering","Aircraft Structures and Materials","","","",""
"uuid:5669b269-cd33-4d63-b95a-990e64371054","http://resolver.tudelft.nl/uuid:5669b269-cd33-4d63-b95a-990e64371054","The opportunities for establishment of logistics clusters in Indonesia","Martalia, L.","Tavasszy, L.A. (mentor); Van Ham, J.C. (mentor); Verburg, R.M. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering Systems and Services","","Management of Technology","",""
"uuid:9e95d11f-35ba-4a12-8b34-d137c0a4261d","http://resolver.tudelft.nl/uuid:9e95d11f-35ba-4a12-8b34-d137c0a4261d","ETA prediction for containerships at the Port of Rotterdam using Machine Learning Techniques","Parolas, I.","van Duin, J.H.R. (mentor)","2016","The hinterland transportation of incoming containers at container terminals is a complex problem, due to the various actors involved and their often conflicting interests. A promising solution towards the problem for hinterland network operators is that of synchromodality, a concept that refers to on-line network planning for hinterland transportation. However, a hindrance to the efficient planning and execution of hinterland transportation is that there is currently no accurate way of predicting the estimated time of arrivals (ETA) for containerships that are reaching container terminals. This results in huge uncertainty over the types and amounts of cargo that reach the terminals, which in turn hinders the fast and cost efficient distribution of the products to inland destinations through trucks, trains or barges. The current paper will propose a machine learning approach for predicting the ETA of containerships heading towards the Port of Rotterdam, by combining position data from GPS signals with weather predictions. It was found that significant improvement for the ETA predictions, compared to the current situation could be achieved, especially for the cases of the vessels that are more than 60 hours away from the port. Furthermore, the weather interpretation was not of significant importance for estimating the time of vessel arrivals at the port. The value of such an information tool for the various stakeholders involved was also investigated. The interested parties, for which the importance of ETA predictions of sea vessels was assessed are : terminal operators (European Container Terminals in the case at hand), hinterland transportation companies (e.g. European Gateway Services ), the Port of Rotterdam, carriers and importers.","machine learning; neural networks; support vector machines; estimated time of arrival; container transport; hinterland transportation","; en","master thesis","","","","","","","","","Technology, Policy and Management","Management of Technology","","","",""
"uuid:dcb5ae46-f534-450a-b34a-4c6d756f61e2","http://resolver.tudelft.nl/uuid:dcb5ae46-f534-450a-b34a-4c6d756f61e2","The pricing of financial options using stochastic collocation and local volatility: Het prijzen van financiële opties met gebruik van stochastische collocatie en lokale volatiliteit","Tijink, J.J.","Oosterlee, C.W. (mentor)","2016","","","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","","",""
"uuid:44cb036b-51b8-4b1e-ba1e-a7d59936caa1","http://resolver.tudelft.nl/uuid:44cb036b-51b8-4b1e-ba1e-a7d59936caa1","Multi-Stage Formation Flight Planning","Doole, M.M.","Visser, H.G. (mentor)","2016","This thesis investigates the concept of fuel-reduction via formation flight of long-haul commercial aircraft on a network-wide scale, particularly addressing the issue of combinatorial complexity in global formation routing and allocation. A multi-stage formation flight planning (MS-FFLIP) tool is proposed as a computationally tractable approach to formation allocation. Assuming all trailing aircraft receive a 10% reduction in the fuel-burn by flying in formation, the optimal routes including the joining and splitting points are computed in four specific stages along the flight depending on the desired formation size. Within a stage, the interconnected allocation problem takes a set of flights, their possible formation combinations and associated costs and optimally assigns them in a fuel-cost minimising formation fleet. The formation flight routing and the associated fuel burn are enumerated rapidly. This fast computation allows the large-scale allocation problem to be solved via a Mixed Integer Linear Program and optimised through an external optimiser solver. Within the context of this thesis, a stage is defined as a collection of events where all flights merge into formation at optimal joining points, driven by an event-based simulation. The MS-FFLIP tool sequentially creates stages until the largest allowable formation size is created. Additionally, the model is capable of handling large sets of flights and in creating large formations of up to sixteen-aircraft allocated to a formation within a real-time operational time limit. Similar models exist in the current literature, however they are highly prone to combinatorial complexity and as a result, their computation times diverge dramatically with increase in aircraft number and formation size. For this reason, the models were not able to capture the majority of the potential fuel-savings. The MS-FFLIP model was compared against the conventional single-stage centralised formation flight planning approach for which 50 eastbound transatlantic flights were evaluated for optimal routes and assigned to formation fleet of up to four-aircraft per formation. The experiment for the centralised formation flight planning approach showed a 5.72% reduction in fuel-burn against its corresponding solo flights, while requiring 58 CPU minutes of computation time. On the contrary, the MS-FFLIP model performed the same experiment within a time frame of 14 seconds while demonstrating potential fuel savings of 5.66%. Moreover, a case study for 267 transatlantic flights was simulated by the MS-FFLIP tool. The results revealed a 6.88% in potential formation fuel savings against solo flights, with up to sixteen-aircraft allocated to a formation, while performing the computation within 9 CPU minutes. The developed model also showed significant fuel saving potentialcompared to local optimisation-based cooperative planning frameworks.","Formation Flight; Centralised Formation Flight Planning; Flight Operations Optimisation; Airline Fuel Saving Strategy; Combinatorial Complexity; Combinatorial Explosions; NP-hard","en","master thesis","","","","","","","","","Aerospace Engineering","Air Transport and Operations","","","",""
"uuid:e20c58d8-b5c0-44f9-97d2-3eb5d7ac9187","http://resolver.tudelft.nl/uuid:e20c58d8-b5c0-44f9-97d2-3eb5d7ac9187","Applying streamlined LCA to assess the environmental impact of the hotel branch","Bruinsma, M.C.","Guinée, J.B. (mentor); Korevaar, G. (mentor)","2016","The Master’s programme Industrial Ecology is jointly organised by Leiden University and Delft University of Technology.","LCA; streamlining; environmental impact; hotel","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","Industrial Ecology Joint degree Leiden & Delft","",""
"uuid:3b7f7452-b129-4b37-a555-a772a13d4950","http://resolver.tudelft.nl/uuid:3b7f7452-b129-4b37-a555-a772a13d4950","By-pass pigging: Experiments and simulations","IJsseldijk, H.P.","Henkes, R.A.W.M. (mentor)","2016","Pipelines are used in many industries as a means of transporting fluids, for example in the oil and gas industry. Pigs are devices that move through such pipelines, for instaoce to cleao the pipeline or to perform internal inspections. They are driven by a pressure difference over the pigs. Since this is a risky operation, there is a strong motivation to control the motion of these pigs. One possibility is to use so-called by-pass pigs. These pigs have a hole through their body such that fluids cao by-pass the device. This lowers the pig velocity. If the by-pass area cao be varied during a pigging operation, it is possible to control the pig velocity. This concept is relatively new aod not yet completely understood. Research is currently carried out at the TU Delft in collaboration with Shell to get abetter understaoding of the behaviour of conventional pigs of by-pass pigs. This MSc thesis is part of that project aod focuses on performing experiroents for pigging operations in a laboratory environment. The relevance is two-fold From one haod, more insight will be obtained in the encountered phenomena From the other hand, the results can be used to validate the numerical pigging model which is currently in development. The experiroents were carried out in a flowloop at the department of Process & Energy at the TU Delft. The flowloop has a length of 65 m aod a diameter of about 52 mm. Air is used as working fluid aod the flowloop has an atroospheric outlet pressure. Duriog the pigging experiroents, the bulk velocity aod pressure in the flowloop were recorded. Three cameras were used for visual observation from which the velocity was deduced. The modular pig design made it possible to quickly chaoge between different pig configurations. It turned out that small variations in this configuration can have large influence on the pig motion. A pre­ dominant characteristic of the pig motion is the so-called stick-slip motion. This motion is characterized by a quick acceleration and deceleration of the pig as a consequence of a varying friction. A module was added to the numerical pigging model to include the effect of variations in the friction, which forces the siroula­ tion to give a stick-slip behaviour. Besides this, also an aoalytical approach was taken to obtain first insights into this behaviour. The experiroental results show that the maximum pig velocity cao become significantly larger thao the average pig velocity. The ratio of the maximum velocity over the average velocity increases at lower bulk velocities. The stick-slip models cao give a reasonable good estimation of the maximum pig velocity. Besides this, they predict a similar trend in the pressure fluctuations. To compare the influence of the bulk velocity aod the by-pass area, ao extensive parameter study was carried out. Results of 132 pigging runs with two different types of sealing were included. The by-pass area was varied from O % to 4 % and the bulk velocity was varied in the raoge of 1.5 ml s to 7 ml s . The focus was on the effects of these chaoges on the average pig velocity, the average friction and the staodard deviation of the pressure. It turned out that the friction of the pigs used in the experiroents was not depending on the velocity. The staodard deviation of the pressure, which is a measure of the intensity of the stick-slip behaviour, was different for both types of sealing. However, for a certain configuration no dependence on the pig velocity was found. The average pig velocityitselfis largely dependent on the by-pass area. The results were compared with the numerical pigging model, a commercial package and a steady-state analytical model. Itturned out that the average velocity cao accurately be determined with all these models, even if the pig motion shows a strong stick-slip behaviour.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Process and Energy","","Faculty: Aerospace Engineering, Department: Aerodynamics, Wind Energy & Propulsion","",""
"uuid:b56622b5-cd33-44d5-b26c-8c4a565fc11f","http://resolver.tudelft.nl/uuid:b56622b5-cd33-44d5-b26c-8c4a565fc11f","Model-Based Governance of Phosphate Market Imbalances: Mixing and Matching Model Building Blocks","Keijser, B.M.J.","Verbraeck, A. (mentor); Pruyt, E. (mentor); Storm, S.T.H. (mentor); Bastein, A.G.T.M. (mentor)","2016","Phosphorus is essential for all life on this planet. It is needed to provide for high productivity yields in modern agriculture: a key part of the Green Revolution that took place in the twentieth century. There is no substitute for phosphate rock and phosphate fertilizer in their agricultural role. Both the markets for phosphate rock and phosphate fertilizers have an oligopolistic structure, oligopolistic behaviour is observed as well as an increasing influence of state ownership. Exporting countries are thus in the position to influence importers by using their phosphate resources in a geo-strategic way through trade restrictions and strategic market behaviour. This master thesis advocates and demonstrates a novel approach of modelling: namely to use stock-flow mechanisms, decision-making mechanisms and market-clearing mechanisms to represent commodity markets. The focus is on the process of constructing a quantitative model through mixing and matching of building blocks that originate from different modelling methods: system dynamics, agent-based modelling and economic game theory. Conclusions are drawn on what policies European phosphate-importing countries should use to strengthen phosphate market governance. Conclusions are also drawn on how to take the utmost advantage of modern computational tools to flexibly model problems in commodity markets.","Phosphate market; Quantitative modelling; Model building blocks; Trade restrictions; Oligopoly economics","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi Actor Systems","","","",""
"uuid:e3a3d28c-70d9-4c4e-841c-fe8a2fe6ab96","http://resolver.tudelft.nl/uuid:e3a3d28c-70d9-4c4e-841c-fe8a2fe6ab96","Towards understanding the 3D infrasonic wavefield","Van der Woude, T.","Evers, L.G. (mentor)","2016","In the evening of 2014, June 03, two explosions occurred at the MSPO-2 plant of Shell Moerdijk. The Cabauw infrasound array detected the infrasound generated by the blasts as two distinct signals. The distance between Shell Moerdijk and the Cabauw infrasound array is 40.5 km. This array is unique as it measures infrasound in 3D. The Fisher analysis is used to process the infrasound signals. The horizontal 2D Fisher analysis veries the infrasound source to be Shell Moerdijk, and it shows an azimuthal deviation of 2.1 caused by the influence of tropospheric crosswinds. The Shell Moerdijk explosions are used as a case study to investigate the measurement of infrasound in 3D. The frequency of the signals detected at the tower has been observed to be lower than the frequency of the signals detected at the surface. A possible reason is that the infrasound signals propagate through small pipes before reaching the tower sensors. Consequently, a reduced coherency between the tower and ground signals exists. Due to this lack of coherency, the 3D Fisher analysis was not able to perform correctly. The signature of the infrasound waveform with altitude is examined in detail, obtained by the vertical measurement of infrasound. It is showing up- and down-going waves in both explosion signals which can only be seen in the 3D vertical measurements. Ray tracing confirms the existence of these different waves. In summary, this research shows the added value of measuring infrasound in 3D.","3D infrasonic wavefield; up- and down-going waves; explosive source","en","master thesis","","","","","","","","2016-12-31","Civil Engineering and Geosciences","Geoscience & Engineering","","Double degree Applied Geophysics","",""
"uuid:d52c040d-9423-4790-996c-1af1e1dfe5a7","http://resolver.tudelft.nl/uuid:d52c040d-9423-4790-996c-1af1e1dfe5a7","Personalization vs. Privacy: Overcoming the Users’ Privacy Concerns in the Indonesian Peer-to-peer Ridesharing Service","Aditya, F.D.R.","Ubacht, J. (mentor); Kroesen, M. (mentor); Janssen, M.G.W.H.A. (mentor)","2016","The recent ICT developments have created new business model innovation opportunities. One of notable opportunities is to offer personalization to the peer-to-peer (P2P) ridesharing users in transportation service. However, to offer personalization, the company will need more sensitive personal information disclosure from its customer. Therefore, the action of P2P ridesharing companies to offer personalization to its users might be unsuccessful due to the disability of the companies to acquire the users’ personal data and deliver the personalization service that caused by the elevated privacy concerns of the users. This study aims to offer recommendations to P2P ridesharing companies to overcome the users’ privacy concerns. By using a survey questionnaire to 265 Jakarta respondents, this study concludes that the utilitarian approaches are the most effective means to increase the users’ willingness to disclose personal data. However, the privacy assurance approaches are also important and preferred by the respondents to ensure the users’ privacy right. Additionally, the study also concludes that the most expensive privacy type in this context of study is the privacy of behavior and action, whereas the privacy of location and space can be bought by offering the usefulness of the service only. This study can be improved in several areas, most notably in the area related to sampling strategy. Furthermore, the follow up research can also be conducted by performing design science research to apply this study in a legitimate artifact.","peer-to-peer ridesharing; privacy; privacy calculus; conjoint analysis; privacy valuation","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","","",""
"uuid:2db8fe18-436c-4c76-9947-014e33ddffcf","http://resolver.tudelft.nl/uuid:2db8fe18-436c-4c76-9947-014e33ddffcf","Mobile crowdsourcing in an enterprise environment","Bashirieh, S.","Bozzon, A. (mentor)","2016","As crowdsourcing gains popularity, organizations seek ways to systematically and reliably engage with their own skilled and trusted workforce, to enrich their data creation. In this setting, mobile crowdsourcing platforms allow for opportunistic task executions and thus, potentially, for higher execution throughput. How to engage (and retain) employees in enterprise crowdsourcing campaigns is, however, an open research topic. When would employees be more willing to contribute in a campaign? What would be the right moment to seek for their contribution? This thesis contributes to knowledge on those matters. To answer these questions, we surveyed 93 IBM employees, to discover the factors that might affect engagement in enterprise mobile crowdsourcing. The survey informed the design of an experiment that involved 83 employees of a multinational company. The experiment aimed at studying the effectiveness of different notification strategies aimed at nudging employees for task execution during their work time. We studied how factors such as time and context of notification can affect the participation and retention of employees in crowdsourcing campaigns. Results show that break times are the best to solicit help from employees. In addition, we discovered that “aggressive” notification strategies act as deterrent for participation, and that moderate yet regular nudges are the most likely to yield useful contributions.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Software Technology","","","",""
"uuid:366c5c7f-79d8-4895-ad55-6f3a7a6570f6","http://resolver.tudelft.nl/uuid:366c5c7f-79d8-4895-ad55-6f3a7a6570f6","Imprints in Dual Phase Hot Rolled Steel","Tombrock, A.J.C.","van der Zwaag, S. (mentor)","2016","DP600 HR is one of Tata Steels hot rolled products. It is a dual phase steel unique to Tata Steel for its two step cooling process and low coiling temperature. The finished product has encountered several surface defects, one of which are imprints. In order to find the root cause for the imprints a literature study, a factory inspection and a number of experiments have been conducted. To find the origin of the imprint two coils have been produced with the intention to create imprints on the surface. With these trials and the observations in the factories within Tata Steel it is found that a loose oxide layer of 4-10µm thick is formed at the end of the hot strip mill process. Due to the low coiling temperature a fair amount of water remains on the strip until further processing. At both the recoiler and the pickling line a build-up of loose oxide is seen. From both former trials and the trials done during this project it is seen that a build-up of oxide leaves imprints with a depth of 20 to 150 µm. In order to confirm that the imprints can indeed be produced by loose oxides, two experiments were conducted. Loose oxide particles and bearing balls have been pressed into the steel surface with the result in both cases of imprints that can be compared to the ones obtained in the factory. In this work a new procedure has been developed to test the growth of the oxide layer as well as its adhesion to the steel surface. 1. To grow an oxide layer the Thermo Mechanical Testing Simulator is used. This device is normally used to test hot deformability of metals but its heating chamber can also simulating the run out table two step cooling cycle. Both cooling rates are varied between 30, 60 and 90 K=s keeping initial, intermediate and final temperatures equal to the factory values. 2. To test the adhesion of the oxide to the steel a tensile test is performed with the aid of the ARAMIS optical strain measuring system. Since the ARAMIS system follows optically the changes in the steel surface during straining the oxide layer is usually previously removed, because it will break first and interfere with the measurement. However this fact is used here to our advantage since the strain that is necessary to remove the layer can now be quantified. This loss in measurement data corresponds well to the initial loss of oxide from the sample. From the ARAMIS data it is seen that increasing the cooling rate of the first cooling step decreases the amount of strain required to break the oxide from the steel. The second cooling step does not have a pronounced effect on the adhesiveness. The microscopy analysis showed an unknown oxide layer at the steel/oxide interface. This layer is mainly iron oxide but contains an unusual amount of chromium. It is presently not known how the chromium is distributed in the oxide nor what its effects on the properties of the scale are. It is concluded that the imprints are caused by a build-up of loose oxide on the work rolls. The imprints can be prevented by not using the levelling and scale breaker work rolls. The adhesiveness of the oxide to the steel under deformation can be improved by reducing the cooling rate on the first cooling step. Using the Thermo Mechanical Test Simulator a controlled layer of oxide can be grown on steel samples. The adhesiveness of the oxide layer can then be tested by means of a tensile test while being recorded by ARAMIS. This set-up can be further improved to obtain a standardised oxide adherence testing method.","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures and Materials","","","",""
"uuid:14154bd2-d5a5-46bb-8d37-172725d6953b","http://resolver.tudelft.nl/uuid:14154bd2-d5a5-46bb-8d37-172725d6953b","Preferential crystallization of a racemic compound via its conglomerate co-crystals","Villamil Ramirez, O.F.","Kramer, H.J.M. (mentor)","2016","Preferential crystallization, as a powerful chiral resolution technique, is intrinsically limited to chiral molecules that crystallize as conglomerates. Many studies have been conducted on using chemical reactions to convert the target molecules, which originally form racemic compounds, into conglomerate-forming derivatives salts or by creating solvate, for the application of preferential crystallization. Up to this date conglomerate co-crystals of racemic compounds have never been applied as the intermediate for chiral resolution. In this study, preferential crystallization of the model compound Ibuprofen (IBU), originally a racemic compound, was carried out via its conglomerate co-crystal with 2,4-bipyridine ethylene (BPE) in heptane. Suitable operation conditions were selected based on pseudobinary phase diagram of the model compound system constructed under different IBU-BPE ratio. A unique measurement method combining polarimeter and Nuclear Magnetic Resonance (NMR) measurements was developed to identify the enantiopurity and the yield of the final product, which was a mixture of racemic IBU and IBU-BPE co-crystals, a likely result from this complex system. With respect to the results, preferential crystallization of IBU was successfully performed by slowly cooling down a saturated solution of racemic IBU-BPE, initially at T=57.5°C, after seeding it with S-IBU/BPE crystals to T=53°C with a cooling rate of 0.3°C/min. The recovered crystalline product contained pure IBU and a mixture of R-co-crystals and S-co-crystals with a yield of 44%, with the amount of S-co-crystals recovered four times higher than the amount of R-co-crystals present in the final product. The existence of R-IBU/BPE indicates that the primary nucleation of the undesired enantiomer still took place. This can be minimized by performing the experiment at bigger scale, where samples of the mother liquor can be taken during the process in order to monitor the evolution of the enantiomeric excess enabling the defining of an optimum filtration time. The crystallization of racemic IBU along with the cocrystals lowered the purity of S-IBU. By using new ratios of IBU/BPE close to the stoichiometric co-crystal ratio and with IBU in excess, this impurity can be diminished. Additionally a comprehensive study of the Metastable Zone Width (MSZW) in a bigger volume and the exploration of mixture of solvents can improve the definition of the final temperature in order to avoid the presence of racemic IBU and R-co-crystals in the crystals produced.","conglomerate co-crystal; preferential crystallization; chiral resolution","en","master thesis","","","","","","","","2017-08-24","Mechanical, Maritime and Materials Engineering","Process and Energy","","","",""
"uuid:03ab3216-1dcd-45ce-8e71-4b015326af6f","http://resolver.tudelft.nl/uuid:03ab3216-1dcd-45ce-8e71-4b015326af6f","The City is Mine: Evaluating the Potential of Copper Urban Mining in Amsterdam for 2050","LIN, H-C.","Van der Voet, E. (mentor); Van Bueren, E.M. (mentor)","2016","Are cities mineable? How much can the old copper scraps be transformed into ""nutrients"" of the city and join the economy again? Can urban mining really take us towards a circular economy? Although there has been extensive discussions on the possibilities of urban mining in academic literatures and public media, the research on quantitatively evaluating the potential of urban mining is relatively little, especially when it comes to future material stocks. This study aims at examining the ambition of copper urban mining activities on the basis of quantifying the in-use stocks and demands, envisioning urban mining operation in action, assessing their sustainability implications, and identifying the knowledge gap to bring urban mining to practice in a circular economy. The study provides new ground in the domains of material flow analysis by generating necessary information to kick start the exploration of urban mining within the framework of circular economy.","","en","master thesis","","","","","","","","","Architecture and The Built Environment","Management in the Built Environment","","Industrial Ecology","",""
"uuid:b094112e-4209-45b7-8dd6-6bafd8515c7f","http://resolver.tudelft.nl/uuid:b094112e-4209-45b7-8dd6-6bafd8515c7f","Assessing the Effects of Deafness on Cortical Activity Peaks in the Developing Human Auditory System Measured by an EEG Neuroimaging Method","de Vreede, D.K.","Gordon, K. (mentor); French, P.J. (mentor)","2016","Bilateral cochlear implants (CIs) can restore hearing in deaf children, by electrically stimulating the auditory pathways. The effects of long-term deafness have been extensively studied and it is well known that deafness leads to cross-modal reorganization of the brain. However, the short term effects of CI use on the developing auditory networks are still relatively unknown. This study looks across a broad range of frequencies to analyze neural firing patterns within the developing auditory networks, and relates them to distinct functionalities in the cortical networks. Time-frequency windows were defined as: delta-band activity (δ): 0.5-4 Hz, theta-band activity (θ): 4-8 Hz, alpha-band activity (α): 8-13 Hz, beta-band activity (β): 13-30 Hz and gamma-band activity (ᵞ): 30-70 Hz. EEG cortical responses were recorded in bilaterally implanted deaf children and time-frequency analyses was applied in combination with spatial filtering, which showed distinct but consistent patterns of activity, characteristic for each frequency-band. Neural activity amplitude and latency were found to be significantly affected by frequency-bands but not by `type' of sound stimulus (ranging from biphasic pulse trains to a word), with temporal activity occurring at different latencies depending on the frequency-band. It was found that evoked auditory activity activates a complex network of cortical activity, despite cortical immaturity and deafness, imperative for hearing.","beamforming; hearing loss; cochlear implants; frequency filtering","en","master thesis","","","","","","","","2021-08-24","Electrical Engineering, Mathematics and Computer Science","Biomedical Engineering","","","",""
"uuid:9cdb4d17-5104-491a-8249-daca5666f016","http://resolver.tudelft.nl/uuid:9cdb4d17-5104-491a-8249-daca5666f016","Flight Training Effectiveness of Ecological Interface Design: An Evaluation of the Total Energy-Based Perspective Flight Path Display","Deerenberg, R.","van Paassen, M.M. (mentor); Mulder, M. (mentor)","2016","","EID; Training","en","master thesis","","","","","","","","2021-08-24","Aerospace Engineering","Control & Operations","","ATM & Airports and Safety","",""
"uuid:bd061b99-e4c3-4400-a240-95011d697340","http://resolver.tudelft.nl/uuid:bd061b99-e4c3-4400-a240-95011d697340","The Role of Motion Feedback in Manual Preview Tracking Tasks","Morais Almeida, J.","Pool, D.M. (mentor)","2016","","","en","master thesis","","","","","","","","2021-08-24","Aerospace Engineering","Control & Operations","","Control & Simulation","",""
"uuid:cc9b668e-df22-43ae-8154-720ffaa8a1a4","http://resolver.tudelft.nl/uuid:cc9b668e-df22-43ae-8154-720ffaa8a1a4","Active Learning by Discrepancy Minimization: A Comparison of Active Learning Methods Motivated by Generalization Bounds","Viering, T.J.","Loog, M. (mentor); Krijthe, J.H. (mentor)","2016","In many settings in practice it is expensive to obtain labeled data while unlabeled data is abundant. This is problematic if one wants to train accurate (supervised) predictive models. The main idea behind active learning is that models can perform better with less labeled data, if the model may choose the data from which it learns. Active learning algorithms propose which unlabeled objects should be queried for their labels with the goal of improving the predictive model the most with as little queries as possible. We study active learners that minimize generalization bounds. We introduce a novel active learning algorithm, motivated by the fact that its generalization bound is tighter than the one used by the state-of-the-art MMD active learning algorithm. We find the surprising result that our introduced active learner performs worse in terms of the mean squared error. We find that tighter generalization bounds do not guarantee improved performance for active learning in general, and we characterize which properties of generalization bounds are relevant for performance instead. Inspired by these insights, we introduce a second novel active learning algorithm, which minimizes a looser generalization bound, that compares favorably with the MMD active learner. Our novel active learner compares favorably as well with the state-of-the-art TED active learner in case of model misspecification. Our analysis which describes favorable properties of generalization bounds for active learning is novel, and can be applied to construct new improved active learning algorithms in the future. Our analysis is likely applicable as well to the closely related fields of sample bias correction, transfer learning and domain adaptation.","active learning; machine learning; kernel methods; learning theory","en","master thesis","","","","","","","","2018-11-30","Electrical Engineering, Mathematics and Computer Science","Intelligent Systems / Pattern Recognition and Bioinformatics","","Media Knowledge Engineering (MKE) track of Computer Science","",""
"uuid:c268ef75-8c9e-44ff-8121-4640b610cd2c","http://resolver.tudelft.nl/uuid:c268ef75-8c9e-44ff-8121-4640b610cd2c","Hematopoiesis of a Healthy Supercentenarian is Dominated by One Myeloid-Biased Stem Cell Clone for at least 9 Years","Makrodimitris, S.","Reinders, M.J.T. (mentor)","2016","","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Intelligent Systems","","Pattern Recognition and Bioinformatics","",""
"uuid:554fe317-e008-4f05-a489-4bd6a76a4ec9","http://resolver.tudelft.nl/uuid:554fe317-e008-4f05-a489-4bd6a76a4ec9","Model-based Fault Detection and Diagnosis of Inventory Record Inaccuracy in Customs Warehouse Management in the Netherlands","Tian, Y.","Herdeiro Teixeira, A.M. (mentor)","2016","Nowadays, as the increasingly intense international trade and logistics, the administrative burden of customs has been dramatically raised. In order to reduce repetitive taxing work and promote automatic administration, custom warehouses with electronic declaration systems are introduced and implemented by EU countries. The owner of custom warehouses can postpone the payment of import duties under the supervision of customs in case these goods are re-exported. The implementation of customwarehouses has significantly simplified custom formality and enhance administrative efficiency. However, its successful management highly depends on the accuracy and reliability of inventory information. Inaccurate inventory records may lead to a variety of undesirable effects, such as troubles in taxing and compliance failure with custom terms. As the common way to address this problem in the past, the traditional physical auditing is always time-consuming and costly, especially when the inventory is various and massive. Moreover, since the multiple stakeholders involvement in custom warehouses, the fault detection and diagnosis processes for inventory record inaccuracy in reality are usually problematic and complicated. Without the effective communication and cooperation between different stakeholders, it is difficult to solve inventory record inaccuracy issues and to guarantee the efficiency and reliability of customwarehousemanagement.","","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","","",""
"uuid:bb15bdfd-2e7b-4ba0-961e-811cb73e6c44","http://resolver.tudelft.nl/uuid:bb15bdfd-2e7b-4ba0-961e-811cb73e6c44","Shallow foundations for subsea structures: a comparison between design codes and numerical analysis","van de Riet, B.A.M.","Hicks, M.A. (mentor); Pisano, F. (mentor); Brinkgreve, R.B.J. (mentor); Burgers, R. (mentor); Yenigul, N.B. (mentor)","2016","Subsea structures such as pipeline end termination (PLET) structures are currently designed according to the ISO:19901-4 and API RP 2A-WSD design codes by Allseas. It has become interesting to investigate the embedded conservatism in these design codes as not only the increasing demand of installations in larger water depths and/or on poor soil conditions, but also the use of larger pipeline diameters and the increase of the size and weight of subsea structures. This embedded conservatism could have a significant effect on the economical and operational aspects. The economical aspects can be explained by the costs for fabrication and offshore installation and the operational aspects by the operational limits of current vessels. From a literature review is found that Terzaghi derived an equation for the bearing capacity of the soil for very specific conditions. Afterwards several investigators including Brinch Hansen, Vesic and Meyerhof revised the solution in order to determine the bearing capacity of the soil for more general situations. Brinch Hansen is mostly followed by the ISO design code, while Vesic is followed by the API design code. Two different safety approaches are found from an analytical analysis of the investigated design codes, namely the partial safety factor approach in the ISO design code and the global safety factor approach in the API design code. Both the investigated design codes only determine a safety factor against bearing failure and sliding failure in order to determine the safety of a foundation design. In this study four case studies are investigated and a review of the site investigation reports is performed. Since no sufficient information regarding soil deformation parameters are provided to Allseas only the embedded conservatism in the design codes regarding soil strength is investigated in this study. For all four case studies an increasing shear strength profile is determined. This profile can however not be modeled in the API design code, therefore a constant shear strength profile which gives an equivalent soil bearing capacity as compared to the increasing shear strength profile is determined. Two methods using the numerical program PLAXIS are applied to determine a constant shear strength profile. In this investigation the numerical program PLAXIS is used to determine the embedded conservatism in the ISO and API design cods. Two PLAXIS models are used, in the first model the partial safety factor approach is considered and partial load and material factors are applied to the input values. The results of the PLAXIS model are compared to the results from the ISO design code. In the second PLAXIS model the global safety factor approach is considered and the results are compared to the results from the API design code. The ratio between the safety factors against bearing and sliding failure as determine by PLAXIS and the design codes is used in this study to investigate the embedded conservatism in the design codes. Ratio’s between 1.5 and 2.3 are found for the foundation designs of the four investigated case studies, which indicate that there is some embedded conservatism in the design codes. A sensitivity analysis is performed to investigate the influence of the aspect ratio and the embedment depth on the embedded conservatism. The results of these sensitivity analyses show that a variation in aspect ratio does not have an influence on the embedded conservatism, while embedment depths larger than 0.6 meter do have an influence on the embedded conservatism in both the ISO and API design codes. An example calculation is made in order to investigate the cost reduction for a design made by a numerical analysis instead of the analytical design codes. In this calculation is shown that the foundation area can be reduced by 47%. The smaller foundation area results in a cost reduction of 16% for the fabrication of the total subsea structure. An even larger reduction in costs for the installation operation could be achieved if the size of the structure can be optimized by a numerical analysis because of the high day rates for Allseas installation vessels.","shallow foundation; numerical analysis; subsea structures; plaxis; design codes","en","master thesis","","","","","","","","2018-08-23","Civil Engineering and Geosciences","Geoscience & Engineering","","Geo-Engineering","",""
"uuid:51828825-d26d-47e6-b760-6f14f2ba2f7b","http://resolver.tudelft.nl/uuid:51828825-d26d-47e6-b760-6f14f2ba2f7b","Hydrodynamic Limit for the Symmetric Inclusion Process","Ayala-Valenzuela, M.A.","Redig, F.H.J. (mentor)","2016","This thesis deals with the proof of the hydrodynamic limit for the symmetric inclusion process (SIP). After introduction of our process and some basic concepts on the theory of Markov processes we proved self-duality for the SIP. Once equipped with SIP self-duality, and together with an assumption on the initial configuration, we were able to prove that the evolution, on the diffusive scale, of the empirical density of the symmetric inclusion processes in $\mathbb{Z}^{d}$ is described by a weak solution of the heat equation.","Hydrodynamic limits; Interacting Particle Systems; Symmetric Inclusion Process","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","DIAM","","","",""
"uuid:f9bc7e57-c03d-4dfa-9e91-8e7d008fe64f","http://resolver.tudelft.nl/uuid:f9bc7e57-c03d-4dfa-9e91-8e7d008fe64f","Ulticast: Designing an add-on that creates new material benefi ts and increases the printing speed for FDM printed parts","Rossing, L.","Tempelman, E. (mentor); Van de Geer, S.G. (mentor)","2016","Ultimaker produces desktop Fused Deposit Modelling (FDM) 3D printers. One downfall of the FDM technology is that it is relatively slow in producing solid parts, compared to other rapid prototyping techniques. Larger volumes increase the printing time exponentially. This project started off with a unique idea to replace the internal volume with a material that can be put in place with a much faster process. The designed product is an add-on targeted on agile developers. A unique approach has been used to come to inventive solutions to scale down material and process combinations to the printer context. For three chosen combinations a proof of principle was prototyped. This was done in an iterative way, using the 3D printer to the fullest. With the help of stakeholders, the chosen concept was decided to be the casting of two component resin which creates many new material possibilities and can cater new markets.The add-on is designed to be modular and scalable. The system uses two peristaltic pumps which pump resin trough flexible tubing from two material containers to the print head. A static mixing nozzle in the printhead is used to mix the resin. This static mixing nozzle can be printed on an Ultimaker 2+. The curing of the resin is an exothermic reaction so overflow channels are needed to be printed to account for the expanding volume. Furthermore a material and software system are proposed. For this thesis the final proposed design has been integrated into the Ultimaker 3 and the prototype in the Ultimaker 2+. The final design for the concept of the add-on was proven to show clear potential to fulfil the original assignment and drivers with the help of a technology and business evaluation. Stakeholders were presented with the final design and interviewed. The prototype of the add-on was simulated in the printer context and delivered proof that the filling principle is feasible. The use case for the 3D printed cast could be printed almost 2,5 times faster. This add-on can make FDM technology for big printers more attractive. When the resin is mixed correctly material benefits such as increase in strength and stiffness can be realised dependent on the type of PU used. Two component PU is so versatile many other material opportunities are in reach of this concept which creates potential to reach entirely new markets.","3D printing; FDM; increasing speed; material benefits; casting; 3D printed static mixing nozzle; two component polyurethane","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design Engineering","","","",""
"uuid:932b7311-e542-4c75-be2d-c73b3adb5647","http://resolver.tudelft.nl/uuid:932b7311-e542-4c75-be2d-c73b3adb5647","Research on procedures to estimate 3D response of Vortex Induced Motion (VIM) of multi-column floaters based on 2D CFD calculations","Liu, Y.","Huijsmans, R.H.M. (mentor); Maximiano, A.S. (mentor); Koop, A. (mentor)","2016","It is well known to offshore industry that current can trigger significant in-line and cross-line motions on floating offshore structures due to resonance. For a small dimension structure, like risers, this will lead to Vortex Induced Vibration (VIV). A lot of research has been carried out on this topic. However, for a large dimension structure, like semi-submersible floater, the frequency of motion is relatively low, resulting in the so-called Vortex Induced Motion (VIM). In the last decade, the concept of VIM has gained increasing attention. Nowadays it is one of the most important design topic for deep draft semi-submersibles, especially within the scope of riser fatigue analysis. Currently, model test is the preferred way to predict the VIM behaviour of a known offshore structure. However, due to time and cost restrictions, it is not feasible to perform VIM model tests in the early design stage. Therefore, a need for a fast and reliable method to predict VIM arises. A good candidate is performing CFD calculations. With the development of numerical method and hardware, at the moment CFD can provide reasonable VIM predictions. Compared with model test, CFD can save a lot of time and is relatively cheap to perform sequences of tests with different parameters which will greatly benefit the early design stage. Several 3D 3DOF CFD VIM calculations for offshore structures have been performed in MARIN and the result is quite comparable to model test result. However, about 2 weeks per calculation are required. At an early design stage, a faster prediction of the VIM behaviour of an offshore structure with desired parameters through CFD calculation is desirable. This target could be achieved through 2D CFD calculation. Compared with 3D CFD calculation, CFD VIM in 2D can be executed and accomplished within 1 or 2 days. The columns and pontoon of the floater will be treated separately in CFD. CFD VIM calculations in 2D could also provide reasonable and comparable results. It is feasible to study the VIM behaviour of multi-column floaters through 2D CFD calculations. As a consequence, in this project, the research on VIM behaviour of multi-column floaters will be performed in ReFRESCO, a commercial CFD code of MARIN. Different structural parameters (mass ratio, external damping levels, geometry of the columns) and environmental conditions (the velocity of current) will be investigated in this project, in terms of their influence on VIM behavior. This can give some insight about VIM behaviour of multi-column floaters and how to reduce it. Besides, the numerical set-up will also be surveyed, like time step and grid size sensitivities, to ensure the accuracy of the calculation. Finally, results from 2D CFD calculations will be compared with those from 3D CFD calculations. The feasibility of the estimation of a semi-submersible’s VIM response in 3D from 2D CFD calculations would be checked.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Offshore and Dredging Engineering","","","",""
"uuid:92868b2d-87c4-4e0b-9761-698dd54f02f9","http://resolver.tudelft.nl/uuid:92868b2d-87c4-4e0b-9761-698dd54f02f9","Cruise Performance Optimization of the Airbus A320 through Flap Morphing","Orlita, M.","Vos, R. (mentor)","2016","In the era of increasing aviation traffic the conditions are right to promote design of ambitious concepts. At Fokker Aerostructures attention is drawn to smooth in-flight shape morphing to produce a structurally functional Variable Camber Trailing Edge Flap (VCTEF). The deployment mechanism would fit into the flap, not limiting other functionality such as Fowler motion, while at the same time allowing small camber variations during cruise. This is based on the assumption that such morphing will bring performance improvements which are commercially interesting. The main goal of this research was therefore to predict these performance benefits and thus the applicability for a specific case of the Airbus A320 aircraft in cruise flight. This aircraft is large enough to accommodate the technology, it is operated in great numbers and cruise is the most fuel demanding part of its mission. Since the concept is in the development phase the further task is to determine the morphing design setup which performs best. The amount of morphing is driven by a circular reference function, which is added to the base geometry at any desired streamwise cut of the wing by manipulation of the airfoil coordinates as seen on the cover. The design is specified by the points on the airfoil upper surface where the morphing begins and ends, boundaries of the morphing region where upper surface bending is allowed. As also found in other literature it is shown that morphing can bring drag reduction for a section, wing and the complete aircraft. This varies throughout the cruise, which is translated to more sophisticated performance indicators for comparison and evaluation of the benefits. The first indicator is the increase of range over the design mission for the given aircraft. The second and third are the fuel savings which can either be obtained by increasing the cruise end weight, or by decreasing the cruise beginning weight, both by the amount of the saved fuel while keeping the aircraft range constant. In order to evaluate these indicators, the Breguet range equation is used in a discretized form, utilizing an interpolated lift-to-drag ratio determined by aerodynamic analysis at 7 cruise points. This was done using both the 2D solverMSES and a quasi-3D tool Q3D developed at TU Delft comprising ofMSES and AVL vortex lattice solver. For the analysis a complete A320 model is required, which was not available and was created from the known performance data and partially assumed geometry. The unknown wing geometry was optimized with respect to the mid-cruise drag simulating an already efficient aircraft, as suggested by literature. Other model components were the horizontal stabilizer, fuselage and center of gravity position allowing trim at the reference cruise points and obtaining the lift requirements for the wing and a representative section. Under these lift requirements the 2D and 3D analyses were performed at individual cruise points to obtained improved lift-to-drag ratios which could be then used to evaluate the range improvement. Itwas found thatwith morphing in 2Dthe drag reduction can amount up to 9% at the beginning of cruise but parabolically decreases towards mid cruise after which it remains below 0.5%. This is primarily due to manipulation of the shockwave and the boundary layer at the given lift requirements, which is most dominant at high cruise lift coefficients. Since the induced drag was found not affected by the assumed morphing, such improvements are further scaled down when evaluated for the entire wing and even further from the aircraft point of view, resulting in a range improvement in order of 20km and fuel savings of below 0.5% of trip fuel. A sensitivity analysis on the design variables has shown that these performance benefits have small sensitivity to the size of the morphing region and that a very aft located regions are the most beneficial, suggesting that a small tab at the trailing edge might be a better and easier solution. In view of these results the smooth morphing concept is deemed not applicable for the cruise of short range aircraft such as A320. However, given the parabolic behaviour of the drag improvements, larger potential can be expected for long range aircraft, which is the main resulting recommendation of the conducted research. Furthermore it cannot be excluded that other regimes could benefit more from the morphing concept, such as high-lift, which would probably require wind-tunnel testing, as discussed in the final Appendix of this work.","morphing; camber; transonic; drag; optimization; cruise","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy & Propulsion","","Flight Performance and Propulsion","",""
"uuid:aa5d1e6e-cba4-4906-978c-ca5f60d9a91a","http://resolver.tudelft.nl/uuid:aa5d1e6e-cba4-4906-978c-ca5f60d9a91a","Relative Navigation in Asteroid Missions: Dual Quaternion Approach","Razgus, B.","Mooij, E. (mentor); Choukroun, D. (mentor)","2016","","","en","master thesis","","","","","","","","2017-08-23","Aerospace Engineering","Space Engineering","","Astrodynamics & Space Missions","",""
"uuid:c4801d87-72f8-4461-a3f7-d3dcb780c60f","http://resolver.tudelft.nl/uuid:c4801d87-72f8-4461-a3f7-d3dcb780c60f","Evaluating the performance of expert-finding systems: Defining Practical Key Performance Indicators","Hansen, P.","Brazier, F.M. (mentor); Dignum, V. (mentor); Verburg, R. (mentor)","2016","","","en","master thesis","","","","","","","","2016-08-23","Technology, Policy and Management","","","","",""
"uuid:5696010a-1cc8-4433-9de3-39d2e18ed0f7","http://resolver.tudelft.nl/uuid:5696010a-1cc8-4433-9de3-39d2e18ed0f7","Diversified Bits and Pieces: Patterns and Motives of Diversification Strategies of Internet-based Firms","van den Berg, H.G.W.E.","Bakker-Wu, S. (mentor); van der Vorst, R.R.R. (mentor)","2016","The goal of the thesis is to identify patterns and motives of diversification strategies of internet-based firms. This corporate strategy of entering new markets with new products has mainly focused on conventional firms. However, since the raise of the Internet the characteristics of firms have changed. The study involves the online character of firms in the assessment of diversification strategies. The graduati¬¬on project aims to explore the patterns and motives through a multiple case study design. The findings suggest that gaining market-power and the reaction to competition and market mechanisms are important motives. In contrast to the literature study, financial interest is not found to be of significant importance. Next to that, case evidence has indicated the urge to innovate and the availability of resources as patterns prior to the act of diversification. No evidence is found concerning the influence of external events on the implementation of diversification strategies.","Diversification; Strategic Management; Innovation; Internet","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovation Management","","","",""
"uuid:9ef11baf-576c-4eb0-a84d-b507590b6000","http://resolver.tudelft.nl/uuid:9ef11baf-576c-4eb0-a84d-b507590b6000","Sloshing: Topside Storage Tank Application on Floating Offshore Structures","van Dijk, N.","Huijsmans, R.H.M. (mentor)","2016","Offshore structures with partially filled storage tanks may experience sloshing of the cargo when exposed to waves. Inventive use on the topside result in storage tanks which are built as an integrated part of the deck structure. Weight control and available space is often a critical issue for offshore projects and can be improved by this application. CB&I have decided to carry out a research related to the occurrence of sloshing and impact pressures for these, so called, in-deck tanks. The sloshing assessment procedure is an important part of the structural strength checks. Sloshing occurs when the natural period of the fluid coincides to the motions of the storage tank. Four factors mainly contribute to the sloshing phenomenon. Namely, tank dimensions, fill, fluid properties and motion characteristics. However, the complex, chaotic and non-linear behaviour of sloshing makes it hard to predict or estimate impact pressures. In-deck tanks are applied at the topside of the Aasta Hansteen SPAR project, carried out by CB&I. The application of these tanks faced difficulties concerning the sloshing assessment procedure. There is no method applicable related to this situation. Therefore, a conservative method has been defined as a temporary solution. For future implementation of these tanks, better understanding and knowledge of fluid behaviour is essential. In order to tackle this problem, a CFD analysis is carried out in two phases and concludes with a statistical analysis in order to estimate sloshing impact pressures. The first phase relates to a general 2D CFD simulation for various cases. The second phase includes 2D long time simulations of sloshing cases extracted from the first phase. The results of the first phase show that no sloshing occurs for the Aasta Hansteen SPAR related cases. Where the motion period of 60 seconds is too far away from the period of the 1st wave mode, which is around 8 seconds. FPSO related cases contain a period around 10 seconds and show sloshing impact behaviour. The impacts occur specifically for longer tank lengths and higher filling levels as these cases coincide better with the motion behaviour of the tank. Noted that the combination of input parameters for which sloshing occurs is highly dependent on the forced excitation on the tank, where sloshing behaviour is sensitive to changes of these parameters. Furthermore, a motion case analysis is added and different sea states are assessed from mild to harsh tank motion excitations. Resulting in sloshing for harsher sea states and higher accelerations. Overall, sloshing impacts conclude in the order of 100 kPa - 300 kPa. The impact area includes the vertical wall and 2.4 meters on the top of the tank. In the event of non-impulsive oscillating behaviour (no sloshing), one can apply the linear theory for an accurate prediction of the pressures. However, when the fluid motion becomes chaotic and non-linear, there is no method able to accurately predict the impact pressures. The results of the second phase contain the sloshing impact order of magnitude for eight individual sloshing cases. With difference in fluid, tank length, fill and motion type. Six of these cases can be compared to one another and resulted in a fill/length ratio of 0.063 for the highest impact pressures. A lower viscosity of the fluid seems to increase the sensitivity to sloshing behaviour. Filling levels of 50\% - 70\% show high sloshing impacts, where 80\% fill does not result in sloshing anymore. The increase of tank lengths results in higher sloshing impacts. Briefly summed up: 7m no sloshing, 12m semi sloshing, 15m sloshing impact order 150 kPa - 200 kPa and 20m sloshing impact order 300 kPa - 500 kPa. Two fitting curves are used in order to establish the Exceedance Probability Function. Namely, the Generalized Pareto and Kernel Smoothing. Both show a good fitting, but present different behaviour in the so called 'tail' of the Probability Density Function. A good distribution of this 'tail' result in a better Exceedance Probability Function. A decision on the best fitting curve is not made due to the lack of sufficient simulated statistical data. The sensitivity analysis proved the Kernel Smoothing fitting more robust compared to the Generalized Pareto. Also, the reduction of statistical data resulted in the highest sensitivities within the sensitivity analysis. Which underlines the need of more simulation and statistical data for improvement of the results.","Sloshing; CFD; ComFLOW; In-deck Tank; Impact Pressures","en","master thesis","","","","","","","","2017-08-23","Mechanical, Maritime and Materials Engineering","Marine and Transport Technology","","Offshore and Dredging Engineering","",""
"uuid:feac55be-3f8a-4cb0-bbec-4e3859fc64ce","http://resolver.tudelft.nl/uuid:feac55be-3f8a-4cb0-bbec-4e3859fc64ce","Training Effectiveness of Flight Simulators with Outside Visual Cues","Mendes, M.F.S.","Pool, D.M. (mentor); Mulder, M. (mentor); van Paassen, M.M. (mentor)","2016","","","en","master thesis","","","","","","","","2021-08-23","Aerospace Engineering","Control and Operations","","Control & Simulation","",""
"uuid:b7c85603-b198-44fd-b3f9-127f3095fbcc","http://resolver.tudelft.nl/uuid:b7c85603-b198-44fd-b3f9-127f3095fbcc","Remediation measures for solid bulk cargoes: Moisture content reduction inside a ship's cargo hold","Schoenmakers, M.","Van Paassen, L.A. (mentor); Van Giffen, I.K. (mentor); Schott, D.L. (mentor); Keersemaker, M. (mentor)","2016","Solid bulk cargo liquefaction is a serious problem in the shipping industry. Numerous capsized ships have been reported as a result of liquefied cargo. Only between 2009 and 2016 nine ships sunk, causing 100 casualties. Most accidents did happen with nickel ore transport but other fine grained solid bulk cargoes report accidents as well. The repeating accidents caused by shifted cargo and liquefaction, alarmed the international maritime organisation (IMO). The IMO deals with maritime safety and provides the standard for safe handling and transportation of solid bulk cargo in the International Maritime Solid Bulk Cargo (IMSBC) code. According to this code a material which is classified as a cargo that may liquefy, may only liquefy when its moisture content is above the transportable moisture limit (TML). Once the moisture content is above TML the cargo should not be shipped. However, in case the cargo turns out to be too wet during transport, there are no regulations or methods suggested by IMO to reduce the moisture content. The goal of this thesis is to find an effective method to reduce the moisture content, which is (also) applicable inside a ship’s cargo hold. In this study five different remediation measures are investigated: pumping, air injection, surface evaporation, heating and application of super absorbing polymers (SAP’s). A theoretical evaluation showed that in case a material is sufficiently coarse grained or permeable a wet base will develop and pumping the water from the bottom is the first most practical method to reduce the average moisture content. However, if the material is fine grained and has a low permeability or high water retention capacity, water will not freely drain downward and consequently pumping does not work, hence other methods need to be evaluated. For the four other options water removal rate, zone of influence and costs were evaluated for assumed material and boundary conditions. Theoretical analysis showed that heating to 100°C turned out to be the best remediation measure in terms of water removal rate and costs. Using heating probes which are inserted into the cargo moisture content could be reduced up to 1% within one week. However, no experiments were performed to validate the theoretical analysis for heating and the zone of influence was not determined. Besides heating, air injection turned out to be a good remediation measure, in terms of costs, zone of influence and water removal rate. The other two measures were considered to be too expensive, caused by high material costs or a very long remediation time. Column experiments were performed to validate the theoretical analysis for air injection, surface evaporation and application of SAP’s. The experimental results confirmed the theoretical water removal rate for surface evaporation, while water removal rates for air injection and application of SAP’s was higher than expected from theory. For SAP’s it turned out that the swelling capacity and corresponding water absorption capacity was higher than assumed. For air injection temperature, relative humidity and volumetric flow measurements combined with the accuracy of the weight loss measurements, the difference between theory and experiments might explain experiments had 30% higher water removal rate. All experiments were performed only once on one single material, therefore the results cannot be directly applied for all type of solid bulk cargoes and more research is still advised","solid bulk cargo; moisture content; dehydration; transportable moisture limit; air injection; surface evaporation; super absorbing polymer; heating","en","master thesis","","","","","","","","2021-08-23","Civil Engineering and Geosciences","Geoscience & Engineering","","","",""
"uuid:db849c70-8d1c-49d0-898d-cbc89d20b693","http://resolver.tudelft.nl/uuid:db849c70-8d1c-49d0-898d-cbc89d20b693","Delamination Growth Predictions of Non-Uniform Sandwich Details","Zhang, D.","Alderliesten, R.C. (mentor); Branner, K. (mentor); Joosse, P. (mentor)","2016","Sandwich composite is commonly used as a material of wind turbine blades. In order to meet the practical needs, the design of sandwich always contains some designed-in imperfections, like ply drop of facing laminates, core with resin grid and so on. These unavoidable designed-in imperfections may induce delamination problems and result in huge economic loss. This Master thesis project aims to predict the delamination growth of sandwich with facing ply drop. Di↵erent failure modes of sandwich are studied; the potential relation between facing ply drop and delamination is clarified. Virtual Crack Closure Technique (VCCT) based delamination growth analysis is applied in numerical study. Pristine sandwich and ply drop sandwich with face/core interface crack are tested. After previous simulation results are compared with experimental results, a new simu- lation method combining VCCT and post-buckling delamination analysis is established. However the numerical simulations do not match with experiments well and need to be improved.","","en","master thesis","","","","","","","","2021-08-23","Aerospace Engineering","Aerospace Structures & Materials","","EWEM European Wind Energy Master","",""
"uuid:43f7c868-dade-4ace-b76d-5f6d93ebfa2b","http://resolver.tudelft.nl/uuid:43f7c868-dade-4ace-b76d-5f6d93ebfa2b","Towards responsible governance of energy projects: A dynamic analysis of values, interactions and events in the decision-making process on shale gas in Lancashire, UK","Galeano Galván, M.L.","van Eeten, M.J.G. (mentor); Cuppen, E.H.W.J. (mentor); Ravesteijn, W. (mentor)","2016","Research Problem Energy projects are developed and executed in a complex network of interconnected actors, whom depend on each other’s resources for reaching their individual interests and goals with regard to the project. These actors have a diversity of values (convictions or beliefs of what is worth striving for society to be good), perspectives and goals. Energy projects can give rise to controversies due to the different perceptions of actors regarding the project and its implementation process (the institutions in which the technology is embedded). Whereas controversies may be perceived as barriers for the implementation of energy projects, they can also provide opportunities for the articulation of conflicting values. In that light, the notion of responsible innovation can add a normative dimension to the governance of energy projects. Responsible innovation endorses the inclusion of the diversity of relevant public values to the development and implementation of energy projects. Hence, the responsible governance of energy projects implies creating strategies and solutions to accommodate the variety of values at stake. For values to be accommodated in energy projects, they first need to be identified. The public debate can be used as a mean for the identification of relevant public values. However, this may prove challenging. The implementation of energy projects is a dynamic process, consisting of a series of intertwined decisions, involving different groups of actors at different decision-making times and places. The multiplicity of decisions lead actors to express different values or conceptualizations of the same value according to the topic under discussion. Hence, beyond methodological challenges, issues of power and agenda setting may lead to the contestation of the legitimacy of the identified values. The politics involved in the decision-making process may influence how values are articulated and when. Power imbalances may lead to emphasis on the values of powerful actors in the public debate. In addition, the process of agenda setting within arenas may encourage the expression of the values that “fit” the topic under discussion, while others remain hidden. Hence, the most frequently expressed values might not reflect the most relevant values from a democratic perspective. In fact, if the expression of values depends on specific groups of actors interacting at particular times and places, the legitimacy of the identified values might be contested. The multiplicity of interactions in energy projects is reflected in different aspects of the decision-making process, such as the locations of decision-making (arenas), the degree of coordination between actors with similar goals (coalitions) and the interventions aimed at steering the process in desired directions (strategies). Therefore, this thesis focused on exploring how these aspects of the decision-making process shaped the expression of values in the public debate – the rhetoric use of values. Case study: shale gas exploration in Lancashire In that light, a case study was deemed necessary to facilitate an in-depth exploration of the dynamics of the decision-making process. The exploration of shale gas in Lancashire, UK was selected for several reasons. The occurrence of two earth tremors in 2011, related to the first fracking well built in Lancashire, led to a moratorium of the technique that lasted 18 months. Afterwards, the safe development of fracking was supported through the development of a proper institutional environment. The exploration activities were reactivated by Cuadrilla’s proposal to develop the sites at Roseacre Woods and Preston New Road in Lancashire. This decision started a formal decision-making process surrounded by uncertainties and controversy. The uncertainties were related to the potential benefits and impacts of fracking in the environment, surrounding communities and the economy. The controversy was raised by the multitude of perspectives over if and how to implement the technique. Due to this combination of characteristics, this case was found suitable to analyse the research problem at hand. Hence, this report aimed at answering the following research question: How has the rhetoric use of values been shaped by the arenas, coalitions and strategies in the decision-making process on shale gas in Lancashire, UK? Research Methodology Based on the research question, the research was divided into two parts based on the theoretical and empirical needs. First, the theoretical research was done by means of a literature review of the theories of decision-making in networks and VSD. Second, the empirical research was executed by means of a qualitative longitudinal analysis of newspaper articles available regarding the decision-making process in Lancashire, UK. As this method generates a large amount of data, a focus was kept on the aims of the research to select the data for analysis. Conclusion The rhetoric use of values refers to the expression of values in the debate. The dynamics of the decision-making process might shape the expression of values in three ways. First, strategies may act as triggers for actors to highlight specific values or conceptualizations of values. Second, arenas constrained the expression of (conceptualizations of) values according to the topic under discussion. In addition, not all actors can participate in the different arenas, which might limit their participation in the articulation of values of the different arenas. Finally, coalitions acted as platforms for actors to express the different conceptions of values through the execution of join actions.","Energy transitions; Controversies; Rhetoric use of value; Arenas; Coalitions; Strategies; Decision-making process","en","master thesis","","","","","","","","","Technology, Policy and Management","Policy, Organisation, Law & Gaming","","Engineering and Policy Analysis","",""
"uuid:8b1b2072-4609-4acc-a02e-f925cd6dc205","http://resolver.tudelft.nl/uuid:8b1b2072-4609-4acc-a02e-f925cd6dc205","The Effects of Oil on CO2 Foam Flooding under Miscible Conditions","Li, K.","van Kruijsdijk, C.P.J.W. (mentor); Vincent-Bonnieu, S. (mentor); Kahrobaei, S.S. (mentor)","2016","Miscible gas injection, as an Enhanced Oil Recovery (EOR) method, can improve the recovery factor by swelling the residual oil and phasing out the interfacial tension. However, the sweep efficiency of miscible gas injection is not satisfactory because of conformance problems, such as viscous fingering, channelling and gravity override. Foam can solve the problems associated with gas injection by reducing the mobility of gas and therefore sustaining a stable displacement front. It is known that under immiscible conditions, the presence of oil can damage foam stability through different mechanisms. However, it is not fully understood how interactions between gas and oil under miscible conditions may affect foam flow behaviour. In this study, the effects of oil on foam flow behaviour under miscible conditions are investigated. To this end, several foam flooding experiments have been conducted using CO2 and decane, as a model gas and a model oil, respectively, under miscible conditions. These core-flooding experiments have been conducted with different molar fractions of CO2 and decane. Through these experiments, it has been found that both supercritical CO2 foam and decane emulsion display a low-quality regime and a high-quality regime, whereas, in the presence of oil under miscible conditions, CO2-decane-surfactant floods do not show low and high-quality regimes. Based on CO2 molar fraction in CO2-decane mixture and its effects on foam behaviour, three different regimes have been observed. Regime 1 (CO2 molar fraction in CO2-decane mixture is larger than 0.8) has the highest apparent viscosity among the three regimes. In regime 2 (CO2 molar fraction in CO2-decane mixture is below 0.2), the apparent viscosity is still high, although lower than that of regime 1. Finally, within regime 3 (CO2 molar fraction in CO2-decane mixture ranges from 0.2 to 0.8), the apparent viscosity is the lowest (around 50 cP) among the three regimes and does not experience significant changes. Furthermore, through a set of shear-thinning experiments, it has been shown that both supercritical CO2 foam and decane emulsion, as well as CO2-decane-surfactant floods in regime 3, all exhibit shear-thinning rheology. Moreover, in regime 1 and regime 2, there is a transition at shear rates from 10 s-1 to 100 s-1, where the apparent viscosity is increased by one order of magnitude. In regime 3, however, the apparent viscosity is at its largest value already (up to 560 cP) when shear rate is even lower than 20 s-1. Finally, in order to examine the capability of the STARS foam model to capture the oil effects on foam behaviour under miscible conditions, the experimental foam-scan data has been used to estimate the STARS foam model parameters. It has been found that the STARS model is not able to capture the oil effects on foam behaviour under miscible conditions.","CO2; Foam; Decane; Emulsion; Miscibility","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Applied Earth Science","",""
"uuid:9a1c2d76-13ec-4675-8541-150bb7ba41b5","http://resolver.tudelft.nl/uuid:9a1c2d76-13ec-4675-8541-150bb7ba41b5","Verification and validation of a spray water model for maritime structures","Eikelboom, H.","Uijttewaal, W.S.J. (mentor); Aalbers, A.B. (mentor); Hofland, B. (mentor); Hoving, J.S. (mentor); Wenneker, I. (mentor)","2016","Water spray is a major contributor to icing of ships and maritime structures. The small size of the droplets in the water spray causes them to easily cool down and freeze to the different structures. Icing can hamper operations on both ships and other maritime structures due to blockage of equipment and slippery deck areas. Besides, if icing occurs on a small ship it is even possible that the increased mass causes the ship to become top heavy, with risk of capsize. De-icing can be costly, dangerous and often both. A better understanding of the spray water forming process can be used to design new structures for minimal ice forming. Reducing the icing on existing structures is also possible by small modifications or optimizing the sailing routes for ships. This in turn can lead to lower operating costs and down-time. In this research the objective was to validate the model of Aalbers en Poen (2015) for the prediction of spray water, in particular the prediction of the droplet mass median diameter. In this model it is assumed that the process of a wave running up against the bow of a ship is comparable to the penetration of water by a wedge. Such a process generates a jet which in turn is expected to breakup in a similar fashion as a jet from a splash plate nozzle. Aalbers and Poen use wave mechanics to determine the immersion velocity of the ship and the dead rise angle between the water and the ship. This information can be used to predict the jet root thickness and the jet velocity with the model of Faltinsen (2002). Both parameters are used as the input for a modified model of Adams (1997) to predict the droplet mass median diameter. Simmons (1977) proposed a distribution that can depict the droplet-size/volume-fraction independent of the spray mechanism. Therefore Aalbers and Poen assumed that this distribution can be applied to their problem. For the validation and verification process tests, for this thesis, have been conducted in the concept basin of MARIN. In the tests a wave was used to produce run-up against a wall to create spray. The waves used had the maximum height possible in the basin and were just feasible to check the model of Aalbers and Poen as intended. This size is significantly larger than that of e.g the splash plates in the original models but still not at full (ship) scale. The tests were performed to study the breakup process and in particular the size and distribution of the formed droplets. An algorithm was developed to extract this data from high-speed recordings of the event. The observed breakup process was different from the process assumed in the model of Adams. Nevertheless, the observed and predicted values for the droplet mass median diameter and jet velocity agree reasonably well. The distribution of Simmons was shown to be applicable. The test setup could not fully verify the usage of Faltinsen's theory because the jet root thickness could not be quantified. However, the observations showed that the assumption of Aalbers and Poen to use Faltinsens's theory is promising.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","","","","",""
"uuid:681a1aee-f2df-4477-bf66-523c6204d5c1","http://resolver.tudelft.nl/uuid:681a1aee-f2df-4477-bf66-523c6204d5c1","Opportunistic Routing for Indoor Energy Harvesting Wireless Sensor Networks","Garrido Barrabes, E.","Venkatesha Prasad, R.R. (mentor)","2016","Internet of Things (IoTs) is envisioned to enable smart spaces such as smart homes, interactive museums or personalized trade in shopping malls. In these smart homes, several sensors and actuators assist in automating tasks of daily life by forming wireless sensor networks (WSNs). Active and Assisted Living (AAL) is one of the important applications of smart homes, wherein activities of elderly persons are mainly monitored and assist them through actuators when required. With a huge number of sensors involved in AAL applications across smart homes, powering the nodes is an important issue. To this end, make use of ambient energy harvesting technologies for enabling perpetual operations of the WSNs. Due to limited energy harvesting opportunities in indoor environments, the energy levels of nodes in the WSN varies over space and time. Therefore, collecting data reliably over such a network is a significant challenge. There are many proposals in this domain, including MAC, routing, etc., in such WSN. Although several routing protocols exist for WSNs and EHWSNs, they do not consider the limited energy availability. Consequently, they do not adapt to the conditions, therefore fail in their objective. We propose a novel routing protocol called Harvesting Energy Aware Routing with adaptive Duty Cycling (HEAR-DC), based on the philosophy of using available energy, and routing the data packets opportunistically. HEAR-DC has several components: (i) transmitter-initiated MAC with opportunistic reception and duplicity avoidance; (ii) an energy harvesting aware gradient; (iii) adaptive duty cycling; and (iv) adaptations to support mobile nodes and sparse data traffic. We also develop a trace-based energy harvesting simulation module for Cooja, which does not exist as yet. With this module, real data traces can be used for evaluating energy-harvesting WSN protocols. We present the design and implementation aspects as well as limitations of this module in the current state. We evaluate our protocol against the state-of-the-art opportunistic routing protocol ORW. Results show that our proposed protocol achieves a latency reduction of 64% with 92.75% lower packet loss as compared to ORW in the best case of high energy availability. In the worst case, HEAR-DC achieves a substantial latency reduction of 37% and packet loss reduction of 48% compared to ORW. HEAR-DC achieves almost a factor of four lower delay than ORW when only ‘events’ are transmitted.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Software Technology","","Embedded Software","",""
"uuid:fa946b9b-d044-4a75-bcea-90a40f0706d3","http://resolver.tudelft.nl/uuid:fa946b9b-d044-4a75-bcea-90a40f0706d3","Electricity Flexibility in Industries","Kalaiyarasan, K.","De Weerdt, M.M. (mentor); Boersma, B.J. (mentor)","2016","Large scale deployment of renewable energy for production of electricity have led to impact on the electricity scenario of many countries. Because of the seasonality of newly added power sources electricity infrastructure has to be flexible enough to feed in with additional power in case of deficit and consume more in case of surplus. This leads to volatility in the electricity market prices. This variability in the electricity market may have significant impact on energy intensive industries. To cope with the fluctuations in the energy market without making considerable investments in infrastructure, it is necessary to have flexibility in electricity demand. Not all industries have the scope of having flexible demand. Thus in this study a methodology has been formulated to identify industries with potential of having flexible demand. Based on the methodology two cases are identified for analysis – chlorine production and steel production using electric arc furnace. Thermodynamic models of both the cases were developed to simulate their production and power consumption behaviour. The models were validated using data from literature as well as data from production plants. The simulated scenario is used to develop a Mixed Integer Linear Programming (MILP) model to optimise the production based on electricity price in APX market. The cost of production for the optimised schedule in comparison with the minimum and maximum cost of production for the same throughput on the same day is used to calculate the electricity flexibility for that unit by exploiting the variation in the electricity market price. The range between minimum and maximum saving in operation cost is defined as electricity flexibility of that unit. Three optimisation models were developed for different cases – electrolyser cell, steel production with one production line and steel production with two lines. The models were evaluated with different utilisation levels. Operation of an unit at higher utilisation level provide relatively less freedom to shift the production loads thus resulting in lower electricity flexibility. Higher electricity flexibility is observed at low utilisation levels due to increased possibilities of moving production loads to low electricity price durations. The steel production optimisation model was used to predict cost savings in an actual steel plant and the results were used to study the accuracy of the model. In the actual plant a saving of 4% is possible by optimising the production demand at an higher utilisation level and at a lower utilisation level savings of 14.8% is possible without considering raw material cost. Accuracy of the optimisation model was verified by comparing the calculated flexibility with the actual flexibility from data. Inputs of the optimisation model were analysed further to identify possibilities of improving accuracy of the model. More accurate representation of power consumption value in optimisation model gives more accurate flexibility values. From the research it can be concluded that one of the least expensive and effective ways to deal with volatile electricity market is demand side management in the industries where there is considerable potential.","Electricity flexibility; thermodynamic models; MILP; steel production","en","master thesis","","","","","","","","2021-04-22","Mechanical, Maritime and Materials Engineering","Process and Energy","","","",""
"uuid:ff0392a9-b49d-4471-8ccb-6ee7030322a8","http://resolver.tudelft.nl/uuid:ff0392a9-b49d-4471-8ccb-6ee7030322a8","Optimising the throughput for an integrated urban-freeway network: Design of an MPC controller using variable speed limits and intersection control","van der Molen, M.W.J.","Hegyi, A. (mentor); van de Weg, G.S. (mentor); Hoogendoorn, S.P. (mentor)","2016","In this MSc Thesis project, a Model Predictive Controller (MPC controller) using variable speed limits and intersection control was designed and subsequently evaluated. The design was based on combining existing control algorithms for urban and freeway networks, integrating them. While this MPC controller improved upon throughput more than MPC controllers that only used variable speed limits or intersection control, the computation time turned out to be too long for real time control and the qualitative behaviour was dubious (the controller used multiple variable speed limit areas instead of one as one would expect). More research into methods to improve upon computation time and into the behaviour has been recommended to deal with these problems.","Traffic control algorithms; Model Predictive Control (MPC); intersection control; variable speed limits; throughput; total time spent; traffic flow models; MATLAB simulation; computation time","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport & Planning","","","",""
"uuid:d5cb223a-4d32-4a4a-bde1-bbadc9420e5c","http://resolver.tudelft.nl/uuid:d5cb223a-4d32-4a4a-bde1-bbadc9420e5c","The Vulnerability Ecosystem: Exploring vulnerability discovery and the resulting cyberattacks through agent-based modelling","Breukers, Y.P.","van Eeten, M.J.G. (mentor); Pieters, W. (mentor); Nikolic, I. (mentor); van Wieren, M. (mentor)","2016","Software vulnerabilities are a major enabler for cyberattacks, and are therefore responsible for a sizeable portion of the risk for large organisations in cyberspace. Little research has been done on the dynamics underlying this risk. The goal of this paper is to find how organisations and software vendors can influence the overall risk level as a result of vulnerabilities. An agent-based model is developed to capture the behaviour and complexity of the vulnerability ecosystem. Somewhat counter intuitively, the model shows that not necessarily the number of vulnerabilities, but rather the diffusion of the knowledge on vulnerabilities, has a strong influence on the resulting attacks. Instead of trying to fix all vulnerabilities, focussing patching efforts on vulnerabilities that are actually being exploited can significantly reduce the risk of organisations. The model further showed that organisations should improve their own efforts to deploy patches on shorter notice to narrow down the window of vulnerability.","vulnerability discovery; cyberattacks; risk exposure; agent-based modelling; patching","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi-actor Systems","","","",""
"uuid:e140aeab-891d-437d-b96d-8a5c1ed9d94a","http://resolver.tudelft.nl/uuid:e140aeab-891d-437d-b96d-8a5c1ed9d94a","Quantitative Assessment of Adhesively-bonded Joints Using Ultrasonic Guided Waves","Verze, M.","Kassapoglou, C. (mentor)","2016","Maintenance is a costly but vital part of an aircraft lifespan. Regulations demand scheduled and rigorous inspection to ensure safety and airworthiness in the long term. Studies estimate that up to a third of the total operational cost of an aircraft is spent in maintenance checks. Consequently, design choices are made to take into account these requirements, which mean that conservative but easy to maintain designs are often preferred. Adhesive bonding is a typical example of a design choice that is overlooked due to the lack of efficient and reliable non-destructive maintenance and inspection. The mechanical advantages of adhesive bonding make it a promising substitute to mechanical joining methods but a consistent lack of confidence in the long term behaviour of these joints is limiting its broad application. Available NDI methodologies are limited both in reliability and versatility, and weak bonds can still go undetected. A guided waves based method has been developed and used to inspect a set of different adhesive interfaces in especially manufactured specimens. The method estimates an equivalent bondline length which can be directly correlated to the residual strength of the inspected bond. The tests showed good correlation even in cases where conventional ultrasonic methods (C-scan) were unable to detect any defects. The proposed method promises to further enhance the detection of weak bonds in bonded joints providing a rapid and accurate evaluation and direct correlation with mechanical strength of inspected bonds.","C. thesis; msc; adhesive; Lamb waves; inspection; guided waves","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures & Materials","","Aerospace Structures & Computational Mechanics","",""
"uuid:0292b82d-4c15-4475-9cd8-3ab3a5c8d55c","http://resolver.tudelft.nl/uuid:0292b82d-4c15-4475-9cd8-3ab3a5c8d55c","Customer analytics maturity of Dutch SME finance banks: Designing a maturity assessment framework for customer analytics implementation at the Small and Medium-sized Enterprises finance banks in the Netherlands","Shanina, S.","Janssen, M.F.W.H.A. (mentor); Herdeiro Teixeira, A.M. (mentor); van der Voort, H.G. (mentor)","2016","Big data has become an important issue for the SME financial sector. The amount of data stored by banks is expanding and growing fast. These trends provide SME finance banks with a huge opportunity to enhance their businesses. Combining predictive analytics with automatic decision making, popularly known as customer analytics, makes it possible for the SME finance bank to understand their SME clients. In the Netherlands there is a lack of adoption regarding the use of customer analytics, what could be explained by an immaturity of banks. This paper aims at designing a maturity assessment framework for customer analytics implementation for the SME finance banks in the Netherlands. Aiming at assessing the maturity of SME finance banks at different domains, regulation, organization, technology and governance. This has not been done before. A framework is proposed by following the systems engineering design approach. Technology, data governance, organization and regulation are the main domains of the framework. The framework is designed according to the Capability Maturity Model Integration theory and is able to assess both technical and organizational components. However ,the framework should be expanded and more elements should be included into the framework. This enhances the assessment of maturity at Dutch SME finance banks.","Customer analytics; Finance and data-analytics; Big Data; Maturity assessment; SME Finance; Data governance; Data management; Maturity","en","master thesis","","","","","","","","2017-08-22","Technology, Policy and Management","Systems Engineering, Policy Analysis and Management (SEPAM)","","","",""
"uuid:3ecee314-309b-4254-ab0f-34be7da90d9d","http://resolver.tudelft.nl/uuid:3ecee314-309b-4254-ab0f-34be7da90d9d","Effect of temperature on fatigue crack growth in adhesive bonds","Usman, M.","Alderliesten, R.C. (mentor); Pascoe, J.A. (mentor)","2016","In this research effect of temperature on the disbond growth rate due to fatigue load in adhesive bonds is investigated. Concept of release of strain energy has been used. In particular the effect of temperature on the relationship between disbond growth rate and strain energy released per load cycle has been investigated. The general trend observed was the increase in disbond growth rate with the increase in temperature. This effect has been quantified and a simple model for the effect of temperature on the disbond growth rate has been developed using the concept of release of strain energy per cycle.","fatigue crack growth rate; adhesive bonds; energy method; effect of temperature","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures and Materials","","","",""
"uuid:dcab28d6-f133-47d9-b552-967eed29892f","http://resolver.tudelft.nl/uuid:dcab28d6-f133-47d9-b552-967eed29892f","Tipping Points in Community-Based Management: A design-science research approach to study Community-based Management of domestic rural water points in Sub-Saharan Africa","Nava Guerrero, G.D.C.","Herder, P.M. (mentor); Slinger, J.H. (mentor); Nikolic, I. (mentor); Annema, J.A. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","","",""
"uuid:e225decc-1acb-4e0d-ab28-11bbab434788","http://resolver.tudelft.nl/uuid:e225decc-1acb-4e0d-ab28-11bbab434788","Exploiting the Reconfigurability of ρ-VEX Processor for Real-Time Robotic Applications","Yousaf, M.M.","Wong, J.S.S.M. (mentor)","2016","Autonomous mobile robots generally have limited computational power on-board, and they have to perform their tasks in real-time in order to interact with their surroundings effectively. Therefore, there is a need to utilize the available computational capabilities efficiently. The ρ-VEX is a run-time reconfigurable VLIW processor. This unique processor allows separation of its issue lanes to form independently operating processing cores. Switching between these configurations during run-time allows optimizing the computing resources for the task(s) it is performing. In this project FreeRTOS is ported to the ρ-VEX processor and a control layer is developed. FreeRTOS manages the applications based on given real time parameters. The control layer decides the number of active cores (hardware contexts) and issue width of each core to best match the processing requirements of the applications. In this way, FreeRTOS and the control layer together can reconfigure the number of active cores at run-time. This is a very unique feature of this thesis project and can not be found in any other multicore implementation of FreeRTOS. The control layer along with FreeRTOS provides the user a facility to run applications under real-time constraints and with the best possible efficiency. In order to evaluate the performance, the overhead of the FreeRTOS is quantified and a performance comparison is made between several configurations of this system. Moreover, impact of reconfigurability of the ρ-VEX on the schedulability of real-time applications is measured and it is concluded that (indeed) reconfigurability of ρ-VEX improves the schedulability of the real-time applications","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Computer Engineering","","","",""
"uuid:a9775955-7427-4815-91a3-f4dd5938647c","http://resolver.tudelft.nl/uuid:a9775955-7427-4815-91a3-f4dd5938647c","Great Local Products 'Within Reach': Company positioning and mobile application design","van Ramshorst, C.G.","de Lille, C.S.H. (mentor); van der Vorst, R.R.R. (mentor); Mooij, S.C. (mentor)","2016","Great local products ‘within reach’, the title of this report, represents both the proposition of Rechtstreex as well as it is a play on words, with the final design of this report being a mobile application. This abstract provides a short summary, describing the problem situation, activities carried out, results found and an outline of the final design. Rechtstreex is looking to expand their business, while at the same time trying to become a financially healthy organisation. During initial research within the Rechtstreex context, the problem for these two goals is a lack of growth. With this problem situation identified, analysis can be done to understand the origin of this problem situation. The origin of this lack of growth consists of several factors. Rechtstreex apparently has a high churn, which means losing a significant amount of customers to competitors. Next to a high churn, Rechtstreex does not have a concrete positioning which also has an influence on the employees. The employees do not show a future-oriented mindset, and are reluctant to make hard decisions. Confusion is present within the organisation in terms of direction, resulting in only achieving very incremental improvements. Two projects have been carried out in order to develop a solution for this lack of growth. By means of interviewing, workshops and discussions, an inventarisation is created of the various beliefs about Rechtstreex. Based on these results, a fitting positioning statement has been developed for Rechtstreex. This positioning statement is relevant for the current state of Rechtstreex, and serves as a foundation for a future-oriented positioning. Therefore, the positioning is a first step in the process of becoming more future-oriented as an organisation, creating the first step for sustainable growth. Next to the positioning statement itself, the process as a whole has created awareness among employees in terms of the necessity for future-oriented behaviour. The second part of this project has resulted in the development of a mobile application for Rechtstreex. Research supports the potential effect on engagement and growth that can be achieved through a mobile application for businesses. Application functionality consists of a store for buying products, shopping list, push messages, feedback options, browsing the various producers, recipe inspiration and a groceries planner. This list of functionality was created through internal evaluation of an initial application design, followed by evaluation through a panel of Rechtstreex users. Based on these insights, a development roadmap is created to guide the further development process of the application. Mobile application functions were mainly assessed on their effect on financial revenue, effect on non-financial revenue and impact on social behaviour among Rechtstreex consumers and the Rechtstreex food system. In short, the application was developed focussing on brand fit with regards to the positioning statement, as well as potential sales increase. Referring back to the problem situation, achieving growth, this combination of a positioning statement and mobile application create a solution which creates short term financial growth, growth in commitment to the Rechtstreex food system, and paves the way for sustainable future growth.","E-Groceries; Marketing; Positioning; Mobile Application Design","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Product Innovatie Management (PIM)","","","",""
"uuid:02149e5a-b654-4047-8943-90570e7e67a9","http://resolver.tudelft.nl/uuid:02149e5a-b654-4047-8943-90570e7e67a9","Numerical Reconstruction of Fundamental Solutions of the Stokes System with Finite Elements","Snoeijer, J.","Rüde, U. (mentor)","2016","","","en","master thesis","","","","","","","","2019-08-22","Electrical Engineering, Mathematics and Computer Science","Applied Mathematics","","EM-COSSE","",""
"uuid:75e87ffd-46d3-4bfa-bc47-1e8ef4232ca9","http://resolver.tudelft.nl/uuid:75e87ffd-46d3-4bfa-bc47-1e8ef4232ca9","Understanding e-shopping: Analysis of ICT relation with shopping and shopping mobility behavior","Nicolás Marmisa, P.","Kroesen, M. (mentor); Chorus, C.G. (mentor)","2016","E-shopping might affect in-store shopping in different ways, substituting physical purchases, which is called substitution effect, or on the contrary encouraging more of them, which is referred to as complementarity. Previous studies suggest that the latter is the dominant effect, but that both coexist within the population. Disentangling the shopping behaviour and mobility within the population is the first step to study if this is the case. This research, based on data from the Netherlands Mobility Panel (MPN), performs a latent class analysis in order to identify different profiles of e-shopping/in-store shopping behaviour. The research identifies seven profiles of e-shoppers: typical shoppers, reluctant e-shoppers, occasional shoppers, reluctant in-store shoppers, store regulars, active shoppers and remote shoppers. 47% of the population presents an above average e-shopping frequency and for 30%, e-shopping seems to be the main shopping activity. Nevertheless, with regard to shopping mobility, the differences between the clusters are rather small and not significant for the most part. Therefore, e-shopping does not seem to have a significant reduction effect on shopper mobility, which was one of the considerations of policy makers when e-shopping began to develop.","","en","master thesis","","","","","","","","","Technology, Policy and Management","Transport and Logistics","","","",""
"uuid:55eb51c7-e2b4-4a98-ba85-bb725c03b721","http://resolver.tudelft.nl/uuid:55eb51c7-e2b4-4a98-ba85-bb725c03b721","A study into the joint activity-route choice behaviour of pedestrians during large scale events, based on revealed preference data: SAIL event, case study","Iliadi, A.","Hoogendoorn, S.P. (mentor); Daamen, W. (mentor); van der Spek, S.C. (mentor); Wiggenraad, P.B.L. (mentor)","2016","Nowadays, there is an increasing number of large scale events and a growing number of people tend to participate to these events. In general, during large scale events the combination of the lack of sufficient information to support crowd management with the movements of the crowd can create not only problems on the undisturbed movements of pedestrians but also potentially dangerous situations. Despite the severity of these problematic situations, still there is limited knowledge about the behaviour of pedestrians during mass events which could support the development of a sufficient crowd management strategy. Therefore the objective of this thesis is to analyse pedestrians’ activity and route choice behaviour at mass events and examine how pedestrians make these choices using SAIL event as a case study. The analysis of pedestrians’ activity and route choice is based on Revealed Preference (RP) data.","joint activity- route choice behaviour; pedestrians; large scale events; revealed preference data; GPS; MNL; discrete choice; SAIL event","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport & Planning","","","",""
"uuid:d1d6f874-abc1-4977-8d4e-4b98d3db8265","http://resolver.tudelft.nl/uuid:d1d6f874-abc1-4977-8d4e-4b98d3db8265","The impact of digital transformation","Tolboom, I.H.","Tan, Y. (mentor); Klievink, A.J. (mentor); de Reuver, G.A. (mentor)","2016","Digital Transformation, Digital Technologies, Social, Mobile, Analytics, Cloud, Organizational Effects, Business Models, Business Model Canvas, Data-analysis, Survey. Many organizations nowadays feel pressured to change in order to meet customer demands and competitive pressure due to emerging digital technologies. When a combination of digital technologies is used a common heard buzzword is digital transformation. To date there only little academic research with regards to digital transformation and they often provide holistic non-deterministic definitions. In addition literature that dives deeper into the underlying technologies and effects only address specific elements or industries. This gap is address through a literature research and a business model based questionnaire held amongst IT consultants to determine both the concept and expected effects of digital transformation. We speak of digital transformation if a technology induced change by social, mobile, analytics or cloud technologies significantly impacts a minimal of three out of seven dimension on an individual, firm or social level. The research findings show that digital transformation is expected to change organizations across many different fronts as almost all business model constructs have at least one element that is expected to change moderately or even stronger. The greatest impact will be to organizations’ their value proposition, the customer segments they can identify and serve, the way organizations reach their customers, and the resources they use.","Digital Transformation; Digital Technologies; Social; Mobile; Analytics; Cloud; Organizational Effects; Business Models; Business Model Canvas; Data-analysis; Survey","en","master thesis","","","","","","","","2016-08-23","Technology, Policy and Management","System Engineering, Policy analysis and Management","","","",""
"uuid:b15b8459-0538-4ccf-8b73-976072f38d24","http://resolver.tudelft.nl/uuid:b15b8459-0538-4ccf-8b73-976072f38d24","Development of Climate Resilient Ports: Achieving Viable and Efficient Investments in Landlord Container Terminals","Nugroho, E.S.","Enserink, B. (mentor); Slinger, J.H.S. (mentor); Storm, S.T.H. (mentor); van Dorsser, J.C.M. (mentor); Altamirano, M.A. (mentor); de Jong, M.P.C. (mentor)","2016","Being nodal points in global supply chain network, port operations disrupted by adverse climate change impacts would bear substantial costs. Therefore, adapting ports to climate change and building their climate resilience are essential. However, the needs may not have been adequately acknowledged by port stakeholders. A series of problem explorations at the start of the study identified three barriers that have hindered them to sufficiently adapt their ports to climate change. The barriers are (1) unavailability of an effective general best practice of climate adaptation for ports, (2) uncertainty regarding the viability and efficiency of climate adaptation investments in ports and (3) unclarity about which port stakeholders are responsible for financing the adaptation. To contribute in addressing the barriers, using the landlord container terminal as a unit of analysis, this thesis explored the following research question: “Under what conditions and how can viable and efficient investments in climate resilient ports be achieved?”. An extensive literature review of climate risks, opportunities and adaptation in ports was conducted to identify the observed and potential impacts of climate change on container terminals, as well as the currently available adaptation measures for them. Then, an assessment framework for approximating the financially viable and efficient climate adaptation option in a port was constructed by integrating Weather Value at Risk and Real Options Analysis methods. Afterwards, the possible strategies for allocating responsibilities for developing climate resilient container terminals among port stakeholders were recognized by reviewing (1) the existing contractual protections against climate risks in landlord container terminal partnerships and (2) the barriers to incorporate effective allocation of climate risks into the partnerships. Moreover, as an answer to the research question, seven climate risk management practices in ports and six recommended actions for port stakeholders are presented.","climate change; climate adaptation; port; climate finance; Weather Value at Risk; Real Options Analysis; climate risk allocation; port partnerships","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi Actor Systems","","","",""
"uuid:832fb247-3f7e-4a52-b793-a1b7677903fd","http://resolver.tudelft.nl/uuid:832fb247-3f7e-4a52-b793-a1b7677903fd","Three dimensional Finite Element Modeling of a Brushless Doubly-fed Induction Machine","Wani, F.M.","Polinder, H. (mentor); Nilssen, R. (mentor)","2016","As we move deeper into offshore waters to harness more of wind energy there is a need to keep the total operational and maintenance costs to a minimum, of which expenditures related to the generators form a major share. Brushless Doubly-fed Induction machine used as a generator, due to the requirement of a lower rating converter, and the absence of brush-gear and slip-ring arrangement can help in mitigating those costs. 2D models of the BDFIM have been developed to arrive at a better design so that the machine is able to compete with and possibly, outperform other solutions in the market. However, much like 2D models of the other machines, these models usually neglect or at best approximate certain practical aspects. These aspects mostly include effects arising due to physical features such as end windings, laminated structure of the iron, skew in the conductors, etc. 3D modeling can allow us to more accurately model these effects and perhaps, result in an even more efficient design. However, there are some caveats in this approach. 3D models, as expected, are mathematically more complex to formulate and solve. Besides, there is a multifold increase in the number of unknowns to be solved for which poses a significant challenge in terms of the memory requirement and the time cost of the model. The methods adopted to solve 2D problems hitherto, magnetostatic as well as transient, are not suitable to solve large scale 3D models. The objective of this thesis is primarily to present a strategy to efficiently solve such 3D problems. This approach is then extended to estimate the axial flux in the BDFIM and to ascertain the validity of the 2D multi-slice model. In this regard, different magnetic potential formulations were considered, and the mathematical properties of the matrix system resulting from each formulation were outlined. Special emphasis is placed on the implementation of iterative solvers and multigrid preconditioning. Also, it will be demonstrated that a formulation resulting in fewer degrees of freedom does not necessarily translate into a memory and/or time efficient model.","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","DC Systems, Energy Conversion & Storage","European Wind Energy Masters EWEM",""
"uuid:bc079a36-45f6-4b84-934a-786bf03e4a08","http://resolver.tudelft.nl/uuid:bc079a36-45f6-4b84-934a-786bf03e4a08","Organizing servitization: Developing a framework to guide the transition for manufacturing firms","Lobert, L.","van der Duin, P.A. (mentor)","2016","Since the early 1990s an increasing number of manufacturing firms such as IBM, Xerox and MAN are adding services to their products to meet the competitive pressures of product commoditization, stagnating sales and more demanding customers. This transition process is referred to as servitization. It appears that many manufacturing firms are struggling to implement a servitization strategy to attain their strategic goals since they lack insights into the strategic and organizational aspects of this transition. A framework that integrates these aspects can present the currently lacking comprehensive overview of servitization for manufacturing firms. This framework offers a guideline to successfully carry out servitization. Each transition phase can be described according to the following strategic dimensions: type of offering, type of services, type of customer, customer relationship and innovation orientation. Each phase also aligns with a certain organizational structure, since misalignment is a barrier to successful servitization for manufacturing firms. Proceeding in servitization therefore involves an organizational transition. Manufacturing firms move from a product-centric to first a more service- and then a customer-focused structure that integrates product and service activities, to fulfil the strategic goal to become a solution provider.","servitization; transition process; organizational structure; manufacturing firm; framework development","en","master thesis","","","","","","","","","Technology, Policy and Management","Values Technology and Innovation","","Economics of Technology and Innovation (ETI)","",""
"uuid:d41b297e-3194-4d36-97bb-603a949f97f4","http://resolver.tudelft.nl/uuid:d41b297e-3194-4d36-97bb-603a949f97f4","Requirements on and Antecedents of Big Data Quality: An Empirical Examination to Improve Big Data Quality in Financial Service Organizations","Fardani Haryadi, A.","Janssen, M.F.W.H.A. (mentor); Hulstijn, J. (mentor); van der Voort, H.G. (mentor)","2016","Big data has been widely known for its enormous potential in various industries, including finance. However, more than half of financial service organizations reported that big data has not delivered their expected value, yet. Among all, data quality is one of the issues behind this phenomenon, and thereby being the focus of this study. The objective of this research project is to develop key requirements for a reference architecture that improves big data quality in financial institutions. A joint approach between Requirement Engineering and Data Quality Management frameworks is performed to yield the desired requirements. Data collecting method through three case studies, seven content analysis, and literature review are performed to obtain the most comprehensive result. Overall findings indicate that antecedents of big data quality consist of data, technology, people, process and procedure, organization, and external aspects. They also encompass discovery of big data value, accessibility to data, and operationality of big data projects. Furthermore, there are six Information System requirements and two Human and Organizational requirements to improve big data quality.","big data; data quality; big data quality; antecedents; requirements; reference architecture; finance","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi-Actor Systems","","","",""
"uuid:4b4c4e4b-5e45-4166-bd2c-f35a1e495c6a","http://resolver.tudelft.nl/uuid:4b4c4e4b-5e45-4166-bd2c-f35a1e495c6a","Learning Depth from Single Monocular Images Using Stereo Supervisory Input","Paquim, J.","de Croon, G.C.H.E. (mentor)","2016","Stereo vision systems are often employed in robotics as a means for obstacle avoidance and navigation. These systems have inherent depth-sensing limitations, with significant problems in occluded and untextured regions, leading to sparse depth maps. We propose using a monocular depth estimation algorithm to tackle these problems, in a Self-Supervised Learning (SSL) framework. The algorithm learns online from the sparse depth map generated by a stereo vision system, producing a dense depth map. The algorithm is designed to be computationally efficient, for implementation onboard resource-constrained mobile robots and unmanned aerial vehicles. Within that context, it can be used to provide both reliability against a stereo camera failure, as well as more accurate depth perception, by filling in missing depth information, in occluded and low texture regions. This in turn allows the use of more efficient sparse stereo vision algorithms. We test the algorithm offline on a new, high resolution, stereo dataset, of scenes shot in indoor environments, and processed using both sparse and dense stereo matching algorithms. It is shown that the algorithm’s performance doesn’t deteriorate, and in fact sometimes improves, when learning only from sparse, high confidence regions rather than from the computationally expensive, dense, occlusion-filled and highly post-processed dense depth maps. This makes the approach very promising for self- supervised learning on autonomous robots.","","en","master thesis","","","","","","","","","Aerospace Engineering","Control and Operations","","Control & Simulation","",""
"uuid:f3eedfa0-65a9-458f-8da6-574fb930381b","http://resolver.tudelft.nl/uuid:f3eedfa0-65a9-458f-8da6-574fb930381b","Dissipation in the Abelian Sandpile Model","Jongbloed, H.","Redig, F.H.J. (mentor); Thijssen, J.M. (mentor)","2016","The Abelian Sandpile model was originally introduced by Bak, Tang and Wiesenfeld in 1987 as a paradigm for self-organized criticality. In this thesis, we study a variant of this model, both from the point of view of mathematics, as well as from the point of view of physics. The effect of dissipation and creation of mass is investigated. By linking the avalanche dynamics of the infinite-volume sandpile model to random walks, we derive some criteria on the amount of dissipation and creation of mass in order for the model to be critical or non-critical. As an example we prove that a finite amount of conservative sites on a totally dissipative lattice is not critical, and more generally, if the distance to a dissipative site is uniformly bounded from above, then the model is not critical. We apply also applied a renormalisation method to the model in order to deduce its critical exponents and to determine whether a constant bulk dissipation destroys critical behaviour. Numerical simulations and a statistical analysis are performed to estimate critical exponents. Finally, we give a short discussion on self-organized criticality.","Abelian Sandpile Model; Bak-Tang-Wiesenfeld model; Self-organized criticality; dissipation; renormalization; random walks; Feynman-Kac formula; Fortran","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","","","Double Bachelor Applied Mathematics/Applied Physics","",""
"uuid:acf4b7fd-619c-47f5-8eca-099daa6aa644","http://resolver.tudelft.nl/uuid:acf4b7fd-619c-47f5-8eca-099daa6aa644","Fluid Flow in Fractured Reservoirs","Asrul, F.","Rosen, W.R. (mentor); Gong, J. (mentor)","2016","Naturally fractured reservoirs (NFRs) are important because they hold a significant quantity of oil reserves. Researchers build reservoir simulators to help them studying and predicting NFR performance. Some of them use the dual-porosity concept in their reservoir simulator, where the model consists of two continua: matrix and fracture. Fluid transport between matrix and fracture is defined by a parameter which is known as the shape factor. It describes the geometry of a characteristic matrix block surrounded by fractures. Commercial reservoir simulator accounts for only one average shape factor for each simulation grid block, which is calculated from fracture spacing. However, one grid block may contain a collection of matrix blocks and variable fracture spacing. Our study objective is to evaluate whether published methods to compute an average shape factor for a collection of matrix blocks give a good approximation to recovery from that collection of blocks; we use the solution to the heat-conduction equation to determine this. Our work is based on the knowledge that the heat-conduction equation is identical to the pressure-diffusion equation under certain assumptions. Recently, a simple method to calculate the dimensionless temperature in a collection of rectangular objects of different sizes has been created. We test the methods of calculating shape factors with collections of matrix blocks that represent, at least approximately, the distribution of matrix-block shapes that Gong and Rossen (2014) found in their study of discrete fracture networks (DFN). Our work starts by converting statistics for a DFN, as reported by Gong and Rossen, into an orthogonal fracture set. We generate five realizations to approximate the DFN and simulate remaining oil in place as a function of time for these realizations. We then compare the remaining oil we calculate to that estimated using three methods in the literature: the methods of Vermeulen, Warren and Root, and Lim & Aziz. We calculate shape factor based on average fracture spacing from each realization. From this study, we can conclude that Vermeulen method, coupled with equivalent fracture spacing, can approximate oil production in a region with variable fracture spacing with an acceptable accuracy. Specifically, it stays within a few percent of cumulative recovery until about 70% of oil is recovered.","Dual Porosity; Shape Factor; Thermal Conductivity; Remaining Oil","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering","",""
"uuid:1dd8082f-f4a5-4df6-88bb-e297ed483b54","http://resolver.tudelft.nl/uuid:1dd8082f-f4a5-4df6-88bb-e297ed483b54","Techno-Economic and Environmental Analysis of Oil Crop and Forestry Residues based Biorefineries for Biojet Fuel production in Brazil","Cornelio da Silva, Constança","van der Wielen, L.A.M. (mentor); Posada, J.A. (mentor); Mussatto, S.I. (mentor)","2016","The well-known volatility of petroleum price and, subsequently, of its derived products has long time urged the attention for the discovery of alternatives to fossil oil. More recently, the dichotomy food-fuel, has redirected the research towards second generation feedstocks, in which biomass residues are included. In line with this, a consortium of Dutch and Brazilian companies and Universities – HIP – joined forces to evaluate the potential of biojet fuel production in the Brazilian supply chain for which this study contributes. This project aims to identify the most promising combinations of location/feedstock/technology/ by-products for a biojet fuel biorefinery in the Brazilian context. For this, different feedstock that are significantly available in Brazil are considered, namely wood residues (Eucalyptus, Pine), agro residues (Coffee and Rice) and oil crops (Macauba). Then, 208.5 kton of biojet per year is taken as base capacity which supplies in 10% the project demand for 2020 of both São Paulo and Rio de Janeiro airports. Furthermore, this value also corresponds to a “drop-in” fuel at 50/50 blend of biojet and conventional jet. To achieve the established goal, around 120 scenarios were generated focusing on the comparison of lignocellulosic and/or lignin valorisation processes, the latter coupled with sugar production to be used in fermentation. Therefore, three pre-treatment processes are considered (steam explosion, dilute acid and organosolv), followed by enzymatic hydrolysis; and two possible thermo-chemical processes are included for lignin upgrade to biojet fuel (fast pyrolysis and gasification Fischer-Tropsch). Regarding lignocellulosic material, its valorisation is also achieved by thermo-chemical processes; being fast pyrolysis, gasification Fischer-Tropsch and hydrothermal liquefaction the alternatives studied. Additionally, the conversion of the extracted oils into biojet is obtained via hydro-processing of esters and fatty acids which step is common to all oil crop scenarios. In general, literature data was used to support most technology related decisions. Then, the number of scenarios is refined according to feedstock availability, gross profit estimation and minimum sales price, for which a price of 605 US$/ton is assumed for the biojet produced. After this preliminary analysis, the six most promising scenarios are simulated: fast pyrolysis and hydrothermal liquefaction for Pinus, Eucalyptus and Macauba, while aiming for the introduction of more detail in the techno-economic evaluation of these scenarios. Consequently, the biojet minimum sales price for the simulated scenarios ranged from 834-1188 US$/ton, being a biorefinery in Rio Grande do Sul for the conversion of Pinus via hydrothermal liquefaction the best performing scenario. Furthermore, it is also confirmed that overall these scenarios have a lower climate change impact, 1-19 kg CO2/GJ biojet, and require less primary energy, 16-395 MJ/GJ biojet, for biojet production in comparison to its homologous from crude oil, 87.5 kg CO2/GJ jet and 1210 MJ/GJ jet, respectively.","Lignocellulosic feedstock; Biojet fuel; oil-crops; minimum selling price; GHG emissions; Brazil","en","student report","","","","","","","","","Applied Sciences","Biotechnology","","BioProcess Engineering Design","",""
"uuid:54d9a01f-0e49-4919-9625-60c4ccbebf30","http://resolver.tudelft.nl/uuid:54d9a01f-0e49-4919-9625-60c4ccbebf30","Modelling of the dynamic pressuremeter test in porous soil using analytical and numerical methods","Kaltsas, D.","Van Dalen, K.N. (mentor)","2016","The pressuremeter test is one of the most popular in-situ soil testing methods. Developed in the 1950s in France, the test has evolved throughout the past decades and was built into a cone resulting in a cone pressuremeter device (CPM). A proposed extension of the test involves time dependent pressurisation of the device resulting in wave propagation in the soil. Therefore, the dynamic behaviour can be determined. The possible applications of this extension include the response of an offshore monopole to dynamic loading and liquefaction analysis. The present research work deals with the modelling of the dynamic version of the pressuremeter test. The soil where the test was assumed to be conducted was saturated sand. Therefore, it was modelled as a porous medium, following the poroelastic theory by Biot (1956). The latter was based on the consolidation theory (Biot, 1941).","COMSOL Multiphysics; Geotechnical engineering; soil dynamics; porous media; cone pressuremeter; Biot poroelasticity; nonlinear analysis; Hyperbolic law","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","","",""
"uuid:5da89ce5-fae0-4b43-ba77-78463e90bc2e","http://resolver.tudelft.nl/uuid:5da89ce5-fae0-4b43-ba77-78463e90bc2e","Het schatten van de Minimale Effectieve Dosis in een Dosis-Respons Model","Steijn, Jos (TU Delft Electrical Engineering, Mathematics and Computer Science)","van der Meulen, Frank (mentor); van Elderen, Emiel (graduation committee); van Iersel, Leo (graduation committee); Delft University of Technology (degree granting institution)","2016","","","en","bachelor thesis","","","","","","","","","","","","","",""
"uuid:3ccdff6e-acfb-4e48-845b-7ef6f9860ca3","http://resolver.tudelft.nl/uuid:3ccdff6e-acfb-4e48-845b-7ef6f9860ca3","Analysis of Tidal Inlet Stability","Guo, L.","Schuttelaars, H.M. (mentor)","2016","The main goal of this project is to predict the in uence of non-uniform basin depths on the stability of single inlet systems. This is achieved by making use of Escoffier's principle, which poses a relationship between the relative water flow velocity in an inlet and the rate of change of its cross-sectional area. To this end, a thorough analysis is performed on the equations that govern water motions, principially the linearised shallow water equations. An analytic solution is obtained, and some results are simulated in MATLAB.","tidal inlet stability; channel inlet; barrier islands; Escoffier's principle; shallow water equations; analytical approach","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","Applied Mathematics and Applied Physics","",""
"uuid:3f4a74c8-f46e-4976-989a-364865281e08","http://resolver.tudelft.nl/uuid:3f4a74c8-f46e-4976-989a-364865281e08","Combined Aerostructural Wing and High-Lift System Optimization","van den Kieboom, K.T.H.","Elham, A. (mentor)","2016","Current wing design processes are strongly governed by cruise design requirements. This approach to wing design leaves little room for high-lift device design optimization where trade-offs may be required between cruise wing and high-lift wing design. Moreover, the optimization of high-lift devices typically focuses on optimizing flap/slat setting, gap and overlap, without taking into account aeroelastic effects on high-lift performance and the effect of flap design on wing weight. Ideally, the aerostructural optimization of the high-lift system and cruise wing should therefore be combined throughout the complete optimization process. In this study, an existing aerostructural analysis and optimization tool used for cruise wing design is extended to include the analysis and optimization of high-lift systems. The analysis tool makes use of a quasi-three-dimensional (Q3D) aerodynamic analysis in which a three-dimensional inviscid analysis is coupled with two-dimensional viscous analyses performed at several spanwise sections. The Q3D aerodynamic analysis is extended with high-lift capabilities, where the Pressure Difference Rule is used to predict maximum lift coefficient. Coupling of the extended aerodynamic analysis to the structural solver FEMWET enables high-lift aerodynamic analysis while taking into account aerelastic deformation. As a test case, the aerostructural optimization of a Fokker 100 class wing has been considered for minimizing fuel weight. Design variables included wing geometry, airfoil shape, structural thickness and flap settings and span. The proposed optimization formulation resulted in a fuel weight reduction of 9.65%, while satisfying airfield performance requirements of the initial design. This proves that combined aerostructural optimization of wing and high-lift system design is effective and deserves continued research.","Aerostructural; High-lift devices; MDO; FEMWET; Pressure Difference Rule; Wing optimization","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy & Propulsion","","Flight Performance and Propulsion","",""
"uuid:1fb50322-bf2f-4cc1-922e-344fb2bbb918","http://resolver.tudelft.nl/uuid:1fb50322-bf2f-4cc1-922e-344fb2bbb918","Strength Analysis of 26K NT tank container","Bao, J.","Romeijn, A. (mentor)","2016","","","en","master thesis","","","","","","","","2021-08-12","Mechanical, Maritime and Materials Engineering","Marine and Transport Technology","","","",""
"uuid:090ef1b0-5453-4d20-9241-40d7e870037d","http://resolver.tudelft.nl/uuid:090ef1b0-5453-4d20-9241-40d7e870037d","Radiation, Diffraction Analyses on Side by Side Applications","Oude Ophuis, R.B.A.","Huijsmans, R.H.M. (mentor)","2016","Offshore barge mooring is considered a critical operation. The vessels need to be in close proximity, but may under no circumstance collide. Mooring lines and fenders are installed to keep the vessels in place. Experience has shown that lines tend to break occasionally; one of the reasons for this can be the interaction effect of the two vessels and the occurrence of standing waves in the gap. In principle, energy is trapped in the enclosed space between the vessels as a form of resonance. Previous research has shown that the increased vertical flow resulting from the standing waves separates around the bilge, extracting energy out of the upwards motion of the water. To determine the required strength and amount of the mooring lines, the motions and forces on the vessel are usually determined using diffraction analyses. These analyses are based on the principle of linear potential flow in the frequency domain, which allows for computationally quick calculations of the behavior of a vessel. Unfortunately, the creation of vortices is a non-linear effect that is overlooked by potential theory. This results in an unrealistic over-estimation of the water height in-between the vessels (sometimes up to ten times the incoming wave), which could affect the hydrodynamic behavior and thus the results of a mooring analysis. This thesis focuses on the effect of this over-estimation and on how to mitigate this effect in existing software. The Deepwater Construction Vessel Aegir and a cargo barge alongside are modeled in MultiSurf. The resulting patch models are inserted in WAMIT for diffraction analysis. The single body behavior is analyzed, as well as the over-estimated multi-body behavior. To mitigate the over-estimation of the resonant effects, a flexible damping lid is modeled in between the vessels. The lid is mass-less, has no thickness, and is non-permeable. Through so called modal analysis, the response of the water in between the vessel is decomposed into several so called mode shapes, each with a different response. The lid is made flexible by allowing movement only in these mode shapes. The superposition of the forced response of each of these modes provides the total response of the lid. This approach creates additional degrees of freedom to the system that can be damped accordingly, decreasing the energy of the water in the gap. The damping is determined by recreating a model test from literature in WAMIT and tuning the lid to match the measured water level. Large but narrow peaks are found in the various RAOs of both vessels. These are also analytically determined by solving an eigenvalue problem. It is shown that the peaks observed in the motion-, wave force-, water height-, and mean wave drift RAOs can be damped out with the lid. The effect is greatest on the water elevation and wave drift forces. Looking at the response of the vessels in an actual sea state, it is found that the motion and force response are of the same magnitude as those of a single vessel. Due to the non-symmetry of the vessel configuration some small differences are seen, but not as dramatic as in the RAO s. The damped lid shows to have only a marginal effect on these responses. The water elevation and wave drift force responses show large responses compared to those of the single-body. Application of damping shows significant influence on these two responses. This thesis demonstrates that the motions and exciting wave forces obtained through regular diffraction analysis are practically unaffected by the over-estimated gap resonance, and that a damping lid is only necessary when the water elevation in the gap or the drift forces are of importance, such as in a mooring analysis. With the barge shadowed by the Aegir in optimum heading however, the behavior is always determined correctly, and use of a damping lid is not deemed necessary. The model serves as a simple basis for future mooring analyses. In the future more focus should be put on determining damping for other vessel configurations, possibly through model testing or use of computational fluid dynamics.","Gap resonance; Side by Side; mooring; offshore; damping lid; diffraction; radiation; WAMIT","en","master thesis","","","","","","","","2021-08-21","Mechanical, Maritime and Materials Engineering","Marine and Transport Technology","","Offshore engineering","",""
"uuid:49fa1f7b-c203-47ef-b4b1-99ee6738e996","http://resolver.tudelft.nl/uuid:49fa1f7b-c203-47ef-b4b1-99ee6738e996","Research on a Subsea Separation Concept for Deep Sea Mining Purposes","Derksen, G.A.D.","Van Rhee, C. (mentor); Keetels, G.H. (mentor); Schipaanboord, A.A. (mentor)","2016","Royal IHC is exploring the possibilities of a subsea separation concept for its deep sea mining concept. The most promising concept from literature was the crossflow separator, this separation concept was further researched. This to have a look at the performance of such a crossflow separator and to find out what geometry is most suitable for implementation on the hydraulic crawler. This research addresses three topics: the selection, the computational modelling and the experimentation of the crossflow separator. The aim of this research is to determine the performance of the separator by means of the separation efficiency of sediment and nodules.","Polymetallic Nodules; Manganese Nodules; Subsea Separation; Deep Sea Mining; Deep Sea Mining Concept; Separation Concept; Separation Method; Sediment; Deep Sea Sediment; Drift-Flux Model; Computational Fluid Dynamics","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Offshore and Dredging Engineering","","","",""
"uuid:3b04d65c-b767-4a5f-99e7-4b3b20aafc1d","http://resolver.tudelft.nl/uuid:3b04d65c-b767-4a5f-99e7-4b3b20aafc1d","Simulation of Foam Front Propagation through a Water-flooded Reservoir For Enhanced Oil Recovery","Amin, A.","Rossen, W.R. (mentor); Vincent-Bonnieu, S.Y.F. (mentor); Hussain, A.A.A. (mentor)","2016","Surfactant Alternating Gas (SAG) is one of the Enhanced Oil Recovery (EOR) methods used for controlling gas mobility during displacement of oil in a reservoir. This method is based on alternating injection of surfactant solution and gas to create a so-called “gas foam” in the reservoir. The success of this method relies on the stability of the foam. The effect of oil on foam can be detrimental, decreasing the sweep efficiency, and therefore it is important to understand this effect. So far very few studies exist that include the effect of oil in foam EOR simulations. In this study, the behavior of foam front propagation through a water-flooded reservoir is investigated by doing reservoir simulations in the presence of oil using MoReS, Shell’s in-house reservoir simulator. This study is divided into two main parts. The first part is about a foam override phenomenon observed in a single slug SAG flood. The injected gas tends to flow upwards, initiating a region in which foam cannot exist, and oil tends to migrate and accumulate at the bottom of the reservoir, where the saturation of oil becomes high enough to kill the foam. When the effect of oil on foam is excluded, the foam front is stable and no override is seen. The theory of such a displacement has not yet accounted for the effect of oil on foam. When performing simulations in a fine-grid model, an irregular saturation distribution can be seen at the end of a single cycle SAG flood. This irregularity, referred to as a fluctuation phenomenon, is the second part in this study. It is investigated whether the fluctuation phenomenon has a physical or numerical origin. This is investigated by doing, amongst other things, a study of the foam-model functions, grid refinement and time-step reduction. 1D, 2D and 3D simulations are presented in this study, in which all three phases are incompressible. The displacement of oil by “gas foam” is assumed to be immiscible, and the effect of capillary diffusion is excluded in the simulations.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","","",""
"uuid:c68563ea-554a-4afd-b9d6-378ac80f42c5","http://resolver.tudelft.nl/uuid:c68563ea-554a-4afd-b9d6-378ac80f42c5","Improving Spatial Resolution of Tomo-PIV using Particle Tracks with Vortex-in-Cell","Singh, P.","Scarano, F. (mentor)","2016","Tomographic PIV is one of the most recent and advanced tools used in experimental fluid dynamics. Low spatial resolution is currently a problem in large scale tomo-PIV as concentration. A novel technique is proposed which leverages the temporal information of full Lagrangian particle tracks to improve the spatial resolution of tomo-PIV in low seeded flows. The method produces dense Eulerian velocity field from sparse scattered particle tracks obtained from state of the art particle tracking algorithms. The method is based on a variational principle wherein iterative simulations of Vortex-in-Cell method (Schneiders et al., 2014) are performed to minimise the norm of the difference between simulated and measured velocity of scattered particles. The LBFGS method, which is a gradient based numerical optimisation technique, is utilised to solve this optimisation problem. Gradient of the cost function with respect to the system’s degree of freedom is computed using the adjoint method. Initial and boundary conditions are taken from tomo-PIV measurements. A two dimensional analytical vortex blob is considered for the numerical validation of the proposed method. Both qualitative and quantitative analysis are performed for its detailed assessment. The method demonstrates significantly improved reconstructions as compared to methods which rely on instantaneous information of the particles. It is also revealed that utilising longer particle tracks within the methods framework further improves the reconstruction quality and spatial resolution. The method even shows significant noise reduction capability which enhances when using longer tracks. The proposed method is also validated in an unsteady and evolving flow field by simulating the Von-Karman shedding in the wake of a long cylinder. Even at very low seeding densities the method successfully reconstructs the main structures of the flow. Providing more temporal information again proves to augment the methods ability to produce more accurate reconstructions.","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy & Propulsion","","","",""
"uuid:23963780-b4d6-4bcf-947f-1d7aa2136a25","http://resolver.tudelft.nl/uuid:23963780-b4d6-4bcf-947f-1d7aa2136a25","Multi-Speed IT & IT Governance: Analyzing the effects of ‘Multi-Speed IT’ on the existing governance within companies","Möhlmann, R.","Janssen, M.F.W.H.A. (mentor); Klievink, A.J. (mentor); Holstijn, J. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","Systems Engineering, Policy Analysis and Management","",""
"uuid:a109024b-b5b8-4081-a9c1-2392734cca4f","http://resolver.tudelft.nl/uuid:a109024b-b5b8-4081-a9c1-2392734cca4f","The design of a mobile outdoor cooking island for Dutch design label Weltevree","de Smidt, F.","Sonneveld, M.H. (mentor); Minnoye, A.L.M. (mentor); Muskens, R. (mentor)","2016","The project began with the need of Dutch design label Weltevree for a new outdoor cooking product. Analysis of the outdoor cooking landscape revealed that the main part of an outdoor dinner, the preparations, often take place indoors. Why not moving this main part of the dinner more towards the outdoor environment, whereby the host feels more appreciated about his fresh and pure made dishes, or even whereby friends and family can join, experience and could help with cooking the dinner. A design vision was created: Creating a mobile outdoor cooking product which supports the cook and his guests in having an successful outdoor cooking dinner, getting the preparations more towards the garden environment and being a part of the outdoor dinner, with the visual and interaction appearance of an open kitchen. The final design “The Weltevree Mobile Outdoor Cooking Island” is adventures in the way of making your own spot for cooking within your garden, it provides comfort in the way that it creates an perfect situation with all the facilities to having an successful outdoor cooking dinner. The mobile outdoor cooking island provides different functions such as an innovative heating source to cook with cookware and grilling, a waterplace, a movable transmissive countertop, a smart layer which includes the sink and a multifunctional crate. And all these functions are situated on a compact cooking island which is basic in his appearance and still innovative. The mobile outdoor cooking island is the central subject and object of your garden and outdoor dinner, whereby cooking and dinning blends in together, and provides the social and experiencing aspect of having an outstanding and enjoyable outdoor dinner. The final design is elaborated in an 1 to 1 visual and functional prototype to finalise the design and validate it.","outdoor; cooking; kitchen; island; mobility; design","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","","",""
"uuid:2e5d4f0e-549f-4282-acc4-4b68af254bb0","http://resolver.tudelft.nl/uuid:2e5d4f0e-549f-4282-acc4-4b68af254bb0","Improve multidisciplinary team meetings by design","Beem, J.A.","Melles, M. (mentor); van Rompay, T. (mentor); Peerdeman, S. (mentor)","2016","This thesis shows the graduation project of the Master program Design for Interaction at Del University of Technology. The goal of this graduation project is to investigate how design and technology can support and enhance MDMs in order to create a high-quality work environment in which different medical specialists can work together at their full potential. The neuro-oncology MDM at the VUmc is used as case for this project. The first section of this report ‘chapter 1: literature’ reveals many possible MDM improvements as proposed in literature, based on previous MDM observations in the VUmc (TeAMS program) and outcomes of student projects with the same topic (DPH). However, very few of these presumed improvement markers of MDMs have been proven, and moreover, these studies are o en superficial and only correspond only to the explicit and observable knowledge (Sleeswijk, 2009). Two approaches are formulated to translate the literature insights to the user study: (1) factors by which the efficiency and quality of the MDM can be assessed, and (2) influence factors on teamwork and interaction between the members of the neuro-oncology at the VUmc. The second section of this report ‘chapter 2: user study’ takes you through the participatory research conducted via a mixed-method approach in order to create a meaningful design. The user study led to twenty-six insights, which are visualized as ve workmodels. It can be concluded that poor verbal communication due to the sub-optimal environment and disturbances such as pagers, and lack of possibly crucial information, could lead, especially in the decision-making step, to mistakes. My challenge as a designer is to make sure every individual doctor has a his or her own beneficial value (complementing the diversity of patient-centered teamwork) and at the same time make sure that every member complements each other to form a greater whole (improving inter-collegial teamwork). In the third section ‘chapter 3: design vision’ attention is given to the starting points of the design phase. Combining al insights from the literature and user study led to seven improvements opportunity areas. Two of them are chosen: ‘lack of team cohesion’ and ‘poor verbal and little non-verbal communication’, both in which a attitude change from the MDM members is needed from ‘my contribution to the meeting is essential’ to ‘‘my contribution to the meeting is essential and the contribution of the others is equally essential’. The requirements are set for the design: the product-service system should evoke the following interaction qualities: proud of our team accomplishments, unity among the group, job satisfaction in providing optimal care for patients, and meaningful patiënt discussions. This is similar to the experience of performing in a classical symphony orchestra. The fourth section ‘chapter 4: conceptualization’ focuses on the process concept development. A created template (visual of MDM as a bell jar) provides two scopes for the concepts. The first scope is to improve the environment of the current MDM (aim of concept ‘MDM Blueprint’). The second scope is to improve the interaction between the members by designing a tool. This second scope has two alternatives: to improving communication between the members (aim of concept ‘Viscom’), or to activate inactive members (aim of concept ‘MDM Advisor). The Blueprint and Viscom concepts are chosen to further develop in the ideation phase. Three iterative studies with external experts and end-users are conducted, which provided insights to optimize and improve these directions. The fifth section in the report ‘chapter 5: final design’ shows the final concept ‘MDM+’ and its features. In figure 1 (right page) the MDM+ concept aims to improve both: (1) a better suited discussion environment (MDM Blueprint) and (2) bring communication differences into line (Viscom application). The sixth section ‘chapter 6: evaluation’ explains that in order to have a critical review from the end users, two evaluation studies were needed. The first study was a demonstration for the entire MDM group, and it showed that the participants expected an increase the mutual understanding between the members. The second study was a simulated MDM discussion, in which the first the application was used during a patiënt case and secondly the usability was discussed. It showed that a faster information exchange between the members is possible with Viscom. However the biggest barrier of the Viscom application will be the smooth technical integration of the application into the current hospital system. This report ends with ‘chapter 7: conclusions and recommendations’. It can be concluded that the core of the MDM is the discussion. The discussion is fueled by the information on visible scans and the knowledge of the members, but the ‘raison d’être’ of the MDM is the information exchange. The final ‘MDM+’ concept proved to complement both the layout and the potential to let the top-specialists further develop themselves in inter- collegial teamwork and communication, thus resulting in a more efficient multidisciplinary meeting. The recommendations entail different future perspectives of the implication of the MDM PLUS+ to different MDMs and initial proposals to test the Blueprint and Viscom for future development, in general and for the VUmc. For the full source report email jessebeem@gmail.com.","multidisciplinary team meetings; healthcare; environment; application","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Applied Ergonomics and Design","","Design for Interaction, specialization Medisign","",""
"uuid:912562f5-987c-4998-87a8-176e5484be94","http://resolver.tudelft.nl/uuid:912562f5-987c-4998-87a8-176e5484be94","Secondary voltage control in the Netherlands: A feasibility study","Dileep, D.","Rueda Torres, J.L. (mentor); Franke, S. (mentor)","2016","Increasing integration of renewable energy sources, distributed generation, smart grid technologies and other power electronic devices into the grid has given rise to numerous challenges in the electricity transmission and distribution system operations domain. Due to the increasing complexity in electricity market processes, control of active power in steady state time frame is becoming infeasible to system operators at normal state of operation. Whereas, steady state control of reactive power is still viable to system operators. System operators control reactive power resources in steady state time frame to maintain a good voltage profile within their power system. Currently, this steady state voltage control is performed manually for the Dutch transmission system by control engineers with minimum real-time computational assistance or no intelligent decision making tool. This master thesis focuses on the feasibility study conducted for employing an intelligent control system that can be used for secondary (or steady state) voltage control scheme in the Dutch transmission system. The feasibility study included literature reviews on hierarchical voltage control schemes of other European transmission systems, the approach of secondary voltage control in the national control center and several artificial intelligence techniques available for coordinated voltage control. In this study, a rule based controller was developed to initialize a reactive power optimization problem (with multiple objectives) for a mixed integer linear programming solver. Rule based controller was preferred mainly due to its simplicity, adaptability and ease of mapping expertise of control engineers (& requirements of transmission system operators) as rules. Simulation based studies were performed using this controller on an 80-bus test system and snapshot scenarios of the Dutch transmission system after defining multiple areas within them.’","reactive power optimisation; voltage control; mixed integer linear programming; power system","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","Intelligent Electrical Power Grids","",""
"uuid:c184e88d-fd07-43c2-8b39-278fc047ed37","http://resolver.tudelft.nl/uuid:c184e88d-fd07-43c2-8b39-278fc047ed37","Optimal Control Allocation on Over-Actuated Vehicles","van den Berg, E.H.","Hellendoorn, J. (mentor)","2016","Modern passenger vehicles are equipped with an increasing number of actuators that may be used to actively control the lateral and longitudinal dynamics of the vehicle. During limit-handling situations, proper coordination of all the available actuators by the vehicle stability control (VSC) can lead to improved overall control authority, which in turn may lead to improved handling performance and decreased intrusiveness to the driver. The difficulty, however, is coordinating the available actuators, given that the addition of actuators typically leads to the vehicle becoming over-actuated. The state-of-the-art method of solving the over-actuation is optimal control allocation, often referred to as Global Chassis Control (GCC). A drawback of most formulations of GCC proposed in literature is the prohibitive computational burden associated with the optimization. To obtain a real-time feasible GCC algorithm, the hybrid steepest descent optimization method is applied to the control allocation problem in this work. The optimization problem is set up to minimize tracking errors of virtual control inputs whilst minimizing actuator effort. A high level controller is designed that produces these virtual control input targets, which are a yaw moment used to stabilize the vehicle, and longitudinal force, to represent the acceleration intent of the driver. Using online linearization of a nonlinear vehicle model the yaw moment and yaw moment effectiveness of the available actuators is estimated and used to update the optimization problem. Furthermore, improvements are suggested to the hybrid steepest descent method to accelerate convergence and reduce or eliminate chatter at constraint boundaries by dynamically scaling the constraints. The optimal control allocator is shown, using a validated simulation model, to produce improved allocation performance when compared to a simpler control allocation method that is similar to the industry-standard. Furthermore, the proposed algorithm was converted to C-code and implemented on one of the on-board ECUs of a Tesla Model S, demonstrating real-time feasibility. Experimental results for an aggressive single lane-change using this implementation show the algorithm provides good performance compared to an industry standard brake-based stability control system.","hybrid steepest descent optimization; control allocation; GCC; vehicle stability control","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control (DCSC)","","","",""
"uuid:61e1249c-64a4-48b0-8b57-529457c6ed73","http://resolver.tudelft.nl/uuid:61e1249c-64a4-48b0-8b57-529457c6ed73","Strain Gauge Re-Design for Multi Axial Strain Measurements in Anisotropic Composite Materials","Batra, S.","Turteltaub, S.R. (mentor); Kassapoglou, C. (mentor); Mikkelsen, L. (mentor); Gunneskov, O. (mentor)","2016","","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures & Materials","","","DTU Wind Energy-M-0110",""
"uuid:d41f56d6-f1a6-4a22-a8f4-b89bbba31418","http://resolver.tudelft.nl/uuid:d41f56d6-f1a6-4a22-a8f4-b89bbba31418","The Effect of Individualised Haptic Guidance Paths for Free-Air Teleoperation Tasks","Hoeckx, F.","Wildenbeest, J.G.W. (mentor); Abbink, D.A. (mentor)","2016","Haptic Shared Control (HSC) can improve human-in-the-loop teleoperation task execution by providing forces to guide the operator along a path. The current guidance path designs do not take individual movement preferences into account, leading to disagreement between the operator and the HSC controller. This study investigates if a Machine Learning technique, the taskparametrised Gaussian Mixture Model (GMM), can provide guidance paths learned from individual demonstrations. Subjects (n=12) performed a human factors bolt-and-spanner teleoperation experiment with two different types of haptic guidance paths: individually learned paths calculated by the GMM, and paths calculated by the Minimum Jerk (MJ) model. It was found that HSC improved task execution over manual control, and that using GMM paths in learned situations reduced the disagreement. This effect disappeared with situations outside of the learned range: depending on the path, MJ guidance resulted in better or similar operator workload and disagreement compared to the GMM guidance.","","","master thesis","","","","","","","","2017-08-19","Mechanical, Maritime and Materials Engineering","Biomechanical Engineering","","BioMechanical Design (ME-BMD), BioRobotics","",""
"uuid:8db6dc1d-8b9c-44b8-945c-0d60a5e6b081","http://resolver.tudelft.nl/uuid:8db6dc1d-8b9c-44b8-945c-0d60a5e6b081","Expressive Protection Covers for Lightweight Robot Arms","Emirdağ, D.","Verlinden, J.C. (mentor); Cencen, A. (mentor); Schiele, A. (mentor); den Exter, E. (mentor)","2016","This graduation report describes the research and development of a novel product, a weatherproof fabric based robot protection cover augmented with flexible LED displays. The product has a built-in power and data distribution system and is capable of being mounted onto an existing lightweight robot arm (LWA). The project was done with the European Space Agency Telerobotics and Haptics Laboratory to improve the usability and durability of LWAs in future usage scenarios. These scenarios include ones where autonomous or teleoperated robots need to operate in suboptimal conditions in the presence of humans. The proposed product acts as an “Expressive Robot Skin” that enables the robot, and if available its teleoperator to communicate with the person(s) co-present with the robot, as it operates in an environment, such as outdoors. This is achieved by showing graphical elements on the display augmented robot protection cover. The scope of the project consisted of the following: analysis of the state of the art and the users; conceptualization of the device and its graphical language; virtual and physical embodiment of the product, and the evaluation of the proposed product with multiple experiments. For the analysis phase, the state of the art was investigated by reviewing relevant publications, patents and similar devices on the market. Additionally, interviews and observations were done with LWA users in the host laboratory. The findings from this phase were used to create four device concepts with different display capabilities. One concept was selected to be developed further. To evaluate the product, the following studies were run with 28 first time users: (1) a graphical questionnaire, using the virtual model of the product to investigate the readability of the graphical elements, (2) a controlled human-robot interaction experiment using a KUKA Robotics LWA equipped with the product prototype to measure its impact on a user’s ability to identify the actions of the robot in a teleoperated robot-assisted unstowage task scenario, (3) a post-experiment questionnaire to evaluate the user’s overall experience with the product and the robot, (4) testing of the robot equipped with the prototype to calculate its impact on the operation performance of the robot. The results showed that: (1) the graphical language consisting of simple icons and color performed well, but some elements could be erroneously interpreted in multiple ways, (2) the product significantly improved the user’s ability to identify the contact forces on the LWA, but it had no effect on the ability to identify the robot states or motion, (3) the device did not affect the emotional experience of the users, but it was reported to be helpful for identifying the contact forces and the state of the LWA, (4) the robot was able to use its full workspace and preserved 93% of its payload capacity. However, the protection cover of the product required further detailing and testing before it could protect a LWA during outdoor operation. It was concluded that with continued development the product could be beneficial during teleoperated robot missions on-board ISS or on ground; in the automated manufacturing and food processing industry; and with medical robots. The novelty of the product justified the start of a patent application to enable the exploitation of the technology in these fields.","Human-Robot Interaction; Lightweight Robot Arms; Usability; Robot Protection; Flexible Displays; User Interfaces","en","master thesis","","","","","","","Campus only","2017-08-19","Industrial Design Engineering","Design Engineering","","Integrated Product Design","",""
"uuid:7348938d-c490-4b13-a5e8-e771a28b71f2","http://resolver.tudelft.nl/uuid:7348938d-c490-4b13-a5e8-e771a28b71f2","Acceleration of the Multi Level Fast Multipole Algorithm on a Graphics Processing Unit","Wagenaar, C.","van der Heul, D.R. (mentor)","2016","It is of great importance that enemy aircraft can be detected by a radar. Good knowledge about one’s own detectability is also needed to make one’s own visibility as low as possible. Obtaining the aircraft radar signature involves solving a scattering problem with a large, low-observable aircraft. Briefly explained; the incident radar wave induces a current distribution on the surface of the scatterer. Using theMethod ofMoments (MoM), this current distribution can be found as the solution of an integral equation. The current distribution is expanded in a set of basis functions. Because every pair of basis functions interacts through the Green’s function, the continuous equation turns into a matrix equation after discretization. The matrix in this equation is a dense matrix. It is not desirable for practical problems to save the large system matrix, because this limits the problem size considerably. The Fast Multipole Method (FMM) became popular because it makes it possible to reduce the memory storage and the work needed to solve the discretized integral equation greatly. The work scales proportional to O(N 3 2 ), where N is the number of unknowns. The Multi Level Fast Multipole Algorithm (MLFMA) reduces the required memory and computational complexity even more to O(N logN) by having different levels of clustering. Radar signature computations are, with the use of theMulti Level FastMultipole Algorithm, still computationally intensive. Graphics Processing Units (GPU’s) have the potential of high computational power at relatively low cost. Given the hardware and software restrictions of Graphics Processing Units, some parts of the Multi Level FastMultipole Algorithmare better suited for calculations onGraphics ProcessingUnits than others. Only by perfectly understanding the properties of Graphics Processing Units, the applicability of theMulti Level Fast Multipole Algorithmcan be fully exploited. Matrix vector multiplications are the most time consuming parts in the Multi Level Fast Multipole Algorithm and are suitable for parallel computations. Therefore, this part of the algorithmis implemented on theGraphics Processing Units. A discretized Poisson equation will serve as a model problem for the Multi Level Fast Multipole Algorithm computation. The Poisson equation is discretized with finite differences on a structured grid in two dimensions. The discretized Poisson equation is iteratively solved on a Graphics Processing Unit. When the calculations are performed on a Graphics Processing Unit, they take up to 11 times less time in comparison with the same system performed on a single core of a Central Processing Unit (CPU). Three small test problems for theMulti Level FastMultipole Algorithm are used to compare the performance of different implementation methods. The test problems have a realistic structure, but have a smaller number of unknowns compared to real problems. Like inmany other applications, the bottleneck of Graphics Processing Units is the fact that information has to be sent back and forth. When a large amount of data has to be sent back and forth, the Graphics Processing Unit is not faster than a Central Processing Unit. Three test problems are accelera","Radar signature; Multi Level Fast Multipole Algorithm; Parallel computing; Graphics Processing Unit","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Numerical Analysis","","Applied Mathematics","",""
"uuid:c3a3b168-5195-4c21-9d17-f5545c5cfac2","http://resolver.tudelft.nl/uuid:c3a3b168-5195-4c21-9d17-f5545c5cfac2","On the tracing fidelity of helium-filled soap bubbles for PIV experiments","Morias, K.","Sciacchitano, A. (mentor)","2016","This thesis follows a previous study on the aerodynamic characterization of helium-filled soap bubbles (HFSBs) for large-scale PIV measurements and aims at characterizing statistically the tracing fidelity of HFSBs in PIV experiments, considering the statistical distribution of the bubbles diameter, slip velocity, relaxation time and density. High-speed visualizations identify two different operating regimes of the bubble generator. Two dedicated experiments are performed at a spatial resolution such to determine simultaneously the bubbles trajectory and their diameter. The velocity of the bubbles in the stagnation region ahead of a circular cylinder is evaluated by the PTV technique. The results are compared with micron-sized fog droplets taken as reference. The tracking error of individual trajectories is assessed by statistical analysis of the relative slip between the bubble and the airflow. The instantaneous particle relaxation time is retrieved from the ratio between slip velocity and local acceleration. Additional information on the bubble instantaneous properties is taken by inferring the diameter from the distance between the glare points. HFSBs were found to yield, on average, a time response of about 10 µs with a standard deviation that exceeds 30 µs when the nozzle is in a stable operating regime (bubbling). However, when the nozzle operates in a unstable operating regime (jetting), the standard deviation of the bubble diameter and relaxation time can be as high as 70 µm and 50 µs, respectively. The HFSBs relative density to air is estimated using a modified Stokes drag law. HFSBs as flow tracers in a laminar flat plate boundary layer feature a particle-free region close to the flat plate surface. The height of this region is a function of the particle diameter and can be partly explained by the conservation of mass principle of a streamtube far upstream that expands inside the boundary layer.","Particle Image Velocitmetry; PIV; Helium-filled soap bubbles; Tracing fidelity","en","master thesis","","","","","","","","2017-08-19","Aerospace Engineering","Aerodynamics, Wind Energy & Propulsion","","Aerodynamics","",""
"uuid:ca8d6630-4edc-4521-9ff6-30027193cf4a","http://resolver.tudelft.nl/uuid:ca8d6630-4edc-4521-9ff6-30027193cf4a","Quantifying Uncertainty in Fractured Geothermal Reservoirs Using a Discrete Fracture Model","Thomas, A.T.","Clauser, C. (mentor); Bruckmann, J. (mentor); Chen, T. (mentor)","2016","Fractures occur at varying scales and orientations in the subsurface. The role of fractures as conduits for fluid flow in a reservoir must be well constrained for planning and development of a geothermal system. This study examines the uncertainty associated with fractures in a rock matrix using a discrete fracture network modelling approach; fractures are considered as discrete elements embedded in a rock matrix. Many fractures are below the resolution of geophysical investigation methods and are therefore typically described by a statistical distribution of permeability, length and orientation. A Monte Carlo approach is used whereby multiple reservoir simulations are conducted using fracture network parameters drawn randomly from a pre-defined distribution. The simulation results show that fracture permeability is the major factor which influences the production. An increase in fracture network permeability from 8:33x10-^12 m2 to 8:33x10-^10 m2 showed an increase of uncertainty in reservoir production by up to a factor of six when considering the standard deviation. The results show that this uncertainty increases by up to 200 % with the life of the reservoir for the high permeability case. Stochastic analysis of the azimuth uncertainty revealed an opposite trend where a decrease in uncertainty of 34 % was observed over the 30 year simulated lifetime of the reservoir. The orientation of fractures in the reservoir was found to play an important role in the temperature field in the reservoir with increases in production temperature ranging from 0:5 K to 3 K resulting from the proximity of fractures to the production well. The results also indicate that discrete fracture networks which possess similar orientations of fractures produce statistically similar production profiles. The findings of this study show that a stochastic approach can be used for efficient planning of geothermal doublet systems based on expected trends in production.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Joint Master's in Applied Geophysics: Delft University of Technology, ETH Zürich, RWTH Aachen University","",""
"uuid:cef2b0ac-95eb-4325-bc9f-4764458fe467","http://resolver.tudelft.nl/uuid:cef2b0ac-95eb-4325-bc9f-4764458fe467","Empirical Examination on Impact of Firm Characterestics on Equity Crowdfunding Success","Raphael, A.D.","Roosenboom-Kwee, Z. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Values Technology and Innovation","","","",""
"uuid:7266399f-6598-485b-8d21-57932a658e4b","http://resolver.tudelft.nl/uuid:7266399f-6598-485b-8d21-57932a658e4b","Mining sequence for Lappberget 1250","Świtała, J.S.","Ngan-Tillard, D. (mentor); Nyström, A. (mentor); Pytel, W. (mentor)","2016","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Applied Earth Sciences","","","",""
"uuid:eb4a8dd4-e024-48d7-9784-4bbecbebe1f1","http://resolver.tudelft.nl/uuid:eb4a8dd4-e024-48d7-9784-4bbecbebe1f1","The Heston model with Term Structure: Option Pricing and Calibration","van der Zwaard, T.","Oosterlee, C.W. (mentor); du Toit, J. (mentor)","2016","This thesis addresses the calibration of the Heston model with term structure (i.e. with piecewise constant parameters) to a set of European option prices from the FX market. Several option pricing methods are discussed and compared, among which the COS method, Lewis' method and the Andersen QE Monte Carlo scheme. Several modifications are proposed in order to improve the practical usability of the COS method in terms of speed, accuracy and robustness. The calibration of the Heston model with term structure is chosen as a benchmarking test-case for comparing several optimization techniques, that are both open-source as well as from licensed products. The performance the optimizers is measured in terms of speed of the calibration. In addition, a simple hedge test using the calibrated model is used as a secondary performance metric. The combined effort of finding the fastest optimization techniques and fastest pricing method has the potential of speeding up daily FX calibrations performed in many financial institutions.","option pricing; foreign exchange (FX) market; COS method; Heston model with term structure; calibration; optimization; benchmarking; hedging","en","master thesis","","","","","","","","2021-08-19","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","Numerical Analysis","",""
"uuid:73e8dd75-207b-4bfc-aaa5-bcd4fa3e0242","http://resolver.tudelft.nl/uuid:73e8dd75-207b-4bfc-aaa5-bcd4fa3e0242","Estimating an LPV model of neuromuscular admittance with grip force as scheduling parameter","Pronker, A.J.","Abbink, D.A. (mentor)","2016","Development of adaptive haptic shared control system requires understanding of the adaptive nature of the driver’s neuromuscular system. In this study, linear parameter varying modeling techniques are proposed to identify changes in the admittance of the driver’s arm. The admittance describes the relation between the torque applied to the steering wheel and steering wheel angle output. By conducting two experiments it is investigated whether grip force applied to the steering wheel serves as an appropriate scheduling variable for a LPV model. In our approach, first tracking tasks with torque perturba- tions applied to the steering wheel are performed with varying boundary widths to evoke changes in admittance. The relation between grip force and admittance is investigated by estimating neuromuscular stiffness and damping parameters and measuring the force applied to the steering wheel using pressure gloves. An LPV model is identified by linear interpolation between LTI models identified for each condition. Second, a driving task is performed in a fixed-base driving simulator to analyze the variance in grip force and changes in admittance between wide and narrow roads. The results of the driving task are used to assess the validity of the LPV model during driving tasks. It was found that the stiffness and damping parameters varied proportionally with the grip force. Although small variations in grip force levels are found between subjects, it is concluded that grip force is an appropriate scheduling variable for an LPV model. The relation between grip force and admittance was similar during driving tasks compared to the tracking tasks. The derived LPV model accurately describes the changes in the neuromuscular system over the range of grip force variations observed during the driving task. For further validation of the LPV model in the time-domain it is recommended to use time- varying boundary tasks.","neuromuscular admittance; time-varying; grip force; LPV; driving behavior","en","master thesis","","","","","","","","","Aerospace Engineering","Control & Simulation","","","",""
"uuid:249c3a06-501c-4726-be00-58fef2d851c9","http://resolver.tudelft.nl/uuid:249c3a06-501c-4726-be00-58fef2d851c9","Vascoscope","Backus, M.","Ruiter, I.A. (mentor)","2016","Miscannulation during dialysis is the cause of shunt damages such as scarring, narrowing, blocking, infection and internal bleedings. Miscannulations increase the difficulty of flawless puncturing next time and in general reduce the shunt’s lifetime, leading to more invasive surgeries and higher costs. Ultrasound devices are used to simplify the cannulation of these shunts by visualizing them on a monitor. However, because the device that is used nowadays in dialysis centres is big and clumsy and not specifically designed for visualizing dialysis shunts, nurses only use it for the more severe cases. Therefore, the goal of the Vascoscope project was to design an echo device that stimulates the use of ultrasound visualization for cannulating dialysis shunts, thereby minimizing the amount of miscannulations. The final design of the Vascoscope is a highly portable device consisting of a lightweight probe containing an ultrasound transducer and a screen, a battery pack containing the battery and CPU and a wire connecting the two. The screen is placed on top of the probe, which allows the nurse to see the echo visualization and the needle entering the shunt in the same visual field. Next to that, the device is designed for optimal usability with all different cannulation habits; it enables both longitudinal and transversal cannulation, by allowing flexible screen rotation, and offers the possibility to fixate rolling shunts. Lastly, the Vascoscope is free of any edges or corners that are difficult to clean; an essential property in infection-prone environments. With these properties, the Vascoscope is designed for portability and ease of use, lowering the threshold for incorporating the use of ultrasound in the default dialysis cannulation routine. This is supported by user research on the final design. The cooperating dialysis nurses are enthusiastic and mention several improvements on their current echo device, the main things being the portability, ease of use, flexibility for different cannulation methods and simplicity of working with the device. Therefore, the expectation is that dialysis nurses will be more keen to perform ultrasound-guided cannulation when having the Vascoscope at their disposal.","dialysis; ultrasound; cannulation","en","master thesis","","","","","","","Campus only","2017-08-18","Industrial Design Engineering","Integrated Product Design","","","",""
"uuid:0ffc4dfd-40ff-436e-ab21-c11db7380be4","http://resolver.tudelft.nl/uuid:0ffc4dfd-40ff-436e-ab21-c11db7380be4","Practising change(s): Analysing the German niche of permaculture with a Social Learning perspective to monitor social change in sustainability transitions","Ulbrich, R.","Quist, J.N. (mentor); Pesch, U. (mentor)","2016","The Master’s programme Industrial Ecology is jointly organised by Leiden University and Delft University of Technology. - Grassroots initiatives that are inspired by the framework of permaculture try to design effective social and ecological systems. These are aimed to provide ecosystem services, build communities, give opportunities to learn and practice and support the establishment of circular nutrient systems. This they do, guided by a dynamic and systemic “manual” that helps to integrate and create many functions and draw on synergistic effects. Many results of such a design are commonly perceived as positive externalities. Drawing on principles and practices that run counter the current way “things are done”, the permaculture community offers potential insights into transformative processes that question the current growth paradigm and mainstream practices. One of the reasons for the growth of the permaculture community lies within its set-up as a community with strong core values and practices. These shared values and practices align members whilst recognising each individual's competences. However, being embedded in an environment that plays according to very different rules, the community also has to deal with tensions at its boundaries. Applying a systems perspective to complex problems allows seeing interwoven dynamics more differentiated. This thesis investigates the potential of combining two analytical lenses to better accommodate the diverse dynamics intrinsic to social systems in sustainability transitions: While Social Learning Systems allow for a close review of dynamics that make up social learning processes, the framework from transition studies stretches the time frame and puts dynamics in the perspective of broader external factors. With the goal to investigate the internal dynamics in a grassroots initiative that shape social learning processes and the potential for such communities to reach out to their environment this thesis establishes a conceptual framework to analyse the German permaculture community. It furthermore discusses the findings based on the present debate on sustainability transitions. Interviews with permaculture teachers as key informants who apply and teach permaculture design principles make up the empirical basis of this report. Frameworks and concepts from innovation and transition studies and theories on social learning are its theoretical backbone. Findings suggest that the German permaculture community is in the process of better accommodating for its growing diversity. This gives rise to new potential for developments at its boundaries, however, internal challenges are lacking capacity and the absence of commitment to a common strategy that effectively links individuals to coordinated actions. For transition studies, the combination of the analytical perspective adds insights into developments, challenges, and possible diffusion of radical grassroots initiatives as it highlights dynamics and the strong influence of personal agency on social change and diffusion of practices and meanings. For Social Learning Systems, the transition perspective offers an orientation which external factors also need to be considered in the shaping of learning processes in socio-technical systems.","","en","master thesis","","","","","","","","","Technology, Policy and Management","","","Industrial Ecology","4413 TRP 30Y Thesis Research project",""
"uuid:74776d89-3095-4fa8-b38f-8f9996f243f3","http://resolver.tudelft.nl/uuid:74776d89-3095-4fa8-b38f-8f9996f243f3","Quantitative Seismic Interpretation for the Characterization of Clastic Reservoirs in the Gulf of Mexico","Saavedra Castañeda, D.A.","Storms, J.E.A. (mentor); Diephuis, G. (mentor); Barnhoorn, A. (mentor)","2016","New reserves are necessary to compensate the oil depletion of the giant fields in the Gulf of Mexico. This work covers a reservoir characterization workflow of Pliocene shallow marine deltaic sandstone deposits located in the south eastern Gulf Mexico. The traditional reservoir characterization workflow is considered a multidisciplinary integration process in geological and geophysical data is used with the purpose of defining the geometry, internal properties distribution, lateral extension and flow properties of a petroleum reservoir unit. In this study the geological information provided from cores will allow for the understanding of the internal architecture of the reservoir. A petrophysical evaluation was performed in which flow properties such as porosity, permeability, net to gross, and fluid saturations were obtained. Traditional seismic data, has been used for generating a subsurface map of the reservoir units. Sadly, the use of seismic data is subject to a resolution limitation. A novel aspect of the workflow is presented in this thesis: the incorporation of elastic parameters derived from a simultaneous pre stack inversion, which provided the necessary tools to accurately and quantitatively understand the lateral distribution of the reservoir unit. As a result of the reservoir characterization workflow, a reserve estimation was calculated for the newly discovered field. The use of seismic quantitative methods for reservoir characterization helped to correctly delimitate the fluid content and reservoir properties of the field. This will reduce the uncertainty associated with drilling new wells during field development stage.","P Impedance; Reservoir Characterization; Static Model","en","master thesis","","","","","","","","2018-12-31","Civil Engineering and Geosciences","Geoscience & Engineering","","","",""
"uuid:d612b585-89ba-43ab-8c31-1798e14e0c80","http://resolver.tudelft.nl/uuid:d612b585-89ba-43ab-8c31-1798e14e0c80","The characterization and design of a CubeSat Integrated COTS Resistojet thruster","Stohr, P.E.P.","Zandbergen, B.T.C. (mentor)","2016","In this project the performance characteristics and design factors of resistojet thrusters are evaluated. The goal of the research is to provide a validated modelling method to estimate the performance of resistojet heating chambers is provided. For the validation an engineering model is constructed based on Commercial Off-The-Shelf (COTS) components. However, due to leakage from the electrical interface, the engineering model is not usable for model validation. A numerical heating chamber model is developed that models the flow through the chamber using relations developed for metal foam tubes. With experimental data, the prediction errors for the heat transfer and pressure drop rate are estimated at 9.13 % and 21.06 % respectively. It is recommended that the model accuracy is validated using a reviewed engineering model.","","en","master thesis","","","","","","","","","Aerospace Engineering","Space Systems Engineering","","","",""
"uuid:a4bce474-3be7-43c3-9d8e-d42d0c4c539e","http://resolver.tudelft.nl/uuid:a4bce474-3be7-43c3-9d8e-d42d0c4c539e","Low emission zones: A study into the business effects and external effects as a result of businesses anticipating on low emission zone policy","Hogenbirk, M.","van Wee, G.P. (mentor); Annema, J.A. (mentor); Rook, L. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","Management of Technology","",""
"uuid:21a284df-7ada-4e3c-9ff2-2d73cc04f534","http://resolver.tudelft.nl/uuid:21a284df-7ada-4e3c-9ff2-2d73cc04f534","Design, Identification and Implementation of a High-Fidelity Cessna Citation II Flight Simulation Model","van den Hoek, M.A.","Pool, D.M. (mentor); de Visser, C.C. (mentor)","2016","As a result of new aviation legislation, from 2019 on all air-carrier pilots are obliged to go through flight simulator-based stall recovery training. For this reason the Control and Simulation division at Delft University of Technology has set up a task force to develop a new methodology for high-fidelity aircraft stall behavior modeling and simulation. As part of this research endeavor, the development of a new high-fidelity Cessna Citation II simulation model, valid throughout the normal, pre-stall flight envelope, is presented. The new simulation model will replace the current baseline model, which is based on the Cessna Citation I, for an increased fidelity and representation of the dynamics of the current laboratory aircraft, the Cessna Citation II. Aerodynamic model identification was done by employing the Two-Step Method. New in this approach is the use of the Unscented Kalman Filter for an improved accuracy and robustness of the state estimates. For the first time, an explicit model structure is presented for the Citation II. Model structure selection by an orthogonal regression scheme has indicated that most of the six non-dimensional forces and moments can be parametrized sufficiently by a linear model structure. It was shown that only the models for the forces in the X and Y body axis would benefit from the addition of higher order terms relating to the aerodynamic angles. On balance, the models for the non-dimensional forces were improved marginally in comparison to the existing simulation model. More significant improvements were made to the non-dimensional moment models.","aerodynamic model identification; flight data model; Cessna Citation","en","master thesis","","","","","","","","2021-08-18","Aerospace Engineering","Control and Operations","","Control and Simulation","",""
"uuid:e3378483-053e-40e2-9685-cf02a04619bf","http://resolver.tudelft.nl/uuid:e3378483-053e-40e2-9685-cf02a04619bf","Design of a new type of impact hammer: Focusing on the drive system of the ram","van Middelkoop, K.","Metrikine, A. (mentor)","2016","The number of wind farms that are commissioned, being installed, or prepared is still increasing due to a growing demand for green energy. To benefit from the higher wind velocities, which can be found at sea, and to prevent the pollution of the horizon, wind farms are being built offshore and installed in an ever increasing water depth. The technological development of energy generation allows the capacity of wind turbine generators to grow. Consequently the environmental loading increases, which demands the foundation structure to be stronger and hence to be larger. One of the most used foundation structures is the monopile, because they are relatively cheap, simple to produce and simple to install. Usually the diameter of the monopile changes over its length. The bottom part has an increased diameter, not to just provide the required bearing capacity, but moreover to withstand the high bending moments introduced by the environmental loading and the weight of the turbine generator. The diameter at the connection with the tower of the turbine generator is in most cases a bit smaller. Nowadays monopiles with a bottom diameter of 7.2 m and a top diameter of 6.3 m are being installed by Seaway Heavy Lifting (SHL). The impact hammers, which are used to drive these large monopiles, use a ram weight to drive the pile to its final penetration depth. This ram weight is located in the center of the hammer and lifted by a hydraulic system. The diameter of the ram at the tip is about 1.2 m. In order to transfer the impact energy to the pile, which has a much larger diameter, an anvil is used. Due to the deflection of the anvil, energy is dissipated, which reduces the energy transferred from the hammer to the pile. The increasing diameter of the monopiles requires the anvil to become larger, which results in an increase of the energy loss. To compensate for the losses the capacity of the impact hammer is increased. As a consequence the stresses in the anvil will become an issue for the structural integrity and the fatigue life of the anvil. Furthermore the noise levels, caused by the impact driving of the piles, show an increase as a function of the monopile diameter and the impact energy. The noise levels are becoming more and more important due to environmental regulations to protect the sea life. Based on these trends, SHL wants to investigate a concept for a new hammer, specifically aimed for these large diameter monopiles. The design is based on a revolved ram weight that hits an anvil ring, which is directly placed on top of the pile. The direct impact will reduce the energy losses between the hammer and the pile, so that the pile can more effectively be driven. The impact force, which causes the stresses in the pile and the noise emission, is most affected by the contact stiffness between the hammer and the pile. Therefore the focus is shifted towards the design of a hydraulic drive system which is able to adjust the contact stiffness. A Matlab model of the hydraulic drive system is made, for which the impact energy can be changed by adjusting the impact velocity. The model includes a system which can vary the stiffness of the ram assembly, see the figure. The pressure of the nitrogen above the impact head can be changed by changing the volume of oil above the floating piston. An increase in the volume of oil will decrease the volume of gas and therefore increase the pressure, hence the stiffness of the gas is affected. To take into account the behaviour of the soil and the pile during impact, a pile-soil model is included. The pile is modelled by a system of lumped masses, whereas springs and dampers are added to model the soil behaviour. The impact force is calculated based on the contact stiffness and the relative displacement between the ram and the pile top. An extensive analysis is performed on the effect of contact stiffness reduction and the associated pile set per blow. From this analysis it turned out that the pile set per blow is reduced in case the contact stiffness is reduced. Furthermore the maximum impact force is reduced and the impact duration is increased. As a consequence the stress and the high frequency noise levels are reduced, which will contribute to the noise mitigation measurements. The reduced pile penetration can be compensated by either an increase in the number of blows or by increasing the impact energy. However to achieve the same pile penetration for a reduced contact stiffness and a lower peak value of the impact force, the energy needs to be increased significantly. Therefore more energy is required for the proposed hydraulic drive system, although the stress and high frequency noise levels can be reduced.","impact hammer; impact force; noise; contact stiffness","en","master thesis","","","","","","","","2021-08-18","Civil Engineering and Geosciences","Hydraulic Engineering","","Offshore engineering","",""
"uuid:04ec81b4-79cb-4fc2-a063-9a13c8eebe9d","http://resolver.tudelft.nl/uuid:04ec81b4-79cb-4fc2-a063-9a13c8eebe9d","Erasing Borders, European Rail Passenger Potential","Donners, B.J.H.F.","van Wee, G.P. (mentor)","2016","This thesis is assessing the potential of the European rail passenger market with a 4 step model, with a gravity based trip distribution and assessment of level of service characteristics to determine total potential in demand and supply commissioned by Train2EU and Royal HaskoningDHV. The scope has been limited to long-distance (>100km) trips. The main research question is: Where and how in Europe does, currently and in 2030, the biggest potential of international train passenger travel exist, based on passenger-trip potential (demand) and seating capacity potential (supply)? A framework for potential of the current network is developed based on the passenger-trip potential and seating capacity potential, providing input for a preliminary proposal for a passenger core connections network. The passenger-trip potential is based on the classic 4-step transport model and a gravity based trip distribution including barriers as language and country borders. For the modal split and route assignment a network based on the real available network has been modelled based on direct distance, mode dependant detour factors and average speed. The seating capacity potential assesses the currently provided train connections on their effectively offered seats as the result of the performance on several key level of service indicators: frequency, travel time performance, service level, transfers and offered seats. The preliminary core network is a combination of the potential in seating capacity and passenger-trip potential. It proposes 12 new core connections providing a new international network of train connections with a high service standard. This can improve the international market size of passenger rail travel by 22% in the current situation and even 14x for the growth scenario in 2030. Additionally the amount of CO2-emmission could be reduced with 28 million tonnes","potential; 4-step model; gravity model; barriers; long-distance; level of service; frequency; effective capacity; EU policy; TEN-T; core connections; Train2EU","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering, Systems and Services","","Transport, Infrastructure & Logistics","",""
"uuid:d40011ba-73d4-4f48-a76d-1abde874dddb","http://resolver.tudelft.nl/uuid:d40011ba-73d4-4f48-a76d-1abde874dddb","Residential energy rebound effect assessment by using serious games","Garay Garcia, O.","van Daalen, C. (mentor)","2016","Energy reduction has been on the political agenda since the last couple of decades. One of the most common policies to reduce energy consumption has been improving efficiency. However, the phenomenon called the rebound effect may threaten the effectiveness of policies aiming to reduce consumption by improving efficiency. The rebound effect is the process in which energy savings, after energy efficiency improvements, are lower than expected. The scientific community agrees on the existence of the rebound effect and the possibility to measure it. In spite of this agreement, the rebound effect has been catalogued by many scholars as a highly controversial concept. In fact, the two main controversies around the rebound effect are: its size and its importance for the policy making process. The main causes that have produced the rebound effect to be controversial are: methodological issues of previous attempts to assess the effect, different numeric definitions and fuzzy and different system boundaries. The main objective of this thesis research was to find a new and innovative methodology to assess the rebound effect in order to improve the methodological issues of previous attempts that have analyzed the rebound effect. As a result, in reducing the causes of controversies, the controversies themselves may be reduced as well. The methodology that seems to improve the mentioned methodological issues is serious games. Serious games have several advantages that seem to fit and solve the shortcomings of the previous attempts that have analyzed the rebound effect in the past. In particular, the NRG game is the specific serious game that was used to perform the actual assessment in this research. The NRG game was used to carry out a new rebound effect assessment. In doing so, 50 people played the game in two different groups in a way to perform a modified before/after analysis: one group of 25 people using a low efficiency house and 25 people using a high efficiency house. The conclusions of this assessment showed that the rebound effect was, indeed, detected to be present when the behavior of the two groups was compared. In fact, two main signs of the rebound effect were detected. First, having a low efficiency made the low efficiency group to reduce their energy consumption more than the high efficiency group. Second, having a high efficiency made the high efficiency group to increase their comfort level (a direct measure of the luxury level of their houses in the game) more than the low efficiency group. As a result, the rebound effect was detected by keeping track of the total energy consumption and comfort level of each player. In addition, some of the methodological issues of previous rebound effect assessments that have used before/after analysis were improved, for instance, the possibility to perform ex-ante assessments, the inclusion of psychological factors of people in the results without making inaccurate assumptions and the inclusion of more than just one energy services in the experiments, among others. As a result, serious games were proven to be a handy tool to assess and analyze the rebound effect, improving the quality of previous assessments. Despite having proved the usefulness of serious games in assessing the rebound effect, some limitations of this research were identified: the sample under study was not a good representation of the population, the reliability of the rebound effect size calculation is compromised, the findings of this assessment can’t be applied in a real life context and so forth.","rebound effect; serious games; before/after analysis; energy reduction; energy efficiency improvements","en","master thesis","","","","","","","","","Technology, Policy and Management","Multi Actor Systems","","Policy Analysis","",""
"uuid:3c111194-3daa-4069-bfad-6eac52c1554a","http://resolver.tudelft.nl/uuid:3c111194-3daa-4069-bfad-6eac52c1554a","A new trend in air travel?: Exploratory research into self-help hubbing and its consequences","Talen, S.","Van Wee, G.P. (mentor)","2016","Self-help hubbing is the process of transferring between airlines that do not offer a transfer service. In theory, self-help hubbing can take place at any airport between any airline but in practice it often includes at least one low-cost carrier. Self-help hubbing is used by passengers that are searching for lower fares or routes that lack a conventional transfer connection. Passengers book at least two separate tickets and organize the transfer themselves. The decision includes the trade-off between costs, risks and the level of service. Literature seems inconclusive about the airport types that have the most potential and whether self-help hubbing has potential at all. In practice however, airports and sales channels already started delivering supporting services to improve self-help hubbing. This thesis aims at providing more insight into self-help hubbing. In particular, it aims at clarifying the differences compared to conventional transfer, estimating its potential and describing the implications for the aviation industry.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","","","","",""
"uuid:05eeff05-cbf3-4c92-8b2b-926a981e1fc6","http://resolver.tudelft.nl/uuid:05eeff05-cbf3-4c92-8b2b-926a981e1fc6","Reflective Light-Curves of Ellipsoidally-Shaped Outer Solar-System Objects and Exoplanets","Beekman, M.W.","Bouman, W.G. (mentor); Stam, D.M. (mentor); Visser, P.M. (mentor)","2016","Faculty EWI and TNW. BSc Industrial and Applied Mathematics and Applied Physics. In this thesis we consider reflective light-curves of planets, graphs of the intensity of the light originating from the parent star that the planet reflects versus time. In almost all instances, planets are modelled as spheres, like in [1] and [2]. However, planets are better approximated by ellipsoids as proved by Isaac Newton in the Principia [3]. In our own solar system, we can observe that planets are not spheres when we look at for example Jupiter [4] and Haumea [5]. We study the effect on the light-curve of the change from a spherical to an ellipsoidal model. For example, a spherical model of a homogeneous planet at edge-on observation would predict a constant light-intensity during one rotation around its axis due to the symmetry of the model. Note that we assumed here that the planet sits approximately still in the sky during one rotation around its axis. However, an ellipsoidal model of a homogeneous planet at edge-on observation would predict a variable light-intensity. [5] shows a difference between the maximum and the minimum value of the measured light-curve of Haumea of 0.32 magnitudes. This shows that an ellipsoidal model has a significant effect on the light-curve. Furthermore, in contrast to the spherical model, with the light-curves for the ellipsoidal model we can for example calculate the spin of the planet in certain cases, which makes them more interesting. By calculating the light-curve with the ellipsoidal model, we can determine the shape of planets and therefore gain knowledge about the internal structure of planets. Where others, like [5], have calculated the light-curve for an ellipsoidally-shaped planet numerically, we calculate them analytically. We accomplish this with the analytical equation of the light-curve postulated by [6]. We assume that planets have a homogeneous reflecting surface, we assume parallel incident light-rays, we assume Lambertian reflection and we assume that the planet is in a circular orbit around its parent star. We did not calculate the light-curves for non-circular orbits. However, the results can easily be modified to include elliptical Kepler orbits since the light-curve depends linearly on the orbital radius. We consider the applications of a solar system triaxially-shaped planet, like Haumea, a spheroidally-shaped exoplanet and a tidally-locked, triaxially-shaped exoplanet. We considered both edge-on and face-on observation. We confirmed the dimensions given by [5] for the dwarf planet Haumea. We found that for a given tilt of the planet’s rotation axis, there are enough measurable Fourier coefficients to determine the dimensions of the planet in each of our applications. The only exception we found is a spheroidally-shaped exoplanet at edge-on observation without tilt. In that case, we do not have enough information to differentiate between the flattening and the size of the exoplanet.","light-curve; Exoplanet; Haumea","en","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","","","Bsc Industrial and Applied Mathematics and Applied Physics","",""
"uuid:4faf0085-6e82-4647-9061-7334f99c46e7","http://resolver.tudelft.nl/uuid:4faf0085-6e82-4647-9061-7334f99c46e7","Risk based flood management of sewer systems","Baan, J.W.M.","Clemens, F.H.L.R. (mentor)","2016","Traditional sewer flood management is often norm-based. An alternative approach is risk-based sewer flood management. In risk-based sewer management it is possible to decide upon which measures to take in a more expedient way; it is possible to judge which measure is the best in reaching the goal of reducing risks due to flooding. A risk is defined as the chance that an event takes place multiplied with the effect of such an event. By analysing the type of effects and the frequency at which they occur, it is possible to compare different locations where flooding occurs. The main goal of this master thesis is to develop a method for using risk-based flood management of sewer systems. This method has to be appropriate for comparing the risks at different flooding locations and compare possible measures to prevent flooding based on their expediency. The norm used for optimizing the method is that it must result in a general and straightforward applicable method giving an unambiguous result. The following research question covers the goal of the master thesis: ‘How can risk-based flood management of sewer systems be implemented in a general applicable way?’ First an existing method for risk-based flood management is analysed. This method is used as a basis for the method developed in this thesis. In order to try to improve the existing method, the weak points of this method are altered in the method that is developed in this thesis. The method in this thesis determines the risk level using the severity score. The severity score is obtained by multiplying the occurring effects by their accompanying weights. These weights depend on the severity of effects as judged by the municipality. The risk level is then dependent on the severity score and the return period of the situation in question. Besides the risk level, also the annual expected severity score is calculated based on the severity score of a range of return periods, varying from small (0.5 years) to large (100 years). The annual expected severity score is useful to make a distinction between locations with the same risk level. In order to come to a well-founded choice regarding the design storms to use in the method, the Dutch design storms and composite design storms are compared. The underlying principles of both types of design storms are analysed and compared. Besides that, the difference between using a full precipitation series and the design storms is analysed for a couple of sewer systems. Based on these considerations, the composite design storms are used in the method. Based on a literature study the relevant effects of sewer flooding are obtained. The effects that are taken into consideration to determine the risk level are: flooding of buildings, risk of casualties, infection risk, traffic disruption and flooding of public space. For each of the effects, the way in which they can be quantified is determined. In order to be able to judge the applicability of the method, the uncertainty involved in using the 1D/2D-model and allocating the effect category weight are obtained. The model uncertainty is obtained by varying a couple of relevant model parameters and analysing the influence of this variation on the results. From this analysis it follows that the subcatchment area is the most influential parameter. The uncertainty in allocating the effect category weight is obtained by analysing the results of a questionnaire in which respondents are asked to give their judgment about the severity of the relevant effects. This analysis shows that mainly the judgments about intangible effects, like risk of casualties, infection risk and traffic disruption are very wide spread. A comparison of the influence of the model uncertainty and the uncertainty involved in allocating the effect category weights shows that the latter has a much larger influence on the risk level and annual expected severity score than the first, although the influence of model uncertainty is not negligible. The developed method in this thesis is a general applicable method for risk based flood management of sewer systems that gives unambiguous results. The main weak point of the method is the allocation of the effect category weights. In order to improve the method in future, attention has to be given to this aspect. To reduce the relative small influence of model uncertainty, a detailed investigation of the amount and characteristics of subcatchment area surcharging to the sewer system is recommended.","sewer; flood management; risk based; model uncertainty; weights","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","","",""
"uuid:98cd6569-c60b-4c5c-aab1-4211f4ce7a8e","http://resolver.tudelft.nl/uuid:98cd6569-c60b-4c5c-aab1-4211f4ce7a8e","The influence of a rotational turbine model on the mooring loads of a tidal energy converter","Zwennis, R.M.","Metrikine, A.V. (mentor)","2016","In a world with an increasing energy demand and consciousness of the long-term effects of the use of fossil fuels, offshore industries are continuously searching for new sources of energy. Marine tidal currents, which are a consequence of the gravitational forces exerted by the Moon and the rotation of the Earth, have a theoretical potential up to 120 GW. At some of these sites generating energy with the use of tidal energy converters may be economically viable. At the moment the trend tends to go to floating tidal energy converters, since they are easier to install and maintain. To be cost-competitive, more research needs to be done. The goal of this research is how a more sophisticated turbine model influences the mooring loads of a tidal energy converter. Between Texel and Den Helder in the Marsdiep a BlueTEC has been installed. An artist impression of the BlueTEC is shown below. The floating platform is a complex coupled system. The forces exerted by the turbine on the platform influence its motion behavior, but the motion behavior of the platform influences the loading on the turbine on its turn by a changing inflow velocity and angle. At the same time a control system ensures the power output of the turbine by adjusting the rotational speed of the turbine to the optimum. The horizontal motions of the floating platform are restrained by the mooring system, although also the vertical motions are influenced by the presence of a mooring system. While the mooring system influences the motion behavior of the tidal energy converter, this works also the other way around, the motions of the floater influencing the motions of the mooring system. This leads to the inflow velocity at the turbine being dependent on the current, waves and the velocity of the floater, all motions being coupled to each other. Due to non-linearity’s this coupling can only be taken into account in the time-domain. An easy way to take the coupling of the hydrodynamic forces working on the turbine into account on the motions of the floater is by applying an actuator disk model. The basic idea is that the turbine rotor is modelled as a disk with an equal area. Another model that can be applied is a quasi-static blade element momentum model. In this model the tip speed ratio dependent coefficients are taken into account. Also the generator torque working on the structure that is applied by the generator to keep the tip speed ratio of the turbine at the optimum is taken into account. This leads to asymmetrical loading of the mooring lines due to an out of plane moment of the generator torque. A certain base case is formulated and an analysis is done for the mooring loads in this base case. The largest difference in the mooring loads are seen for the maximum fairlead tensions and in the fatigue in the mooring lines. The actuator disk underestimates the loads in the mooring lines, especially for the fatigue lifetime of the lines. A parametric sensitivity study is performed, from this is concluded that these effects are the largest in case of a low current velocity with high waves and a high mooring stiffness. It must be noted that for the BlueTEC the mooring system is displacement driven.","","en","master thesis","","","","","","","","2026-08-17","Civil Engineering and Geosciences","Hydraulic Engineering","","Offshore Engineering","",""
"uuid:d64b13d7-5473-4063-80c7-e5c49b8c3d04","http://resolver.tudelft.nl/uuid:d64b13d7-5473-4063-80c7-e5c49b8c3d04","Computational aeroacoustic analysis of trailing edge noise: A Direct Numerical Simulation using the Lattice Boltzmann Method","Rooks, J.J.W.","van Zuijlen, A.H. (mentor)","2016","To investigate the mechanisms of wing trailing edge noise the aeroacoustic sound generation by a NACA 0018 has been computed with direct numerical simulations using the Lattice Boltzmann Method. Noise from the straight trailing edge wing is compared with the sound generated by a sawtooth serrated wing, which reduces the noise emission according to literature. It was found that for low frequencies the trailing edge serrations reduce the noise significantly, but show a limited (negative) effect at high frequencies.","computational aeroacoustics; trailing edge noise; trailing edge serrations; Lattice Boltzmann method; noise reduction","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics","","","",""
"uuid:f1bc7967-0ab2-4409-966a-116149abef4a","http://resolver.tudelft.nl/uuid:f1bc7967-0ab2-4409-966a-116149abef4a","Development and Validation of an End-to-End Simulator for Frequency Scanning Filtered Rayleigh Scattering Techniques","Cheishvili, K.","Schrijer, F.F.J. (mentor)","2016","An end-to-end simulator for frequency scanning methods of filtered Rayleigh scattering (FSM-FRS) has been developed during the master thesis research at the German Aerospace Center (DLR). It can model several frequency scanning filtered Rayleigh scattering methods and predict the accuracies in measuring flow temperature, pressure, and velocity. In the context of the DLR project OSIRIS, FSM-FRS probe geometries have been conceived for simultaneously measuring airplane velocity, off-board temperature and pressure in-flight with the aim of replacing current measurement sensors in the future. Based on statistical uncertainties calculated using Monte-Carlo simulations, FSM-FRS techniques, probe configurations and settings have been optimized for maximizing the expected accuracies in the above-mentioned parameters. A validation experiment has been successfully performed for checking the reliability and usability of the end-to-end simulator. It was concluded that FSM-FRS accuracy is not sufficient to replace current airplane sensors, but this technique shows very good potential for further research.","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics and Wind Energy","","","",""
"uuid:7839a871-ce63-4799-bedf-8eeec05cee8b","http://resolver.tudelft.nl/uuid:7839a871-ce63-4799-bedf-8eeec05cee8b","Empirical Investigation of Strategy-Based Lane Change Choice: A driving experiment and questionnaire","de Baat, M.J.","Knoop, V.L. (mentor); Hoogendoorn, S.P. (mentor)","2016","","lane change behaviour; microscopic simulation model; interview-based driving experiment; video-based questionnaire","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport & Planning","","","",""
"uuid:83e0fc1e-9496-419c-8b3a-7cba123707c1","http://resolver.tudelft.nl/uuid:83e0fc1e-9496-419c-8b3a-7cba123707c1","Development of a predictive kinematic model for the small overlap crash for obtaining force deformation solution spaces of grouped structural components","Heuijerjans, R.","Turteltaub, S.R. (mentor)","2016","","","en","master thesis","","","","","","","","2021-07-31","Aerospace Engineering","Aerospace Structures and Computational Mechanics","","","",""
"uuid:7de36b4e-32b5-4da7-836b-c8fa40eb254e","http://resolver.tudelft.nl/uuid:7de36b4e-32b5-4da7-836b-c8fa40eb254e","Aerodynamic Stall Modeling for the Cessna Citation II","van Horssen, L.J.","Pool, D.M. (mentor); de Visser, C.C. (mentor)","2016","In 2019 all air-carrier pilots are obliged to go through flight simulator-based stall recovery training. Therefore simulators will have to be equipped with accurate flight models at high angles of attack. This research shows which stall characteristics can be modeled using flight data from symmetrical induced, quasi-steady stalls. Furthermore a well-known stall model structure is used, based on Kirchoff’s flow separation theory. It is shown that the hysteresis effect can be estimated using quasi-steady stall maneuvers. Aerodynamic terms related to the pitch rate, however, are not identifiable using such maneuvers. Transient effects on the other hand, which are normally only expected with highly dynamic maneuvers, could be estimated using the accelerations caused by the stall buffet. Lastly a stall buffet model is proposed, based on power spectral density analysis of the acceleration measurements.","aerodynamic stall modeling; flight data model; Cessna Citation","en","master thesis","","","","","","","","2021-08-04","Aerospace Engineering","Control & Operations","","","",""
"uuid:2da6fb3f-b495-4e39-8895-bf03dffb2895","http://resolver.tudelft.nl/uuid:2da6fb3f-b495-4e39-8895-bf03dffb2895","Rotational jetting in clay","Groen, E.C.","van Rhee, C. (mentor)","2016","In dredging activities submerged jets are widely used for different purposes. Two main applications are considered in this study: 1) a jetting sword on a cable trencher and 2) jets located on a draghead. Both jetting systems are especially designed for jetting sands and achieving large excavation productions in sandy soils. However when cohesive soils have to be jetted, the production decreases drastically. In sandy soils the jet creates wide cavities, while the cavity width produced by a jet in cohesive soils like clay is very narrow. Different researchers found that the cavity depth decreases only a little by an increasing traversing velocity of the nozzle and the cavity width to remain more or less constant, i.e. the jet can process more clay than generally is supplied. This results in the fact that the highest production will be achieved at the highest traversing velocities. However, the traversing velocity of the nozzle is limited by the tool (trencher/draghead). With the use of a rotating jet, more soil is supplied to the jet. The affected area is increased and the combined traversing velocity (normal traversing part + rotating velocity part) is increased as well. In this research the possible increase in excavation production of clay of a rotating jet compared to a conventional non-rotating jet for two purposes is studied; a rotating jet on a trenching machine and a rotating jet on a draghead. This research exists out of two models and two experimental test setups. Bothmodels consist of two modules; a calculation module and a visualization module. The calculation module predicts the excavation production as a function of: 1) Jet pressure, 2) Undrained shear strength, 3) Traversing velocity, 4) Rotational velocity, 5) Nozzle angle, 6) Nozzle diameter. The model is based on the entrainment of water at the back-side of the jet and soil at the front-side of the jet. The more entrainment of soil, the greater the increase in jet mixture density and the higher the decrease in stagnation pressure, resulting in a smaller penetration depth. The visualization module shows the cavity shape based on the cavity parameters modeled in the calculation module. The jet is modeled as a cone based on the geometry calculated in the calculated module. The jet will travel along a trajectory depending on traversing velocity and rotational velocity. The trajectory were the jet has been present, is removed and visualized as excavated volume. Two experimental test setups are developed in order to validate the models. The main parameters varied were the traversing velocity, nozzle angle (45°/60°), rotational velocity and jet pressure. For the trenching setup the increase in excavation production found, lies in a range between 6 and 7 for a traversing velocity of 15 - 215 m/h. For the draghead setup the traversing velocity was varied between 35 and 1800 m/h. The increase in excavation production can be up to a factor 4. The calculation module for the trenching application is able to produce a good estimation of the excavation production for the 45° nozzle. The calculation module overestimates the excavation production of the 60° nozzle compared to the experimental results. For the draghead application the developed model is able to give results close to the corresponding experimental results for traversing velocities lower than 1000 m/h. For larger traversing velocities the model underestimates the excavation production compared to the experimental results. The results of the experiments including the developed models can be used as a base for the design of rotating jets on a jetting sword or draghead.","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Marine and Transport Technology","","Offshore and Dredging Engineering","",""
"uuid:5c637747-1c30-4898-95e9-4d3c0f6fb66d","http://resolver.tudelft.nl/uuid:5c637747-1c30-4898-95e9-4d3c0f6fb66d","Modelling the Impact of Stacking Patterns and Fractures on the Connectivity of Deltaic Reservoirs","Ricardo, M.","Storms, J.E.A. (mentor); Hardebol, N.J. (mentor); van der Vegt, H. (mentor)","2016","Deltaic reservoirs are important reservoirs for the oil and gas production. Most of the hydrocarbons produced from deltaic reservoirs are obtained from the matrix or primary porosity. Besides primary porosity, secondary porosity, formed after deposition takes place, can have a positive contribution on hydrocarbon production. One of the processes that form secondary porosity is fracturing in addition with the addition of dissolved grains and cements. However, the role of fracturing is surprisingly unknown for one of the leading reservoir types in the world. Stacking of sediment packages is cyclic base level change causes the stacking of sediment packages. It commonly occurs in the deltaic system and gives rise to a high degree of interaction between sand and shale. This interior interaction between sand and shale will affect the connectivity of the reservoir, especially vertical connectivity (it could lead to permeability baffles or barriers). Sand to sand contacts are the favourable spots for connectivity to exist, hence, it is impractical to just rely on the sand to sand contacts to maximize the hydrocarbon production. Fractures might be one of the factors that influence the reservoir connectivity besides the sandstone to sandstone contacts, especially if fractures are open and not filled by cements or minerals. The presence of fractures only gives small additions in pore volume, but the biggest impact is that the effective permeability can be increased significantly, affecting the hydrocarbon production. The permeability of fractures is very dependent on the fracture connectivity. Fracture connectivity is a function of fracture intensity, geometry, and orientation. The combination of these parameters mentioned before play a significant role for the development of self-connected clusters, which are networks of connected fractures. The more self-connected clusters within the field, the more hydrocarbons will be recovered (with the assumption of open fractures). The role of fracture connectivity is really dependent on the sandstone architecture in the deltaic reservoirs. It is significant when the reservoir distribution is scattered and no major connected sandstone bodies are formed. The occurrence of fractures, indeed, will increase the interconnectivity of the sand bodies and the effectiveness in terms of reservoir productivity. However, the role of fractures is less crucial on an evenly distributed sandstone together with the existence of major connected sandstone bodies, since the interconnectivity within the sandstone is already preserved as a factor of sedimentological domain. Fractures may only improve the effectiveness of reservoir productivity when connecting the major bodies to the non-connected minor sandstone bodies.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Petroleum Engineering","",""
"uuid:74969fe7-09f3-4b66-bdea-905d9a6f4474","http://resolver.tudelft.nl/uuid:74969fe7-09f3-4b66-bdea-905d9a6f4474","Deeltjes modellen voor de simulatie van transportprocessen: Particle models for the simulation of transport processes","van der Ende, D.G.","Heemink, A.W. (mentor); Roekaerts, D.J.E.M. (mentor)","2016","Dubbele bachelor TECHNISCHE WISKUNDE en TECHNISCHE NATUURKUNDE - Het gebeurt regelmatig: ongelukken met schepen waarbij grote hoeveelheden olie vrijkomen. De gevolgen voor het milieu bij dit soort rampen zijn immens. Het is van groot belang dat er goede methoden zijn die kunnen voorspellen hoe deze olievlekken zich verspreiden. Een veel gebruikte methode is het numeriek oplossen van een bekende transportvergelijking. Bij het oplossen van deze vergelijking komen helaas veel problemen om de hoek kijken. De numerieke schema’s die worden gebruikt zijn vaak niet massa behoudend of kunnen niet omgaan met grote concentratie gradiënten/singulariteiten. Hierdoor moet het probleem soms deels analytisch opgelost worden. Een andere methode die wordt gebruikt en wij gedurende dit project gaan onderzoeken is met behulp van een stochastisch deeltjes model dat uit deze transportvergelijking afgeleid kan worden en het pad simuleert dat verschillende deeltjes afleggen. Een groot voordeel van deze methode is dat bij het gebruik van dit model bovengenoemde problemen niet optreden. Allebei de methodes beschrijven een verspreiding van de deeltjes waarbij er vanuit gegaan wordt dat de verspreiding toeneemt met een factor die evenredig is met de de wortel van de tijd: ‘Fickian’ diffusie. Gedurende dit project is eerst gekeken of het deeltjes model volgens ‘Fickian’ diffusie in staat is om oplossingen te benaderen van ingewikkelde transportproblemen. Door het onderzoeken en simuleren van zo’n moeilijk transportprobleem (dit probleem bevat grote concentratie gradiënten en kan numeriek niet met de bekende methode worden opgelost) kunnen we concluderen dat het deeltjes model hier inderdaad toe in staat is: het kan naar de analytische oplossing van dit moeilijk transport probleem convergeren. Het deeltjes model kan op ontzettend veel van dit soort problemen losgelaten worden. Met behulp van het deeltjes model kunnen dus veel complexe transport problemen opgelost worden die vaak deels niet of zelfs helemaal niet numeriek op te lossen zijn. Aangezien in werkelijkheid de verspreiding van deeltjes zich vaak anders blijkt te gedragen, de zogenaamde ‘non-Fickian’ diffusie, is vervolgens op verschillende manieren geprobeerd het deeltjes model aan te passen zodat de verspreiding van de deeltjes realistischer gaat. Allereerst is het wijzigen van de diffusiecoefficient naar een contrast-patroon onderzocht, waarbij op sommige plekken de diffusiecoefficient als een hoge waarde is gedefinieerd en op andere plekken er omheen als een lagere waarde. Ook is er gekeken of het toevoegen van random sprongen een toegevoegde waarde heeft. Vervolgens is de diffusiecoefficient als een patroon met ‘geulen’ met lage diffusiecoefficient en ernaast langwerpige gebieden met hogere diffusieceofficient onderzocht. Als laatste is een model onderzocht (het ‘vlucht’ model) waarbij de deeltjes hun initiële snelheid ‘onthouden’. Concluderend gaven het diffusiecoefficient geulenpatroon en het ‘vlucht’ model het beste de meer realistische verspreiding weer van deeltjes volgens de ‘non-Fickian’ diffusie.","","nl","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","","",""
"uuid:3abf3659-3546-4250-9d2b-cdc0d01dc4f7","http://resolver.tudelft.nl/uuid:3abf3659-3546-4250-9d2b-cdc0d01dc4f7","Predicting dairy cow parturition using real-time behavior data from accelerometers: A study in commercial setting","Santegoeds, O.J.","Cunningham, S.W. (mentor); Verbraeck, A. (mentor); Warnier, M.E. (mentor); Meijer, L.M. (mentor); Harbers, A.G.E. (mentor)","2016","","real-time data analysis; prediction; machine learning; accelerometers; calving; cow; dairy; behavior","en","master thesis","","","","","","","","","Technology, Policy and Management","Engineering and Policy Analysis","","Engineering and Policy Analysis","",""
"uuid:65d6c2ba-3421-4c4b-a358-0891e79a7652","http://resolver.tudelft.nl/uuid:65d6c2ba-3421-4c4b-a358-0891e79a7652","Computational modelling of Dielectric Barrier Discharge Plasma Actuators for Aerospace Flow control applications","Syed, Z.A.","Kotsonis, M. (mentor)","2016","","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy & Propulsion","","","",""
"uuid:ab8ca849-f728-4cf8-8acc-c24e62426af9","http://resolver.tudelft.nl/uuid:ab8ca849-f728-4cf8-8acc-c24e62426af9","Effect of hydrogen addition on performance and emissions of marine turbocharged diesel engines: In search of the catalytic effect of hydrogen addition","Kes, W.","Visser, K. (mentor)","2016","","","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Marine and Transport Technology","","Ship Design, Production and Operation (SDPO)","",""
"uuid:e4f53a94-348b-4cba-8496-79cf1b454940","http://resolver.tudelft.nl/uuid:e4f53a94-348b-4cba-8496-79cf1b454940","Development of a low-cost Fiber Metal Laminate with a focus on low-velocity impacts","Simoiu, K.M.","Poulis, J.A. (mentor); Gransden, D.I. (mentor)","2016","The development of Fiber Metal Laminates (FMLs) allowed designing lightweight structures that are more impact and fatigue resistant than the individual materials they are made of. However, high production costs restrict the use of FMLs to the aeronautical industry. A reduction in production costs can pave the way for applications in other fields as well. FMLs made of low-cost thermoplastic (TP) prepregs were produced with a heated press in order to reduce production costs. PA6 and PC in combination with an E-glass fabric were chosen for the composite layers and Al-2024-T3 for the aluminum layers. Al-2024-T3 was artificially aged to study the influence of temperature and time on the tensile properties of the alloy. Interlaminar shear strength tests were used to assess qualitatively the influence of process temperature, pressure, time and fabric type on the interface strength. This preliminary study demonstrated that aluminum underwent precipitation hardening. In addition, the combination of composite consolidation and aluminum-composite bonding in one production step (Production 1) resulted in FMLs with large areas of dry fibers. As a consequence, a second production was proposed (Production 2), in which the composite is first produced in the press and then adhesively bonded to the aluminum layers with an adhesive hot melt. Following this experimental study, the aluminum-composite interface strength of the TP FMLs was assessed quantitatively. The assessment was based on single cantilever beam tests, which were used to determine the interfacial fracture energy. Fracture energies in the range of 2500-3200 J/m2 were observed in the experiments. These relatively high values could be explained by plastic deformation of the adhesive at the interface, which failed in a cohesive manner. Low-velocity impact tests were performed with a drop-tower. Samples from Production 1 showed a stiffer response and less energy absorption through plastic deformation of the aluminum layers compared to specimens from Production 2. This behavior was attributed to precipitation hardening of the aluminum layers of Production 2. In addition, a smaller damage area was observed for Production 2 samples, which still had a 60% larger damaged area compared to GLARE. However, a PA6 FML that absorbed the same amount of impact energy for the same damage area as a GLARE reference laminate would cost around 23% less. It shows that TP FMLs have the potential to be a low-cost alternative to impact resistant, commercial FMLs for low-velocity impacts.","","en","master thesis","","","","","","","","","Aerospace Engineering","Structures & Materials","","","",""
"uuid:f6b5bcbb-e894-4361-a1d0-48e6429e3ae4","http://resolver.tudelft.nl/uuid:f6b5bcbb-e894-4361-a1d0-48e6429e3ae4","Supersonic Turbine Design using Method of Characteristics","Anand, N.","Pecnik, R. (mentor); Pini, M. (mentor); van Buijtenen, J.P. (mentor); Otero, G.J. (mentor)","2016","","Supersonic; MoC; ORC; Radial inlet Turbine","en","master thesis","","","","","","","","2021-08-31","Mechanical, Maritime and Materials Engineering","Process and Energy (P&E)","","","",""
"uuid:ab6e8897-88ad-4107-9c6f-950124a11ede","http://resolver.tudelft.nl/uuid:ab6e8897-88ad-4107-9c6f-950124a11ede","A strengthening system for Groningen's unreinforced masonry buildings: Fibre Reinforced Cementitious Matrix: Analytical and Numerical analysis","Conserva, G.","Palmieri, M. (mentor); Marasca, A. (mentor); Rots, J.G. (mentor)","2016","Existing masonry buildings represent one of the most vulnerable building typology if subjected to seismic actions. In the Groningen region a greater part of the building stock is represented by URM masonry buildings, where these structures were generally designed for gravity loads only. As a consequence of the induced seismicity in the study area, the strengthening of this class of structures is key factor. In particular, these URM buildings are characterized by poor masonry quality and not effective connections between vertical and horizontal elements, thus their strength cannot be activated to resist the seismic action as a whole system, preventing out-of-plane mechanisms and local collapses. This research concerns analytical analysis about in-plane capacity of masonry walls strengthened through an innovative technique employing fibres grids embedded in cementitious matrix layers applied on the wall’s surface. Externally bonded reinforcement with Fibre Reinforced Cementitious Matrix (FRCM) are system now widely used to repair and strengthen the existing structures. A Finite Element Model of unreinforced and reinforced walls has been developed and parameters defining masonry and strengthening materials have been verified by means of comparisons with the analytical results. The effectiveness of the FRCM strengthening system has been numerically and analytically investigated through a wide parametric analysis varying the wall slenderness ratio, the overburden load, the FRCM configuration, the masonry material properties and the FRCM material properties. The results evidence that, debonding is the governing failure mechanism for strengthened masonry piers before collapse occurs. They also confirm that for the specific cases study, the FRCM strengthening system added substantial increase to the in-plane capacity and deformation capacity of the masonry pier, showing thus the effectiveness of the FRCM as external reinforcements for masonry structures. Finally, suggestions for further research and improvements are also reported.","Unreinforced Masonry; FRCM; Debonding; In-plane capacity; Groningen; Earthquakes","en","master thesis","","","","","","","","2021-08-16","Civil Engineering and Geosciences","Structural Engineering","","","",""
"uuid:041645ca-928a-458b-8729-608c8f1275b7","http://resolver.tudelft.nl/uuid:041645ca-928a-458b-8729-608c8f1275b7","Analysis and Design of a 2.5GS/s 6-bit SAR ADC with a 3-bit/cycle Resolving Scheme","Ursulean, M.","Pertijs, M.A.P. (mentor)","2016","The thesis analyzes the design challenges that arise when developing high-speed ADCs and shows, through an extensive architecture study, that the SAR topology can be used with a sampling rate of 2.5GS/s if asynchronous processing and a multi-bit per cycle approach are adopted. The transistor-level implementation and simulation of a 6-bit SAR ADC are summarized in order to expose the existing trade-offs in terms of power consumption, speed and area.","TI-ADC; ADC; Analog-to-Digital Converter; SAR; StrongArm Comparator; asynchronous; capacitive DAC","en","master thesis","","","","","","","","2021-8-16","Electrical Engineering, Mathematics and Computer Science","Microelectronics","","Electronic Instrumentation Laboratory","",""
"uuid:11d02643-3a89-4015-80ea-ca4db49bdec6","http://resolver.tudelft.nl/uuid:11d02643-3a89-4015-80ea-ca4db49bdec6","Modeling of a Twisted and Coiled Polymer Muscle","Smit, B.","Vallery, H. (mentor)","2016","A new type of polymeric artificial muscle was recently discovered. This muscle is based on twisted and coiled polymer fiber and is actuated thermally. It has interesting properties, such as low costs and high work densities. Based on fabrication methods, two variations can be considered. The autocoiled variation has a larger load capacity and the mandrel coiled variation achieves larger maximum contractions. This paper aims to model the force increase of a mandrel coiled variation by considering submodels in three physical domains. Multiple models are compared experimentally by the identification of parameters. The model which is considered to be best applicable is the extended Voigt model to describe force increase as function of imposed mechanical strain and temperature increase. The temperature increase is best modeled as a first order function of input power. The model is demonstrated to be of use in feedforward control and to observe muscle temperature.","TCPM; artificial muscle; modeling; control; parameter estimation","en","master thesis","","","","","","","","2021-08-16","Electrical Engineering, Mathematics and Computer Science","Biomechanical Engineering","","Biomechanical Design, Biorobotics","",""
"uuid:e2c73349-91f6-4db7-9f24-cd90f4cbf167","http://resolver.tudelft.nl/uuid:e2c73349-91f6-4db7-9f24-cd90f4cbf167","Global Robust Optimization of Computationally Expensive Systems: A Lavel Rotor Suspended by Fluid Film Bearings","Immerzeel, F.C.","van Ostayen, R.A.J. (mentor); Langelaar, M. (mentor); Eling, R.P.T. (mentor)","2016","Kriging based methods enable the deterministic and robust optimization of computationally expensive systems. With a limited amount of function evaluations the optima are found in an iterative process with expected improvement as infill sampling criteria. Outputs of computer models can be stochastic and/or the models do not always succeed to perform the analysis. For the latter case, a problem is said to be affect by a hidden constraint. Regression Kriging can be included in the methods to deal with the stochastic model outputs. A new method is introduced to handle the hidden constraint in both deterministic and robust optimization. The methods are used to optimize a validated model of a Laval rotor suspended by plain journal bearings. To capture the non-linear behaviour of the self-excited vibrations, a computationally expensive time-transient run-up analysis needs to be performed. The output of this model is stochastic and the model fails to perform a run-up for some combinations of model inputs. The most influential control variables and uncertainties are indicated with an efficient global sensitivity study and used to optimize the system. With the extension of the deterministic and robust optimization, the optima are successfully obtained and can be compared.","robust optimization; efficient global optimization; metamodel; Kriging; hidden constraint; Laval rotor; hydrodynamic bearing; whirl; whip","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering (PME)","","Engineering Mechanics","",""
"uuid:8a18af46-dcc1-495f-8e84-a99604f7e11c","http://resolver.tudelft.nl/uuid:8a18af46-dcc1-495f-8e84-a99604f7e11c","Influencing pedestrain route choice in the railway station by management measures to decrease walking delay","Schotman, L.","Daamen, W. (mentor); Hoogendoorn, S.P. (mentor); Sarvi, M. (mentor); Wiggenraad, P. (mentor)","2016","","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Transport & Planning","","","",""
"uuid:84e73a8e-d143-4d58-ac93-4b74e38653d7","http://resolver.tudelft.nl/uuid:84e73a8e-d143-4d58-ac93-4b74e38653d7","Modelling of Modular Multilevel Converters for Fast and Accurate Simulation of Electromagnetic Transient Phenomena in HVDC Applications","Khan, S.S.","Bauer, P. (mentor); Tadeschi, E. (mentor)","2016","Modular multilevel converters (MMC) are gradually becoming the technology of choice in high-voltage direct current (HVDC) power transmission for grid integration of large-scale offshore wind farms and multi-terminal HVDC transmission schemes. Each phase of an MMC HVDC terminal consists of several hundreds of identical submodules that switch a module capacitor in and out of the circuit to synthesize near perfect sinusoidal ac voltages. This operation requires sophisticated control and modulation techniques. Furthermore, the design of converter, validation of control system and operational planning requires robust simulation models. To precisely model the switching operation in the converters electromagnetic transient solvers are employed. These solvers utilize numerical integration methods and nodal analysis to resolve the system in time domain. However, the design of MMC poses a computational challenge to the classical electromagnetic transient simulations techniques. Independent operation of submodules necessitates explicit their modeling, which leads to hundreds of nodes and semiconductor devices in the equivalent models. This time-varying topology with hundreds of nodes leads to the excessive computational load on the electromagnetic transient solvers. To address this, existing literature proposes numerous efficient equivalent models for MMC based on submodule/arm's Thevenin's equivalence, switching function or average representations. The desired equivalent models for EMT studies are expected to reproduce internal and external dynamics of the converter in stationary and transient conditions. However, many of the existing models in literature lack the capability to capture the blocked state of operation of submodules, which finds application in the dc fault operation of the converter. Furthermore, with an extensive collection of proposed models, existing literature lacks an independent collective objective comparison of proposed models, which enable suitable selection of model based on the simulation needs. In light of these requirements, this thesis presents a comprehensive review and enhances models from the existing literature. Moreover, using PSCAD/EMTDC simulations the proposed models are evaluated against the detailed model of the converter regarding accuracy and computational load. The simulations confirm the ability of enhanced models to capture the dynamics of converter under stationary and severe transient conditions. Furthermore, based on the simulation methods and results, the thesis addresses limitations of the proposed models and presents recommendations for their simulation applications. In addition to the specific focus on electromagnetic transient simulation models, the thesis further aims to serve as a tutorial for the MMCs technology. Therefore the thesis presents a detailed account of MMC operation, and its associated control and modulation system.","High-voltage direct current (HVDC); Modular multilevel converters (MMC); Thevenin's equivalent model; Switching function models; Average value models; Electromagnetic transients","en","master thesis","","","","","","","","2018-08-15","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","Erasmus Mundus, European Wind Energy Master (EWEM), Electric Power System Track","",""
"uuid:dee8f5d8-711e-4992-b45b-9734cb054038","http://resolver.tudelft.nl/uuid:dee8f5d8-711e-4992-b45b-9734cb054038","Potentiality of a velocity profiler to investigate sewers: Results of laboratory experiments","Cedillo Galarza, J.S.","Clemens, F.H.L.R. (mentor); Luxenburg, W. (mentor); Lepot, M.J. (mentor)","2016","In order to propose a new sewer inspection method, a velocity/turbidity profiler (Ubertone, UB Flow F-315) has been tested in a laboratory. A 50 m glass flume has been adapted with several lateral connections (with a range of diameters, angles, intrusions, cracks), supplied by a 1 m3 tank. Placed just below the free surface on a rotating (to scan the wet section) and translating (along the main axes of the flume) structure, velocity profiles have been recorded and accurately positioned along the reach (with data from three laser distance meters and a 3 Mpix camera): a 3D cloud of raw velocities is created. After raw data pretreatment (deduction of translation velocity, Nyquist jumps correction), a five step-interpolation (adapted from [16]) method has been implemented and tested: i) data filtering, ii) transformation to flume coordinates velocities, iii) isotropic gridding, iv) anisotropic gridding and v) continuity correction. In order to perform the last step, two resolution schemes have been tested: staggered and non-staggered grid. With external CFD data, the first one shows its superiority (stability) on the second one and provide consistent results to data obtained from commercial CFD software. Despite the UB Flow provides good average data, its design and instantaneous velocities make it no suitable yet for field application.","sewer systems; staggered grid; non-staggered grid; finite difference stability; Navier-Stokes equations; validation with CFD data","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","","",""
"uuid:e5604e9a-c241-4236-83dd-5fc823e7e284","http://resolver.tudelft.nl/uuid:e5604e9a-c241-4236-83dd-5fc823e7e284","Mission Planning for Sensor Network Deployment using a Fleet of Drones","Goyal, P.","Hoekstra, J.M. (mentor); Blacquiere, G. (mentor); de Croon, G.C.H.E. (mentor); Smeur, E.J.J. (mentor)","2016","Various methods for route planning of on-road vehicles to serve transportation requests have been developed in the literature in order to reduce transportation and operational costs. The applicability and thus development of these methods is primarily motivated by the field of application. This article deals with the mission planning for a fleet of drones to deploy sensors in a network. In particular, they are conceived to complete the task of delivering geophones in the seismic surveys. Unlike conventional on-road vehicles used for delivery purposes, every drone in the fleet is constrained to make a frequent return trip back to the depot to pick-up a new payload and restore its battery. A centralized planner is proposed in this article due to this constraint. The problem of planning is decomposed into two phases: route formation and route scheduling. The first phase is handled using the extensive formulation of Multi-Trip Vehicle Routing Problem (MTVRP) aiming at minimizing the overall journey time. A heuristic method is also proposed for this phase which provides near-optimal solutions in a computationally efficient manner. The second phase of the planning algorithm deals with the unaddressed problem of depot congestion arising due to the frequent visits of each drone to the depot. This problem is expressed in the form of a Mixed-Integer Linear Program (MILP) that can be solved using available software. This phase is computationally intensive and comparatively slow which restricts the usage of this mission planner in the re-planning phase to the cases involving longer journeys with limited number of routes. The results from a flight-test are also presented in order to demonstrate the mission planner.","","en","master thesis","","","","","","","","","Aerospace Engineering","Control and Operations","","","",""
"uuid:b1c2a74b-7f13-4e5a-b93d-dcb43feb1a16","http://resolver.tudelft.nl/uuid:b1c2a74b-7f13-4e5a-b93d-dcb43feb1a16","Improving the implementation of preventive interventions in the Dutch health sector: A study exploring influencing factors to improve the implementation of preventive interventions in the Dutch health sector","Diek, S.","van Geenhuizen, M.S. (mentor); van Hulst, B.L. (mentor)","2016","","","en","master thesis","","","","","","","","","Technology, Policy and Management","Values Technology and Innovation","","Management of Technology","",""
"uuid:43335229-f712-40b6-8bc9-ef7b499a6d58","http://resolver.tudelft.nl/uuid:43335229-f712-40b6-8bc9-ef7b499a6d58","Responsible Digital Hospitality Quality Measurement to Facilitate Reflective Communication","Stolk, K.","Flipse, S.M. (mentor)","2016","In this study, a unique combination of concepts such as hospitality, digitalisation and responsible research and innovation is brought together in the Responsible Digital Hospitality Quality Scale. This conceptual tool is more than a website benchmark or customer satisfaction measurement tool. It measurably defines responsible digital hospitality in six key domains and twenty-three elements using a visual representation of individual element scores and thereby enables digital brand owners, designers and communication experts to constructive communication and self-reflection on a status of a digital brand. The aim of reflexivity and real time assessment are tested in a small focus group session with digital experts. Literature study and expert insights are combined in an iteration process of tool design in order to develop a valid tool in survey format. Additionally, a theoretical framework is developed that combines theories of electronic service quality, user experience, RRI and interactivity. This framework could possibly function as talkative tool alone, however is not further researched within this study. More extended research on figure representation and inclusion of fun elements could provide extra value to the RDH quality tool design in the future.","","en","master thesis","","","","","","","","","Applied Sciences","Science Education and Communication (SEC)","","","",""
"uuid:0254c08e-4718-4737-9807-387ea30da4cc","http://resolver.tudelft.nl/uuid:0254c08e-4718-4737-9807-387ea30da4cc","Stability Analysis of a Laval Rotor on Hydrodynamic Bearings by Numerical Continuation: Investigating the influence of rotor flexibility, rotor damping and external oil pressure on the rotordynamic behaviour","van Breemen, F.C.","Eling, R.P.T. (mentor)","2016","Self-excited vibrations in Laval rotors supported by hydrodynamic bearings, know as oil whirl, induce friction losses and noise, or may even damage the system. This transient dynamic behaviour is caused by the stable and unstable equilibria in the system. To study this phenomenon, this research uses numerical continuation. This method determines the equilibria in the system, their stability and the bifurcations into other equilibria. The sensitivity of these properties with respect to the parameters of the system are studied. The parameters under consideration are the system geometry, rotor stiffness, rotor damping and external oil supply. For this purpose analytic models with the use of short bearing theory and Guembel boundary conditions are made of a rigid, flexible and damped rotor. It extends the results from literature beyond the complications in the model due to the use of numerical continuation, which others did not acknowledge. The novel contribution of the rotor including the damping of the shaft showed to be essential to detect the same dynamic behaviour seen by experimental results from literature. With the addition of the oil feed pressure, the results are strongly improved and are shown to predict the dynamic behaviour of a high speed Laval rotor with reasonable accuracy.","Numerical Continuation; Laval Rotor; Jeffcott Rotor; Stability Analysis; Hydrodynamic Bearing; Short Bearing Approximation; Rotordynamic Behaviour; Oil Whirl; Limitcycle; Bifurcation Detection; MATCONT","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering","","Engineering Mechanics","",""
"uuid:7e5b83c7-fb42-46f4-a85f-a0dab2a3f178","http://resolver.tudelft.nl/uuid:7e5b83c7-fb42-46f4-a85f-a0dab2a3f178","Simulating flutter for a 100-m wind turbine blade: using a CFD-CSD coupled method in OpenFOAM","","Van Zuijlen, A.H. (mentor)","2016","In this study a full CFD-CSD FSI method is developed, extended with a modal approach, to assess the potential flutter behavior of the SNL 100-m wind turbine blade. The FSI method is based on the existing FOAM-FSI framework and extended with a non-linear beam model. The analysis consists of four steps: non-linear steady state FSI deformation of the blade; extracting force distribution gradients of blade deformations along eigenmode shapes; linearize the flow around the steady state using a modal approach; ans simulate the fully linearized model to assess the stability of the system. The method is applied to the SNL 100-m blade in a parked configuration, with nonturbulent, non-yawed inflow wind speeds of 50m=s and 70m=s. The results show a stabilizing effect of the flow for both cases. A further analysis of the modal model suggests that the coupling between the flapwise bending modes and torsional modes are indeed strong, but do not cause an instability for the test case considered. The results of the modal analysis are lightly verified with full CFD results.","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy, Flight Performance and Propulsion","","","",""
"uuid:04cc675c-8aa5-4c74-acbe-466923589bb4","http://resolver.tudelft.nl/uuid:04cc675c-8aa5-4c74-acbe-466923589bb4","Load principles for drive mechanisms of Miter gates: Assessing suitablitliy of current codes and guidelines for drive mechanisms","Gerritse, A.R.","Jonkman, S.N. (mentor); Molenaar, W.F. (mentor); Verheij, H.J. (mentor)","2016","In recent years, in a number of projects gate drive mechanisms of navigation locks were checked or adapted for a longer service life, during which the reliability, safety and availability of the structure had to be guaranteed for an extended period. Several problems were encountered. When verifying the gate drive mechanism, it was found that the drive mechanisms, which have performed well for years, do not meet the current design criteria. According to the current design criteria the capacity of the drive mechanism would be insufficient. The design force according to the latest criteria exceeds the maximum allowed force on the gate, and cannot be applied without major adaptations to the gate and lock head structure, hence, results in high costs. In coming years a series of approximately 50 locks require replacement or large scale maintenance, it is worth considering the apparent contradiction between drive mechanisms that performed satisfactorily for years against the much higher required capacities by the nowadays codes and guidelines, giving rise to the question; Is the approach as presented in the current codes and guidelines, in specific the ROK 1.3 (Richtlijn Ontwerpen Kunstwerken, issued by Rijkswaterstaat), suitable for the design of drive mechanisms? This study presents a case study in order to answer this question. The case study focuses on a navigation lock containing Miter gates. The original design calculations dating from 1974 are compared to an updated calculation according to ROK 1.3. Both calculations are subsequently compared to a more tailored calculation approach that accounts for site-specific forces. This third approach explained some of the key differences between the fully functioning drive mechanism with the capacity as designed in 1974 and the updated calculation applying ROK 1.3. These differences primarily originate in the schematization of the forces affecting the drive mechanism and the resulting combination of forces. Applying the ROK 1.3 resulted in a conservative estimation. Differences in order of magnitude are mostly explained by the adjusted regulations. In the past external forces were not accounted for in the calculations and the reliability requirements have become more stringent.Furthermore, operational requirements were not accounted for in the design phase. Navigation locks have the following main functions: navigation, water management and flood protection. Not defining properly which main function sets the safety requirement for the construction can result in a misjudgement of the loads that affect the navigation lock. ROK 1.3 builds on principles and guidance for the different main functions separately, and these are not harmonized. The requirements for the different main functions relating the availability and structural reliability all translate into partial load factors that are aggregated into a reliability-index. Where one of the requirements leads to the normative reliability-index It is important to account for the different limit states when increasing availability of the system and, contrary to common practice in calculations for civil components, to increase the partial safety factors in the serviceability limit state. This will increase the safety mechanism of the system leading to higher availability. The regulations in ROK 1.3 are useful in designing drive mechanisms as long as the main function that sets the safety constraint of the construction is taken into account. It is beneficial to include site-specific conditions and to estimate the load components more accurate. Although ROK 1.3 does not explicitly discourage this approach, nor does it elaborate on it. At present another more sophisticated approach is not facilitated by the ROK.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","","",""
"uuid:0ced7c24-50e2-4101-8474-382bda41195c","http://resolver.tudelft.nl/uuid:0ced7c24-50e2-4101-8474-382bda41195c","Direct statics estimation from ground-roll data","Bamarouf, T.","Draganov, D. (mentor); Valentina Socco, L. (mentor)","2016","• Statics correction can be computed by inverting the dispersion curves of surface waves (groundroll). • Inversion results generally require intensive processing and are affected by solution nonuniqueness. Here, we directly estimate the statics correction from the dispersion curves of surface waves, without the need for inverting the data. This can be achieved by finding the relationship between the time average S-wave velocity at different depths and the surface-wave phase velocity at different wavelengths. The same approach is found for the estimation of weighted average Pwave velocity, by exploiting the sensitivity of the wavelength/depth relationship to Poisson’s ratio. Prior information of the S-wave velocity profile corresponding to only one of the dispersion curves available in the area is required. We show on a synthetic dataset the possibility of extending this method to higher modes of surface waves. In case of a narrow frequency band, higher modes can investigate deeper; however, the fundamental mode showed better estimation of the near-surface corrections in a broader frequency band. We show the successful application of this method on an oil and gas exploration dataset in a sand dune area. By having only the ground roll in the data and a single profile of VS, we directly estimate the P- and S-wave statics correction. A comparable analysis between fundamental and higher modes of ground roll for the estimation of statics correction showed that using higher modes has the least error estimation, when compared to fundamental mode, in unconsolidated granular environment.","Higher modes; statics; ground-roll; near surface; Rayleigh waves","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","","",""
"uuid:9d591a2c-b8b4-4d84-91bb-e93c48bc9286","http://resolver.tudelft.nl/uuid:9d591a2c-b8b4-4d84-91bb-e93c48bc9286","Analysis of P- and S-wave velocities in the COSC-1 borehole, central Sweden","Ooms, M.C.","Juhlin, C. (mentor); Schmelzbach, C. (mentor)","2016","The Collisional Orogeny in the Scandinavian Caledonides (COSC) drilling project, supported by the International Continental Drilling Program (ICDP) and Swedish Research Council, was designed to study mountain building processes and the geologic structure and composition of the Caledonian orogeny in central Sweden. The 2.5 km deep scientific COSC-1 borehole (ICDP 5054-1-A) was successfully drilled during the spring and summer of 2014, targeting the Lower Seve Nappe Complex (SNC). With a variety of conducted seismic studies, nearly 100% core recovery, downhole well-logging and on-core measurements through the Lower SNC a comprehensive data set was obtained. This study will focus on the analysis of seismic P- and S-wave velocities of the full-waveform sonic (FWS) data set. The resulting velocities, vp and vs, have been used in seismic models and for comparison to other relevant studies conducted in the COSC-1 borehole. vp and vs showed correlation with velocities obtained from Zero-Offset VSP (ZVSP) and core measurements, indicating a varying vp around a constant velocity and a gradually increasing vs with depth. Correlation has been found between a gradually decreasing vp/vs ratio and an increasing SiO2 content with depth, implying a gradual transition in the middle crustal composition from a mafic to a felsic environment with an average SiO2 content of 69.89±9.23 wt% and vp/vs ratio of 1.70±0.14. Seismic modelling of the derived vp, vs and density resulted in 1D synthetic seismograms and a 2D elastic and isotropic finite-difference model, which are correlated to ZVSP and surface seismic. Seismic modelling obtained similar results as the ZVSP, both indicating a reflective Lower SNC. Reflective interfaces in the basal shear zone (>1710 m) are interpreted as due to amphibole-rich gneiss and mica schist interfaces. Similar reflectivity sequences are found in the 3D migrated section, but yielding a poorer correlation with the seismic models. Comparisons of the FWS data set show good correlations with other seismic methods which can be used in the further development of COSC, but also differences which may have to be accounted for in the future.","full-waveform sonic logging; seismic modelling; collisional orogeny; Scandinavian Caledonides; scientific drilling; COSC-1 borehole","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","Applied Geophysics","","63.40163,13.20293"
"uuid:b9b9a646-0988-46fc-9e34-b750f5ecf3b1","http://resolver.tudelft.nl/uuid:b9b9a646-0988-46fc-9e34-b750f5ecf3b1","Development of a Coupled Fluid-Structure Simulation Method in the Frequency Domain","Berthold, C.R.","Boersma, B.J. (mentor)","2016","In this work a numerical method is developed to efficiently simulate limit cycle oscillations and forced response amplitudes of fluid-structure interaction (FSI) problems. The method computes a solution in the frequency-domain and can therefore only deal with periodic unsteadiness. As flow solver the Harmonic Balance method is used which is coupled to structural eigenmode dynamics. In this work the motion of the structure is approximated by one structural eigenmode only. The Harmonic Balance method is a nonlinear frequency-domain method. The newly developed method should be able to determine the resulting vibrational amplitude of a fluid-structure problem more efficiently than a coupled time-domain simulation. The resulting vibrational amplitude of the structure can then be used to evalute a mechanical design with respect to possible damage and/or High Cycle Fatigue. Possible applications of this method lie in the field of aeroelasticity, e.g. the investigation of flutter and forced response situations of blades in turbomachinery. To test the method it is applied to two 2D turbomachinery testcases and also to an elastically-mounted cylinder with vortex shedding. The main topics of this thesis are: 1) Analysis of the model equations and development of a numerical strategy which is solved with a pseudo time stepping method. 2) Investigation of the vibrational frequency of a coupled fluid-structure system which is in general not equal to the eigenfrequency of the structural eigenmode. 3) Application of the method to the testcases and validation against results from time- domain computations. It turns out that the developed method is able to predict forced response amplitudes (Synchronous Vibrations) very efficiently. The computational costs of the frequency-domain method are very promising. Non-Synchronous Vibrations (NSV) are also investigated with the developed method. The limit cycle oscillations of aerodynamically unstable modes of a blade row can be efficiently simulated. In case of a flutter investigation the method is able to predict the frequency of the coupled FSI problem as well as the resulting vibrational amplitude very efficiently. To investigate NSV which are dominated by a flow instability the method is applied to an elastically-mounted cylinder with vortex shedding in the lock-in regime. In this case the method fails in determining the frequency of the unsteadiness but if the correct frequency is known a priori the method is still able to compute the amplitude of the resulting limit cycle oscillations.","Fluid-Structure Interaction; Frequency Domain; Turbomachinery; Forced Response; Flutter; Aeroelasticity; Nonlinear Frequency Domain; Harmonic Balance","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Process and Energy (P&E)","","","",""
"uuid:b5c21532-d84e-49f0-8d18-a181b02e09ac","http://resolver.tudelft.nl/uuid:b5c21532-d84e-49f0-8d18-a181b02e09ac","Finite element discretization of the Spalart-Allmaras turbulence model","Kessels, P.C.J.","Hulshoff, S.J. (mentor)","2016","The Reynolds-averaged Navier-Stokes equations are numerically solved in a segregated manner, along with the Spalart-Allmaras turbulence model. The Galerkin as well as multiple stabilized finite element formulations are derived for the Spalart-Allmaras turbulence model, and tested for robustness. For a turbulent channel flow and a backward facing step case, the Galerkin approach shows to be quite robust by itself, but the multiscale stabilized formulation shows superior robustness in the case of the backward facing step problem, while mostly preserving accuracy.","finite element method; turbulence modeling; Spalart-Allmaras; multiscale; Reynolds-averaged Navier-Stokes","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy, Flight Performance and Propulsion","","Aerodynamics","",""
"uuid:8ce01676-1ff5-4d3e-9d77-79d8d3f9a68c","http://resolver.tudelft.nl/uuid:8ce01676-1ff5-4d3e-9d77-79d8d3f9a68c","Relating Robotic- and Ultrasound-Based Techniques for Characterizing the Effects of Changes in Muscle Properties Following Stroke","Gerritsen, N.T.A.","Dewald, J.P.A. (mentor)","2016","Background: Following a stroke, muscle tissue is prone to atrophy, decreased fascicle length, loss of sarcomeres, and a possible enhancement of collagenous extracellular matrix. Decreased fascicle length associated with a loss of sarcomeres, and a possible enhancement of collagenous extracellular matrix are thought to result in an increase in passive stiffness and therefore contribute to movement dysfunction. Valid and reliable in-vivo, non-invasive, and cost-effective assessment of muscle stiffness eludes conventional rehabilitation practice. Ultrasound-based shear wave elastography is a new measurement technique; capable of measuring muscle elasticity however evidence for criterion validity is lacking for paretic muscle in individuals with stroke. Thus our aim was to (1) to investigate the concurrent/criterion validity of shear wave elastography, for estimating the passive stiffness of biceps, with an established robotic perturbation metric for quantifying joint stiffness in individuals with severe chronic stroke and (2) confirm that both metrics were capable of detecting a difference between paretic and non-paretic arms. Methods: Passive elbow stiffness during extension was quantified using a robotic device that administered a controlled elbow joint extension perturbation. Passive biceps elasticity was quantified using a conventional ultrasoundbased shear wave elastography. Stiffness values were extracted at 7 elbow joint positions in the paretic and non-paretic arms of 10 stroke patients. Sophisticated post-processing of muscle EMG was administered to confirm muscle quiescence during all measurements. Results: There was a significant correlation between passive biceps elasticity and joint stiffness during elbow extension with a strengthening relationship approaching end range extension. Both metrics identified increased passive stiffness with elbow extension joint stiffness 2.26 times greater and biceps stiffness 1.19 times greater on the paretic side. Conclusion: Shear wave elastography appears to be a valid and viable tool for the estimation of muscle stiffness in individuals with severe chronic stroke. The ability to quantify muscle elasticity will expand the depth and breadth of the clinician’s evaluation and offer a new target for therapeutic intervention.","","en","master thesis","","","","","","","","2018-08-12","Mechanical, Maritime and Materials Engineering","Biomedical Engineering","","","",""
"uuid:535d9a98-65ee-4a18-91f0-560562c43a3a","http://resolver.tudelft.nl/uuid:535d9a98-65ee-4a18-91f0-560562c43a3a","Parallelization Of An Experimental Multiphase Flow Algorithm","Mittal, A.","van Gijzen, M.B. (mentor)","2016","","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Applied Mathematics","","","",""
"uuid:902680c8-3b85-44fc-af01-329cfd763823","http://resolver.tudelft.nl/uuid:902680c8-3b85-44fc-af01-329cfd763823","36 Component Seismic Data: Investigating Translational and Rotational Components in Exploration Seismology","Häusler, M.","Schmelzbach, C. (mentor); Sollberger, D. (mentor)","2016","In conventional multicomponent seismic exploration, the wavefield is recorded by measuring translational motions in three directions using three-component sensors. A novel approach in land seismic acquisition is to additionally record the rotational components of the wavefield around the various Cartesian axes and to combine them with translational measurements. Additional rotational measurements provide the opportunity to locally extract valuable information on the propagating wavefield, that either cannot be obtained from conventional translational measurements alone or are challenging to extract. For example, rotational data facilitate wavefield separation, shear-wave (S-wave) imaging, and ground roll suppression because of the direct link between rotation and the S-wave component, but also enable local instantaneous phase velocity estimation. At the Earth’s free surface, rotational motions can be expressed in terms of spatial seismic wavefield gradients. Wavefield gradients can be estimated by differencing the outputs of closely spaced three-component translational sensors. The same approach can be adapted to source arrays: differencing of recordings from closely spaced translational (directed) sources can be used to simulate rotational sources that primarily emit S-waves. The combination of three components of translation and three components of rotation on both the source and the receiver side leads to a total of 36 measurable seismic components. In this thesis, I first verify that array-derived rotational rates, estimated using spatial seismic wavefield gradients, correspond to direct rotation measurements from rotational sensors. Then, I investigate the value of 36-component seismic data using synthetic as well as real field data. I show that rotational components around the vertical axis mainly contain horizontally polarized S-waves. I found that rotational components around the crossline (transverse horizontal) axis mainly contain ground-roll and vertically polarized S-waves and that these data can be combined with translational data to suppress ground roll. I show that the amplitudes of rotational components are dependent on the angle of incidence of the wavefield and that source-sided rotational components are reciprocal to receiver-sided rotational components. To accelerate multicomponent acquisition, I furthermore present a new multicomponent seismic vector-source, which uses the Galperin configuration to obtain orthogonal vector sources of equal impact patterns and constant source-coupling. This source allows a fast multicomponent dataacquisition in engineering and environmental exploration seismology.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience and Engineering","","IDEA","",""
"uuid:43525b03-f784-4db8-85c8-0d68918f1ac6","http://resolver.tudelft.nl/uuid:43525b03-f784-4db8-85c8-0d68918f1ac6","Going against the flow: An experimental investigation into the flow mechanics of dimpled surfaces in turbulent boundary layers","van Campenhout, O.W.G.","Veldhuis, L.L.M. (mentor); van Oudheusden, B.W. (mentor); Schrijer, F.F.J. (mentor)","2016","Any reduction in vehicle drag or fluid resistance provides a potential of substantial energy savings, with obvious benefits to the economy, environment and overall industrial competitiveness. Although various experimental studies have confirmed the potential drag reduction of dimpled surfaces in a turbulent boundary layer (BL), the working mechanism behind the effect remains largely unresolved. An experimental investigation is performed with the objective to strengthen the understanding of this novel aerodynamic surface and its interaction with the turbulent BL. Direct force measurements are combined with Particle Image Velocimetry (PIV) and Particle Image Surface Flow Visualization (PISFV). The direct force measurements reveal that the drag reduction is highly sensitive on flow conditions, a finding with significant implications for further research as well as for potential applications. Furthermore, the PIV and PISFV measurements reveal a spanwise oscillation at the surface and at 0.2δ due to the interaction of individual dimple flow topologies, which are of the converger-diffuser type. The measurement of this oscillation is the first of its kind and provides strong evidence of a state-of-the-art drag reduction theory: the interaction between dimples causes alternating spanwise excitations of the near-wall flow which interacts with the turbulent coherent structures and therefore leads to a reduction of the turbulent drag. This theory is in contrast to what has often been proposed in literature. Dimples potentially have significant advantages over other means of passive flow control for drag reduction: they are very shallow and therefore do not require complicated cleaning or maintenance procedures, also they are not prone to wear such as riblets. Furthermore, they can easily be (retro)fitted on skin panels. This research provides fundamental data that contributes to the understanding of the flow mechanics of these dimpled surfaces in turbulent BLs.","passive flow control; drag reduction; PIV; turbulent boundary layers","en","master thesis","","","","","","","","2018-08-12","Aerospace Engineering","Aerodynamics","","","",""
"uuid:674c3f82-668e-495e-a088-fe964d30320c","http://resolver.tudelft.nl/uuid:674c3f82-668e-495e-a088-fe964d30320c","The application of double-curved precast concrete elements for the project Green Planet","Witterholt, S.","","2016","Zie file: summary.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","","",""
"uuid:2c9e9905-c35a-4749-9b9f-c92362384979","http://resolver.tudelft.nl/uuid:2c9e9905-c35a-4749-9b9f-c92362384979","The Smart Water Meter: A new method to monitor fouling issue","Chen, Z.","van der Meer, W.G.J. (mentor); Liu, G. (mentor); Witkamp, G.J. (mentor)","2016","The drinking water distribution network is a sealed and pressurized system which attached numerous biofilm and microorganism due to the long-time operation. In a foreseeable future, Oasen will adopt reverse osmosis to treat drinking water and thus the nutrient (biodegradable compounds) in drinking water will be slight. In that case, biofilms used to attach on pipelines may die and detach from pipes and these part of biofilm may clog consumers’ water meter. To avoid the potentially clogged water meter issues, a new monitor method, the Smart Water Meter should be used to monitor the fouling issues during the distribution process. This research, supervised by TU Delft and Dutch drinking water company Oasen, is an attempt to verify the Smart Water Meter as a suitable method to monitor fouling issues and figure out what matters contribute to the water meter clogging issues. Three objectives are included in this research: * The first objective is to verify Smart Water Meter as a suitable method to monitor water quality and fouling issues during the distribution process. In this research, the pressure drop is the key factor to detect the fouling issue and two equipment, filtrated clogging potential (FCP) and crossflow clogging potential (CCP), are accepted to monitor the fouling issues both in a short term and long term. * The second objective is to analyze what matters cause the pressure drop/filter resistance increase. This part analysis is to figure out the factors contribute to the potential fouling issues. Research can be divided into three parts, physical part, chemical part, and biological part. Physical part stress on explaining from pressure drop and filter resistance. Chemical part focus on determining the chemical compounds of fouling and biological part centralizes on ATP concentration. A better and comprehensive result could be obtained from the combining of analysis from these three aspects. * The third objective is to investigate the characteristics of these matters, this part analysis is the extension of the second objective and this part research concentrate on the quantities of each matter. In physical part, microscope and particle counter are used to calculate the total clogging particle number. In chemical part, ICP-MS is adapted to detect the concentration of chemicals. This research indicate that the Smart Water Meter is a suitable method to monitor fouling issues during the distribution process. In FCP experiment, filter resistance increased 15% in 16 hours operation time while in CCP experiment, the pressure drop grew from 0 to 10mbar in 27 days. Meanwhile, the clogging mechanisms of FCP and CCP are different; Particle clogging is the most important contributor for FCP while Biofouling plays a significant role in CCP. Therefore, FCP is more suitable to monitor the particles such as detached biofilm while CCP can detect the microorganism regrowth. Further, no relationship can be found between inorganic matters and clogging issues for both FCP and CCP experiments in this research, which indicate inorganic matter is not a primary contributor to fouling issues. Results also indicate that drinking water quality from consumers’ tap is greatly influenced by the distribution process. Both the biofilm and hydraulic retention time will impact the water quality from consumer’s tap.","fouling; distribution process; pressure drop; drinking water","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","","",""
"uuid:fbc873d2-3a78-424b-9c5b-35a3b736d9b7","http://resolver.tudelft.nl/uuid:fbc873d2-3a78-424b-9c5b-35a3b736d9b7","DC/DC Converters for Multiterminal HVDC Systems: Based on Modular Multilevel Converter","Inanc Sari, H.","Tedeschi, E. (mentor); Bauer, P. (mentor)","2016","The increase in the energy demand has resulted in searching for new energy sources. Due to the increased prices of fossil fuels and the environmental issues, renewable energy sources has become popular in Europe. Renewable energy generation such as wind, solar and wave has point-to-point connections with the main grid. In order to minimize the disadvantages of renewable energy sources like energy fluctuations, multi-terminal systems are favoured interconnecting energy generation stations among them with the main grid. The long distance interconnection especially for offshore wind farms and intercontinental connections is not possible with traditional alternating current (AC) technology because of the limited power transfer. Therefore, high voltage direct current (HVDC) technology is considered as the main element for the future multi-terminal grid. The conversion between AC and DC is preferred via voltage source converters (VSC) as they offer more flexible power control compared to traditional current source converters in other words line commutated converters (LCC). HVDC power transmission schemes have been constructed depending on the technology of the time, which means that there is no standardization in voltage levels and configurations of the HVDC schemes. Therefore, the connection of such systems operating at different voltage levels and/or in different schemes such as monopole and bipolar systems for the future multi-terminal DC grids requires DC/DC converters. Although the voltage level and the configurations are the same for both DC systems, DC/DC converter may be required for the power flow control in multi-terminal DC grid. The modular multilevel converter (MMC) seems as the most suitable converter in this application due to its advantages such as low switching losses, high scalability and modularity. MMC control structures are introduced and front-to-front connection of two MMC forming a DC/AC/DC converter is modelled and simulated for different applications such as interconnecting systems with same voltage levels, different voltage levels or different configurations in this study. The passive element sizes of MMC submodules are also compared for different AC side frequency as the component sizes can be decreased thanks to the increase in the AC frequency which has a disadvantage of higher switching losses. Moreover, the designed converters are tested in multi-terminal DC grids to check their performance and functionality.","HVDC; voltage source converters; VSC; modular multilevel converter; MMC; DC/DC converter; multi-terminal DC grid","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Electrical Sustainable Energy","","Erasmus Mundus, European Wind Energy Master (EWEM), Electric Power System Track","",""
"uuid:6e5029b2-6ae4-40e1-9509-01ca18057b2b","http://resolver.tudelft.nl/uuid:6e5029b2-6ae4-40e1-9509-01ca18057b2b","Thermomechanical modelling of a hydrostatic machine tool spindle","de Wolf, B.P.","van Ostayen, R.A.J. (mentor)","2016","Development of high precision machine tools enables the industry to continuously tighten tolerances and surface finish quality demands. Controlling the thermal state of the machine and process has become the dominant challenge for machine tool manufacturers. Defining models that predict the thermally induced deformations is an important aspect of this development. This research project focussed on development of a thermomechanical model of a hydrostatic machine tool spindle. Analytical calculations are performed to describe the viscous dissipation in the bearing fluid, losses in the integrated electric motor, the motor coolant circuit and ambient conditions. The resulting parameters and equations are supplied to a finite element model of the spindle assembly. An important characteristic of the model is a fully parametric set-up to enable easy investigation of design variations. A new modelling method is proposed to address the demand for a parametric finite element model of this complexity. This involves a generalised definitions structure that allows the user to rebuild a model with a single function call. The approach helps eliminate artefacts and provides a platform to store corresponding definitions and simulation results. Validation of the theoretical model is performed with two series of thermal error measurements on industrial machine tool spindles. The design of the spindles is modified to incorporate internal temperature sensors which provide important insight in the thermal state of the spindle assembly. Comparison of the simulated and experimental results shows that the model is a promising tool for evaluation of the transient thermal behaviour.","machine tool spindle; thermal error; finite element modelling; experimental validation","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Precision and Microsystems Engineering (PME)","","","",""
"uuid:f704e017-dc24-48c3-af15-662433444f8f","http://resolver.tudelft.nl/uuid:f704e017-dc24-48c3-af15-662433444f8f","Mechanical behaviour of laminated hybrid adhesive point connections when exposed to humidity conditions","Hanenberg, N.J.F.","Louter, P.C. (mentor); Veer, F.A. (mentor); Santarsiero, M. (mentor); Nijsse, R. (mentor)","2016","The use of structural glass has increased lately, due to the higher demands for transparency in modern architecture. Therefore, the role of glass changes from a mere transparent function (used in windows only) to a function in which the glass is contributing to the bearing structure as well. Because of the fragile nature of glass, it is important to understand its behaviour. Due to the manufacturing process, the strength of glass is decreased severely by small cracks that are present on the surface. Also, glass cannot redistribute stress peaks very well. Stress peaks tend to occur near the connection between two (glass) components, making this a critical part in glass design. A common way to joint glass elements is by a bolted connection. However, this type of connection does not seem suitable for glass design. Due to the drilling process extra cracks will occur, causing a reduced material resistance, and at the edge of the holes large stress intensifications arise. Adhesive connections are more in line with the above stated problems of glass design. No drilling is needed, so no extra cracks will occur. Also, forces are transferred over the entire surface of the connection, leading to smaller stress peaks in the glass. However, adhesive connections know risks of their own. Problems are (1) the sensitivity of some adhesives to aging affects like humidity, (2) the strongly non-linear behaviour of the adhesive which causes stress peaks in the connection, and (3) the complex manufacturing process of the joint, because some adhesives will liquefy and squeeze out during the lamination process. In this thesis a concept for an adhesive connection is proposed that addresses these issues. The concept proposes the use of two adhesives in order to reach the following effect. The first adhesive is a rigid ionomer adhesive (SG) that is very sensitive for humidity and the second adhesive is a flexible silicone adhesive (TSSA) which is not sensitive to aging effects. The first adhesive is used in the centre of the connection and the second adhesive forms an external ring to protect the SG against moisture exposure. Also, the non-linear behaviour of the rigid adhesive is leading to stress peaks at the edge of the connection. By placing a less rigid adhesive at the edge, the overall stress distribution is more evenly divided. The manufacturing process is enhanced because the TSSA remains solid during the lamination process, so it can be used as a mould for the SG that will liquefy. The concept is called a hybrid adhesive connection. The concept is tested for a circular point connection exposed to tensile loading. This is done by means of an experimental and numerical analysis. The experimental analysis focusses on the deterioration of the hybrid circular connection after exposure to either immersion or 100% relative humidity. The numerical analysis concentrates on the stress distribution of the proposed connection. The following conclusions can be drawn from this investigation. In the manufacturing process of the experimental analysis, also the TSSA is squeezed out due to its softness. Therefore, extra measures should be taken to prevent this, which means that the manufacturing process of hybrid connections is not improved in comparison to SG connections. Deterioration of the connection due to humidity is still visible for a hybrid connection containing a ring of TSSA with a width of 5mm. This effect is less visible when a width of 10mm TSSA is used. Unfortunately, due to lamination problems, the samples suffered from poor SG bonding, which makes it difficult to give substantial conclusions on the matter. The numerical analysis gave more clear results. The E-modulus of TSSA is very low in comparison to that of SG. Therefore, the contribution of the TSSA to the bearing capacity of the connection is negligible, so the stress distribution is not enhanced. On the other hand, due to the external ring of TSSA, the stress peaks in the connection are moved inwards, from the perimeter of the connection to the perimeter of the SG centre. This is a positive effect with regard to humidity exposure. The strength of the hybrid circular point connection in comparison to the TSSA connections, is only slightly increased, but the stiffness of the hybrid connection surely is enhanced. Treats of the hybrid connection might be the risk of air bubbles in between the SG and TSSA and the absence of the whitening effect in the TSSA.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Structural Engineering","","Building Engineering","",""
"uuid:b799688a-f2cf-4dc9-96ae-072e51094bfd","http://resolver.tudelft.nl/uuid:b799688a-f2cf-4dc9-96ae-072e51094bfd","Repurposing cultural heritage: A modular approach to sustainable dwelling","Smedes, B.S.","Silvester, S. (mentor); Klein, T. (mentor)","2016","Vacant property in The Netherlands, especially cultural heritage, can be attractive housing facilities when repurposed. However, such transformations are often very pricey one-off projects that do not easily allow for adaptations nor replications once installed. Pun aside, this rigidity characterizes the construction sector in general. It reflects also in their inability to adapt to more environmentally sustainable solutions, even though this sector alone contributes to the emission of up to 30% of global annual greenhouse gasses and 40% of all energy. Loft 10 Green Living wants to combine these two aspects by offering highly sustainable living units for easy transformation of cultural heritage. The developed concept encompasses a set of prefabricated building blocks that form a highly adaptable home in all use phases: before (through customisation), during (through multifunctional spaces) and after (through a facilitated end-of-life). Water and energy saving products, environmentally friendly materials and behaviour persuading solutions create a highly sustainable way of living without compromising on living comfort.","sustainability; prefabrication; customisation; life-cycle assessment","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Industrial Design","","","",""
"uuid:63ab8425-c0e9-4974-9e71-5c5c4321163f","http://resolver.tudelft.nl/uuid:63ab8425-c0e9-4974-9e71-5c5c4321163f","A Helping hand for e-NABLE: Multi-material applications for the Ultimaker 3","Bijadi, S.","Tempelman, E. (mentor)","2016","This report discusses the project in 3 main parts: Part 1 discussed the initial detailed analysis and synthesis of 3D printing and Prosthetics, and subsequent definition of a design goal. In Part 2, the conceptual design, and prototyping and development of a new prosthetic hand platform along with multi-material functionality enhancements on the same, are discussed. Finally in Part 3, scope for further work on this platform along with key recommendations for Ultimaker and e-NABLE are discussed.","3D printing; prosthetics","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Design Engineering","","","",""
"uuid:cfee3e37-1833-4cdd-b29b-9df05d3b2ee3","http://resolver.tudelft.nl/uuid:cfee3e37-1833-4cdd-b29b-9df05d3b2ee3","Pylon Wake Control Using Steady Blowing: A Numerical Study","Jindal, A.","Sinnige, T. (mentor); Veldhuis, L.L.M. (mentor); Hulshoff, S.J. (mentor)","2016","","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy & Propulsion","","","",""
"uuid:abeb3c84-5a6d-4a52-ba69-c4255bc0990e","http://resolver.tudelft.nl/uuid:abeb3c84-5a6d-4a52-ba69-c4255bc0990e","Sustainable prefabricated living units for a circular economy","van der Laan, A.J.","Silvester, S. (mentor); Klein, T. (mentor)","2016","This master thesis describes and evaluates the development of living units for adaptive reuse. In many cases buildings that lose their function over time are destined to be demolished to make space for new estate, however these buildings are often ideal for adaptive reuse. The project was initiated due to a lack of appropriate solutions currently available on the market regarding the sustainability demands. The living units designed in this project are the solution to this shortfall. The focus of this report is threefold: the collaborative first phase explores the current state regarding residing. The individual second phase discusses a design concept for the living units. In the collaborative final phase the individual designs are merged and the ultimate design is elaborated upon. The designed living units are the optimal solution to the challenge of vacancy. The living units consist of building elements that are modular and can therefore be installed in practically all property. Furthermore, the living units can be scaled regarding layout and the building elements can be composed and customised to meet the preferences and needs of the occupants. The living units include sustainable solutions that aim to improve the behaviour of the occupants by providing insights and indications regarding their consumption. When the living units are no longer useful to the occupants, e.g. in case of moving houses, the living units can be taken apart and can be reused or recycled, following the circular economy principles. A subscription based agreement helps the circular economy approach by keeping involved with the occupants, while also providing more freedom in the selection of aspects of the agreement.","sustainable living units; circular economy; adaptive reuse; subscription based living; loft living","en","master thesis","","","","","","","Campus only","","Industrial Design Engineering","Integrated Product Design","","","",""
"uuid:d20fe647-9aab-4aff-b1ec-fa37e180f22a","http://resolver.tudelft.nl/uuid:d20fe647-9aab-4aff-b1ec-fa37e180f22a","The Prospects of Flexibility on Congestion Mitigation against Network Reinforcement","Abi Morshed, F.","Herder, P.M. (mentor)","2016","Traditionally, the transmission network transported electricity over extended distances from local large scale electricity generation plants to distribution networks that transported electricity to end consumers (Ekanayake, 2012). In order to ensure that such a system remains operational, a Distribution System Operator (DSO) maintains, operates and invests in the grid at the distributional level. One of the reasons grid investment is performed is to prevent grid congestion that occurs when the electricity load exceeds the grid capacity. Investing in reinforcing the distribution grid components (cables, transformers, fuses, etc.) increases the capacity of the electricity grid, which, in turn, prevents grid congestion. However, with the advent of renewable energy sources, decentralized generation units and electrical appliances (electric vehicles, heat pumps, electric boilers etc.), there is an increasing pressure on modern electricity networks because of overproduction at the local level that leads to reverse transfer of power, increase in grid losses, and voltage and current fluctuations. This increasing pressure on the electricity grid could be reduced by grid reinforcement; however, it is argued that grid reinforcement and investments cannot keep up with the growth of intermittent renewable energy sources, which may result in interim and short term congestion. Moreover, Haque et al. (2014) claimed that upgrading grid assets, which is considered capital intensive, will not serve as a cost-effective solution in modern grids, as the electricity network congestions are temporary. Although the integration of renewable and distributed resources increases the complexity of operation and preservation of the reliability of the grid, it also provides opportunities to manage the load on the network. These opportunities surface and evolve from the flexibility that results from Demand Response (DR), also referred to as Demand-Side Flexibility. Flexibility, provided by DR, is created by controlling the distributed energy resources and electrical appliances (heat pumps, electric boilers etc.) on the distribution network, which may potentially reduce peak loads or shift loads to off peak periods of time. With flexibility from such decentralized electricity sources and appliances, it is possible to manage the electricity load variability in a more cost effective manner, which may result in postponing capital intensive grid reinforcement (Ecofys, 2015). A third party, called the aggregator, is responsible to aggregate the flexibility from controlled devices and sources. Therefore, the DSO engages in short/long term contracts with the aggregator to procure the flexibility to resolve congestion. Although demand-side flexibility holds potential in preserving network reliability by mitigating congestion, and thus postponing grid investment, the impact of demand-side flexibility provided by Demand Response on congestion mitigation, from a technical perspective, is blurred and indeterminate (Moslehi & Kumar, 2010). Moreover, the prospects of financial savings for the DSO are not guaranteed (Torriti et al., 2010). Therefore, to bridge this knowledge gap, the thesis explored the following main research question: “To what extent can the DSO mitigate grid congestion by means of Demand-Side Flexibility to defer grid reinforcement?”","Demand Response; Demand-Side Flexibility; Distribution System Operator; Network Reinforcement; Electricity Grids","en","master thesis","","","","","","","","","Technology, Policy and Management","Energy and Industry","","","",""
"uuid:ea9b17f0-f51c-4086-9ae0-f54fb3b0408b","http://resolver.tudelft.nl/uuid:ea9b17f0-f51c-4086-9ae0-f54fb3b0408b","Detection of Hidden Cracks in Concrete Structures Using Reverse Time Migration of Ultrasonic Echo Data","König, M.","Niederleithinger, E. (mentor)","2016","Ultrasonic echo measurements are widely used in the field of non-destructive testing (NDT). In civil engineering, concrete structures are evaluated by this technique. Currently, Synthetic Aperture Focusing Technique (SAFT), a group of migration algorithms, is state of the art in ultrasonic data processing. Reverse Time Migration (RTM), recently introduced to NDT, shows significant improvements in mapping complex structures, like vertical steps. Modelling a concrete test specimen, synthetic experiments confirm that RTM can be used to map notches and crack-like structures in concrete. With introducing heterogeneous synthetic models, the influence of the migration velocity of RTM was investigated. Furthermore experiments on real concrete structures were conducted. Thereby ultrasonic echo data is acquired on a test specimen with a known vertical notch. A commercial ultrasonic tomograph and a scanner system, developed at BAM, are used. The data from both systems is processed using SAFT and compared to results of RTM of the scanner data. Results from the SAFT migration do not reveal any information about the vertical notch, other than an indication of the lateral position. Data migrated using RTM shows the side wall of the notch and affirms its potential for such purposes. Processing the data from the commercial ultrasonic device using RTM does not show any improvements compared to the initial SAFT result. The fixed aperture of the device is not suitable for RTM. An additional measurement is performed on a second specimen using the scanner system. The specimen shows several fine cracks of which only outcrops at the sides are visible. Results of RTM indicate mapped signatures from those cracks. Their lateral positions along the specimen as well as an estimate about the crack height can be determined within the final images. This study thereby introduces the applicability of RTM for detecting cracks within concrete structures.","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience & Engineering","","","IDEA League",""
"uuid:a3a6d83d-a439-41ac-b966-0c3cfefe535b","http://resolver.tudelft.nl/uuid:a3a6d83d-a439-41ac-b966-0c3cfefe535b","Modelling Network Contribution Under Changing Capacity For A Hub & Spoke Carrier","Potjer, J.E.","Santos, B.F. (mentor)","2016","The main characteristic of a hub-and-spoke network is that many passengers will reach their final destination via a connection at the hub, instead of a direct flight in case of a point-to-point network. An airline operating such a hub-and-spoke network thus has a significant ratio of transfer passengers on its flights. This means that each flight leg or route should be viewed as part of a greater system rather than an individual item. The use of a hub-and-spoke network entails that air travel demand is defined for an OD market, while the supply is provided in the form of flight leg departures in the network. This implies that there exists a dichotomy of airline demand and supply in an individual OD market. Currently, most of the airlines base their performance assessment on a route-based analysis. How-ever, since one flight leg or route may serve multiple itineraries through the network, not only the performance of that individual flight leg or route has to be analyzed but also the role of the flight leg or route in the network. Despite of the fact that some research on this subject has already been performed, a network-based performance analysis is still not commonly used by airlines operating a hub-and-spoke network. This entails that an airline operating a hub-and-spoke network, such as Kenya Airways, is not capable of fully analyzing the performance of their routes considering its value in the network. This research is contributing to the problem, that airlines operating a hub-and-spoke network are not able to fully analyze the performance of their routes considering its value in the network, by perform-ing a more comprehensive assessment of the contribution of a flight leg or route to a greater system. This is referred to as the Network Contribution concept. Besides the analysis of the pure cancellation of a segment or route, the Network Contribution has also been analyzed under adjustment in capacity, without changing the capacity to zero. The capacity is changed by changing the frequency on a route, or changing the type of aircraft operated on a route.","","en","master thesis","","","","","","","","2021-07-27","Aerospace Engineering","Air Transport and Operations","","","",""
"uuid:8efab9c5-e78b-40ff-ab37-a563366d22f9","http://resolver.tudelft.nl/uuid:8efab9c5-e78b-40ff-ab37-a563366d22f9","The Exploring DelFly: How to increase the indoor explored area of the DelFly Explorer by means of computationally efficient routing decisions?","Fonville, C.R.","de Croon, G.C.H.E. (mentor)","2016","Small robots, such as Micro Aerial Vehicles, form an increasingly popular field of interests in research, industry and the consumer market. The autonomous capabilities of these systems keep evolving and one of the main research goals is to reach full autonomy. However, this is often achieved at the cost of growing hardware demands. In this study a computationally light and efficient way to enhance autonomous on-board exploration capabilities for the DelFly Explorer, a 20-gram flapping wing Micro Aerial Vehicle (FWMAV), is presented. Both theory and new insights were combined to design an exploration algorithm for the on-board stereo-vision system. The algorithm primarily consists of a disparity map based decision tree, different exploration phases and computationally light odometry. Computer simulations proved the effectiveness of the algorithm to enable autonomous exploration capabilities for the FWMAV system. Initial flight tests also show that the proposed algorithm increases its exploration capabilities and form a foundation for future research.","DelFly Explorer; Exploration Algorithm; Autonomous Exploration; Flapping Wing Micro Aerial Vehicle","en","master thesis","","","","","","","","","Aerospace Engineering","Control & Operations","","Control & Simulation","",""
"uuid:aa9c02f9-71e9-4ba4-840d-596aa0e2341a","http://resolver.tudelft.nl/uuid:aa9c02f9-71e9-4ba4-840d-596aa0e2341a","Exact distributions of the multinomial order statistics","Ogay, A.","Cirillo, P. (mentor)","2016","","","en","master thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Applied Mathematics","","","",""
"uuid:69682a06-868a-47e0-81a9-34dbff42c950","http://resolver.tudelft.nl/uuid:69682a06-868a-47e0-81a9-34dbff42c950","Assessment of the influence of permeable pile groins on nearshore hydraulics","Vasarmidis, P.","Stive, M.J.F. (mentor)","2016","One of the most economically and ecologically important issues related to coastal engineering is the preservation of the coastal areas. The mean to do so, is the good understanding of the sediment transport mechanisms that are involved in the transformation of coasts. In order to protect these valuable areas, man-made structures have been developed, such as permeable pile groins which are among the oldest, but also to the most interesting coastal structures. They constitute of wooden piles driven into the seabed perpendicular to the shore and thus they allow both water and sediment to pass through them, acting as a hydraulic roughness on the longshore current leading to a reduced littoral current velocity in their vicinity. However, although permeable pile groins have been constructed along many world’s coastlines through the years, the research done on how they affect the coastal system is still limited. The aim of this study is to bridge the existing knowledge gap and provide a clearer view on the factors that influence the nearshore hydraulics at Domburg coast. The evaluation of permeable pile groins is going to be done by means of a process-based model in SWASH. The SWASH model will be used for the examination of several hydraulic conditions, and will be afterwards validated using the results from the experiments that Hulsbergen carried out. The validation of the model is done in several steps, in order to make the understanding of the influence of each parameter easier. First, only the wave induced current is examined, followed by the tide induced current. The combination of wave and tide induced current follows and last but not least two (2) different types of permeable piles groins are implemented. By this method, not only the influence of permeable pile groins is examined, but also the capability of SWASH itself to capture the real phenomena and give accurate results. The results of the present study revealed that SWASH model is very capable of capturing the real behavior of the current, since high correlation with the experimental results can be observed for all the test-cases examined. The results also showed a clear retardation of the current due to the presence of the pile groins which in turn leads to a reduction in the capacity of the current to transport sediment.","permeable pile groins; SWASH; wave and tide induced current; Domburg coast","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Hydraulic Engineering","","","",""
"uuid:943944d4-3430-4288-9eef-d04e34bb9f67","http://resolver.tudelft.nl/uuid:943944d4-3430-4288-9eef-d04e34bb9f67","Semi-Analytical Composite Oval Fuselage Mass Estimation","Roelofs, M.N.","Vos, R. (mentor)","2016","A semi-analytical weight estimation method is proposed for composite, oval fuselages, but is also applicable to conventional fuselages and to metallic materials. Loads applied to the fuselage include pressurization, steady-state maneuver loads and inertial loads. The primary structure around the passenger cabin is sized, based on first-ply failure using the Tsai-Wu failure criterion, global and local buckling. Moreover, maximum deflection due to transverse pressure is constrained for skin panels and sandwich panels. Sandwich panels are also sized for crippling and wrinkling. Empirical factors are used to calculate secondary structure and non-structural mass. In order to reduce in-the-loop calculation time, surrogate models of the sizing procedures are used, by means of neural networks. Verification of the sizing optimization was done by comparing the sized members with a genetic algorithm and validation of the failure calculations was done by finite-element analysis. It was found that the proposed method is capable of predicting metal, conventional fuselage mass satisfactorily, with acceptable breakdown of weights and estimated thicknesses. Additionally, the method can be used for unconventional aircraft configurations and composite material. Using composite material, a weight saving of around 15% is observed as compared to aluminum.","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerodynamics, Wind Energy & Propulsion","","","",""
"uuid:e714d5b9-236d-4509-bc80-79035c0f4725","http://resolver.tudelft.nl/uuid:e714d5b9-236d-4509-bc80-79035c0f4725","Quadrotor Thrust Vectoring Control with Time Optimal Trajectory Planning in Constant Wind Fields","da Rocha Silva, J.P.","de Croon, G.C.H.E. (mentor)","2016","This work proposes a control strategy to follow time optimal trajectories planned to visit a given set of waypoints in windy conditions. The aerodynamic effects of quadrotors are investigated, with emphasis on blade flapping, induced and parasitic drag. An extended method to identify all the aerodynamic coefficients is developed, and their influence on the performance is analyzed. A computationally efficient three steps approach is suggested to optimize the trajectory, by minimizing aerodynamic drag and jerk while still guaranteeing near optimal results. The derived smooth trajectory is compared with standard discrete point to point followed by low-pass filtering trajectories, showing energetic improvements in thrust and reductions in Euler angles aggressiveness. By exploiting the non-linear aerodynamic effects and using a priori trajectory information, a thrust vectoring controller is designed and compared with a standard PID controller, showing an increase in performance by reducing the tracking delay and extending the flight envelope.","","en","master thesis","","","","","","","","","Aerospace Engineering","Control & Operations","","Control & Simulation","",""
"uuid:ebfa8aaa-5cb7-4655-9fbd-f940e2475695","http://resolver.tudelft.nl/uuid:ebfa8aaa-5cb7-4655-9fbd-f940e2475695","The Financial Feasibility of Flexibility Expansion: An Empirical Analysis on the Financial Feasibility of Demand Side Management for the Aggregator","Jongerden, S.","Blok, K. (mentor)","2016","","Aggregatot; Demand Side Management; Financial Feasibility; Field Trial","en","master thesis","","","","","","","","2019-09-01","Technology, Policy and Management","Energy and Industry","","","",""
"uuid:265f92b0-039c-4370-a5cb-653fd16a953e","http://resolver.tudelft.nl/uuid:265f92b0-039c-4370-a5cb-653fd16a953e","Design and Analysis of a Mechanical Passive Pitch Control System for a Small Horizontal Axis Wind Turbine","Poryzala, T.","De Breuker, R. (mentor); Kim, T. (mentor)","2016","The goal of this thesis is to design, analyze, manufacture, and test a mechanical passive pitch mechanism for a small horizontal axis wind turbine. Several pitching concepts were investigated in the wind industry and related fields before ultimately deciding on a centrifugal governor design concept in a pitch-to-stall configuration. Inertial and aerodynamic models were developed in order to predict steady-state performance and an optimization routine was created to optimize the pitch mechanism configuration subject to manufacturing constraints. Dynamic modeling in HAWC2 validated the steady-state design code, aeroelastic simulations were performed in turbulent wind conditions to simulate the pitch system dynamics. Physical testing of the full turbine was not completed, however the hub sub-assembly was tested on its own to validate the passive pitch characteristics and showed good agreement with the simulation tools developed. The turbine was run in the TU Delft open-jet wind tunnel, but lack of instrumentation meant little data could be collected. Video evidence was collected which showed the pitch system clearly activating at high rotor speeds as expected. In addition to the pitch system design, a new blade structural design was completed with the BECAS design tool using glass fibre instead of the carbon-Kevlar hybrid material used last year.","","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures & Materials","","","",""
"uuid:56835755-8bc7-42be-bf86-54dc29a4ae9f","http://resolver.tudelft.nl/uuid:56835755-8bc7-42be-bf86-54dc29a4ae9f","On the accuracy and convergence of the finite element method for truss and frame structures","van Oostrum, Joep (TU Delft Civil Engineering and Geosciences)","van der Meer, F.P. (mentor); Al-Khoury, Rafid (graduation committee); Delft University of Technology (degree granting institution)","2016","During applied mechanics courses for undergraduates, the words `finite element' will be mentioned if a structural problem is so complex that it cannot be solved analytically or simply because it would require too much time to solve it by hand. The lecturer refers to a software package and explains how to work with it. Unfortunately, during these courses, no time is spend on the inner workings of this software. How does the program know, after virtually assembling the structural members and specifying the load, how the structure mechanically behaves? Another term that is mentioned in conjunction with the finite element method is `approximation'; the method is not an exact method, only approximated values are obtained. The accuracy depends on the amount of elements and the type of element. In this thesis the following question will be answered: what is the influence of the mesh size and the element type used in a finite element protocol for trusses and frames, on the accuracy of the displacements and forces and how does it affect the computation time? The goal of this thesis is also to get a better comprehension of the inner workings of a finite element program with a structural application. All this can be achieved by developing, analyzing and using a finite element program created in the MATLAB environment. To do this, prior literary study is required. Use is made of [1] and sub-paragraph 5.1.1 of [6]. A total of four MATLAB programs are written, one for trusses in 2D space, one for trusses in 3D space and two for frames in 2D space. Frames can namely be described by so-called linear and quadratic elements. What that means will be explained in paragraph 2.2. The structural models will be discretized and solved by MATLAB. The created finite element programs will be described and explained in the second chapter. After reading that part, it will be clear how the programs work and how to use them. In the chapter that follows, three elementary frame structures will be analyzed by modelling them with a varying number of elements and different element types. A comparison will be made with the analytical solution. Other aspects of the programs and how they compare to commercial finite element software is the focus of chapter 4. A geometrically complex structure, like an arch, will be modelled by beam elements and analyzed in the fifth chapter. The findings will be summarized and the central question will be answered in chapter 6.","finite element method; 2D; 3D; Trusses; Frames","en","bachelor thesis","","","","","","","","","","","","Civil Engineering","",""
"uuid:42578519-128b-4f1f-bad9-540f319a8361","http://resolver.tudelft.nl/uuid:42578519-128b-4f1f-bad9-540f319a8361","Runoff Modeling in Drainage Networks Using Probabilistic Graphs","Verstegen, E.","Schoups, G.H.W. (mentor); van de Giesen, N.C. (mentor); Heemink, A.W. (mentor)","2016","Conventional hydrological models use a deterministic approach. One could think of it like a black box, having an input, parameters, relations and an output. The parameters are calibrated by comparing the model output with observations of the system response (for example river runoff). When assessing the uncertainty in models the focus is often on the parameter uncertainty and all other variables are considered to be true. Not taking into account all sources of uncertainty, results in unreliable knowledge about the parameter uncertainty. A method to include more sources of uncertainty is applying a probabilistic model. Using this model, all variables are described by probability distributions. In a spatially distributed model this allows for a spatial estimation of variables and their uncertainty in all model components. Due to the large amount of variables in a distributed model, the complexity of the exact solution of the probabilistic model increases. To be able to efficiently calculate an approximate solution, the probabilistic model is factorized and structured in a factor graph, a form of a probabilistic graph. A factor graph contains factors and variables, where the factors represent the relations between variables and physical knowledge while the variables represent the belief about the data. A factor graph is bipartite, which means that factors are only connected with variables and vice versa. Information propagates through the graph using message passing. This is a process where a factor updates each connected variable, based on a function of the other connected variables. If the graph has a tree structure, message passing starts at the highest level of the tree, progressing downward. This ensures that all information reaches the root of the tree. The process is then reversed in an upward sweep of message passing, propagating all gained knowledge also upstream. When approximations are used or when the tree contains cycles, multiple iterations (downward and upward sweeps) are needed for the variables to converge to their final value. The result is a posterior distribution of every variable in every cell. In this research, a probabilistic graph is applied on a distributed runoff accumulation model. In each cell of the model a local runoff is calculated based on the precipitation, evaporation and an unknown bias term (initiated by bias parameters), which are all represented by a Gaussian distribution. Using flow paths, derived from a Digital Elevation Model, the local runoff is accumulated into accumulated runoff. A physical positivity constraint is added to the accumulated runoff and forcing data to prevent negative values. Multiple runoff observations (Gaussian distributed) are added resulting in spatial estimations of accumulated runoff, local runoff and bias, and an updated belief about the precipitation and evaporation. The bias parameters which initiate the bias in each cell contain uncertainty as well, allowing the parameters to be updated given the data received from the model. After the solution has converged, each dataset has been updated using the model structure (physical knowledge and constraints) and the prior knowledge from all other data sets. By looking at the spatial distribution of the bias, conclusions can be drawn about the quality of the data and the water balance as a representation of the hydrological processes. Areas with a high posterior bias either have a mismatch between forcing data and runoff observations, the water balance does not represent the reality well, or the data conflicts with the positivity constraints. The model is applied on the Volta basins, where 3 areas are identified where the bias is higher than in other areas of the basin, mainly influenced by the positivity constraints. These regions are next to a large river, next to lake Volta and the delta area at the mouth of the river. They are characterized by a negative prior local and accumulated runoff, indicating net evaporation while there is no water available. It is very likely that not the forcing data is faulty in these areas, but that an important hydrological process has not been incorporated. Given the large water bodies in the vicinity, ground water flow from the water body to the constrained cells is most probably the important hydrological process which is missing in the model. By applying probabilistic graphs on other (more complex) hydrological models allows for a better spatial estimation of both the variable values and their uncertainty, as well as a spatial evaluation of the performance of the model structure.","runoff modeling; discharge modeling; graphical modeling; probabilistic modeling; factor graph; uncertainty; spatial distribution; distributed modeling","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Water Management","","","",""
"uuid:38316ed7-e0f4-4695-8094-5eb484022df8","http://resolver.tudelft.nl/uuid:38316ed7-e0f4-4695-8094-5eb484022df8","Radius Bending: User Element Testing and Investigation on Effect of Stacking Sequence on Out-of-plane Failure","van der Meijs, J.J.","Abdalla, M.O. (mentor)","2016","A new doubly-curved shell element (ThickS4 element) is presented and tested for flat and singly curved configurations. The benchmark tests show good correlation for the ThickS4 element with the Abaqus S4 shell element and with 3D FEM for flat and highly curved configurations (R/t ≥ 4). For radius over thickness ratios smaller than 4 the element is not able to predict the correct behaviour. In order to give a good insight in the most critical radius over thickness ratios 1 ≤ R/t ≥ 8 a stacking sequence investigation has been done making use of the Abaqus C3D8 element. An L-shaped structure has been tested for different load combinations of in-plane, out-plane and opening-moment loading. From the investigation a clear overview of best stacking sequences for all the different loading combinations were presented as well as the ratio of ILNS versus ILSS. The results can further be used as guidelines for the initial design of composite C-spars and stringers.","Out-of-plane failure; Radius Bending; Doubly-Curved Shell Element; Interlaminar Stress; Stacking Sequence Investigation","en","master thesis","","","","","","","","","Aerospace Engineering","Aerospace Structures & Materials","","","",""
"uuid:cd6ec19c-4d22-4670-97c7-ced07f5722a0","http://resolver.tudelft.nl/uuid:cd6ec19c-4d22-4670-97c7-ced07f5722a0","De Hankeltransformatie via de Cherednik algebra: The Hankel transform via the Cherednik algebra","Oostendorp, M.G.C.","Groenevelt, W.G.M. (mentor)","2016","In deze Bachelorscriptie bestuderen we een integraaltransformatie welke een generalisatie is van de Fouriertransformatie, deze integraaltransformatie wordt de Hankeltransformatie genoemd. Allereerst bestuderen we de symmetrische variant waarna we overstappen op de niet-symmetrische Hankeltransformatie. We gaan de niet-symmetrische Hankeltransformatie beschrijven aan de hand van een algebra met behulp van representatietheorie wat ons in staat stelt om een Hankeltransformatie te definiëren op een eindig-dimensionale ruimte.","","nl","bachelor thesis","","","","","","","","","Electrical Engineering, Mathematics and Computer Science","Delft Institute of Applied Mathematics","","","",""
"uuid:b5d75c35-fd97-4fe3-a71d-5dec7f174eb0","http://resolver.tudelft.nl/uuid:b5d75c35-fd97-4fe3-a71d-5dec7f174eb0","Series Hybrid Electric Aircraft: Comparing the Well-to-Propeller Efficiency With a Conventional Propeller Aircraft","Lenssen, R.H.","Melkert, J.A. (mentor)","2016","THE aviation industry is responsible for 12% of the total transportation impact of CO2 while awareness, for decreasing the total carbon footprint, is rising. Both the aerospace and the automotive industry are facing an increasing pressure from society to make the transportation sector more sustainable. Within the automotive industry slowly an increase in electric vehicles can be noticed (<1%). Also in the aerospace industry a rise in electrification can be seen, with small aircraft as the E-Star and E-Fan (two seaters) as commercial examples. Electrification of the transportation sector could further result in a decrease in noise and an increase in lifespan of parts as vibrations are decreased. This master’s thesis is written in conjunction with the chair Flight Performance and Propulsion at the faculty of Aerospace Engineering at the Delft University of Technology. The main purpose is to gain more insight in modelling an (hybrid) electric aircraft and the potential improvements with respect to well-to-propeller efficiency (usefull energy over total energy ratio). This is achieved by first creating a baseline conventional propeller aircraftmodel (ATR72) and then a hybrid electric version of the same aircraft. The variations between the sub-models and validation data are calculated in order to have a feeling for the accuracy of each individual model. Furthermore, both the theoretical and current practical state of technologies are used in the overall model. Finally, a sensitivity analysis is performed to find the driving parameters in the outcome of the model. The analysis of the series hybrid electric aircraft showed first of all that the expected advantages of the concept are ’small to non-existent’. The electric energy used to charge the batteries should first of all come from a renewable source of energy to make the concept feasible. Secondly, the theoretical limits of technology should be approached in order for the well-to-propeller efficiency to exceed that of the conventional ATR72 aircraft (with a maximum of 2%). It is seen that the model converges to an all electric version of the ATR72 if the battery energy density is increased to 2,802 [Wh/kg], this would correspond to the theoretical limit of Lithium Sulphur battery-technology. Furthermore, for an increase in voltage the battery efficiency decreases while all other components will improve in efficiency. The optimum is found in increasing the voltage up to the practical limit of 25 [kV]. Electric propulsion creates new design possibilities as distributed propulsion and variable shaft-speed. Within this thesis it is however shown that the ’benefits’ of distributed propulsion do not outweigh the downsides (increase inweight and decrease in efficiency of all components). Furthermore electric motors allow for temporary torque overloading, by decreasing the rotational speed and increasing the torque, the overall result is an increase in efficiency, which could for example be usefull during the climb or take-off phase. Concluding, the concept of series hybrid electric aircraft is at this moment in time rendered infeasible. The potential within a 35 year time-frame is doubtfull as especially battery technology should improvewith at least 400 [%]. In order to accelerate the transition to hybrid electric or all electric aircraft, the main areas of research should be: battery technology and the integration of alternating current and superconducting materials in rotating machine parts.","Electric Flight; Series Hybrid; Optimisation; Modelling; Aircraft; ATR72","en","master thesis","","","","","","","","","Aerospace Engineering","Flight Performance and Propulsion","","","",""
"uuid:9610f971-45b3-4ac4-b1ff-70583123298a","http://resolver.tudelft.nl/uuid:9610f971-45b3-4ac4-b1ff-70583123298a","Piezo-Stepper actuator, Self-Sensing, Impedance Control","Naikwad, S.S.","Hassan Hosseinnia, S. (mentor)","2016","Piezo-Stepper actuators also called as piezowalk actuators find widespread application in precision mechatronic systems. This is primarily because of their high accuracy, long range of motion and compact volume. One such application is in the optical column manipulator of a lithography system. During operation, whenever a motion is desired beyond the analog range of the actuator, a stepping motion is performed. A major performance limiting factor here is that during stepping, large disturbances and vibrations are induced in the manipulator due to hammering action of the clamp piezolegs. To reduce these errors, it is desired to regulate the interaction behaviour of the constitutive piezolegs of the actuator with the driving rod. In this research, the self sensing property of the piezoelectric element is exploited to detect contact and liftoff. Secondly, a self-sensing observer is designed to obtain force and position feedback signals to the controller. Finally using these signals as feedback, a modified impedance control scheme is designed to regulate the internal interaction behaviour between the piezolegs and the driving rod.","Piezo-Stepper actuator; Self-Sensing; Impedance Control","en","master thesis","","","","","","","","","Mechanical, Maritime and Materials Engineering","Precision & Microsystems Engineering (PME)","","","",""
"uuid:87a4ae0e-358f-4ed4-b1aa-3f52e2439775","http://resolver.tudelft.nl/uuid:87a4ae0e-358f-4ed4-b1aa-3f52e2439775","Derivative versus Clean-Sheet Approach: Design and Analysis of Different Concepts for a new Medium-Range Aircraft","Koch, R.M.","Vos, R. (mentor)","2016","Two approaches were used and compared to design a new medium-range passenger aircraft: a derivative design based on the A330-200 long-range aircraft and a completely new clean-sheet design. Four different derivative configurations, with different materials and levels of commonality to the A330-200, and one clean-sheet configuration were defined, designed and analysed. For this a sizing model was developed which consists of geometry definition, weights estimation, aerodynamic analysis, low-speed and high speed performance and a recurring, non-recurring and operating cost analysis. Special attention was paid to the restrictions of the derivative configurations and the wing weight analysis, where the existing semi-empirical method was calibrated with a higher level weight prediction method to improve the quality of the results and to account for carbon fiber as primary wing material. For the estimation of recurring and non-recurring cost new cost estimation relationships were defined and for the aerodynamics and performance calculations literature methods were adapted to be used in a top-down approach, using reference aircraft data, in order to increase their accuracy. It was found that the clean-sheet design offers higher reductions in block fuel as well as direct operating cost. But due to much higher non-recurring costs of the clean-sheet design, the derivatives offer a better ratio of fuel burn and operating cost decrease to the required investment from the manufacturer’s side.","","en","master thesis","","","","","","","","2030-08-05","Aerospace Engineering","Flight Performance and Propulsion","","","",""
"uuid:5e86ad20-828d-4c16-b1c3-09ab24b5cb97","http://resolver.tudelft.nl/uuid:5e86ad20-828d-4c16-b1c3-09ab24b5cb97","Nonlinear Control For a Small-Scale Helicopter UAV in Autorotation","Kerkkamp, J.S.F.","Verhaegen, M.H. (mentor)","2016","With the increased use of small unmanned systems, and even automated ﬂight, safety is becoming an issue. For helicopters speciﬁcally, engine failure typically results in a crash. For manned helicopters, autorotation is a manoeuvre used to reduce the downward speed and land at safe velocities. In this thesis, we examine a technique of modelling and control for such a small unmanned helicopter. Port-Hamiltonian (PH) modelling is a technique to construct a passive system, that can be controlled with Passivity-Based Control (PBC) in order to guarantee stability in the controlled system. This thesis used these techniques to control a helicopter model composed of a basic physical model and an accurate model of the rotor airﬂow and thrust generation, with the ultimate aim to provide control during total engine failure. An Interconnection and Damping Assignment (IDA) controller was constructed for both the vertical and lognitudinal models, with a relatively novel damping method based on exponential damping functions. Use of this controller allowed fast and smooth control of velocities for the vertical and longitudinal models. Finally, the controller succeeds in following the textbook manoeuvres for an autorotative landing: ﬁrst generating forward speed to slow down the descent, and ending in a ﬂare manoeuvre to land with as little speed as possible. The ﬁnal landing could still be considered rough, but the actions of the controller provide a larger chance that the airframe survives touchdown. This thesis is a preliminary step in developing a full PH helicopter model. Recommendations are made to extend and augment the model to implement a more detailed representation of helicopter mechanics. As for the controller, a control strategy for an automated autorotative landing is examined and discussed. It is shown that the suggested controller guides the helicopter model through the autorotation strategy, reducing the downward speed up to landing at the cost of energy stored in the main rotor. A limitation of the controller is that it does not yet implement actuator limits, resulting in a stronger control action than would normally be possible.","UAV; helicopter; autorotation; Port-Hamiltonian; Passivity-Based Control; Interconnection and Damping Assignment","en","master thesis","","","","","","","","2017-08-01","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control (DCSC)","","","",""
"uuid:dfcd14c2-67c8-46bc-8d6b-b5fd69de2297","http://resolver.tudelft.nl/uuid:dfcd14c2-67c8-46bc-8d6b-b5fd69de2297","Development of a luggage security system for travellers","Woo, J.","Horváth, I. (mentor); Kooijman, A. (mentor)","2016","Background The moment of travel is one of the special moments of all people because they experience everything new that may never happen again in a lifetime during travel. However, theft is one of the noteworthy issues that undermines travelling experience. The potential of threat makes the traveller not to focus entirely on enjoying their travel. Although the overall travelling experience is changing with the fast-paced evolution of technology and society, lots of travellers still use a conventional security gear for the luggage security, such as lock or chain. There are a lot of technological potential currently that can stimulate the development of next-generation security gear. Objective Develop a product that can enhance travel experience from luggage security issue. Solution The luggage security system consists of a smartphone application (the central) and an individual device (the peripheral), which is supposed to be placed in luggage. The central and the peripheral continuously communicates each other (via Bluetooth technology), monitors luggage and user’s condition. The system offers appropriate feedbacks and functions to the user according to the distance (connectivity) and movement of luggage. The system offers 1) Monitoring 2) Tracking 3) Recording function as a core feature. Conclusion The backbone technology is feasible, and the system is regarded as practical by user evaluation. The distance measurement technique is not very mature so that there is an accuracy issue. Currently, relative technologies are becoming hype (Internet of Things, Beacon) so that the measurement technique will be improved soon.","luggage; security; traveller","en","master thesis","","","","","","","Campus only","","","Design Engineering","","Integrated Product Design","",""
"uuid:9677ebd9-cf4c-484c-97f8-1f553cd0e873","http://resolver.tudelft.nl/uuid:9677ebd9-cf4c-484c-97f8-1f553cd0e873","Sparse VARX Model Identification for Large-Scale Adaptive Optics","Piscaer, P.J.","Verhaegen, M.H.G. (mentor)","2016","","","en","master thesis","","","","","","","","2019-08-01","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control","","","",""
"uuid:aeb07791-ca4d-4d9f-9c7b-42df747babe0","http://resolver.tudelft.nl/uuid:aeb07791-ca4d-4d9f-9c7b-42df747babe0","Seismic forward modelling of rock and uid properties in carbonates: Reducing uncertainty in data-poor environments","van der Waal, R.","Wellmann, F. (mentor); Drijkoningen, G.G. (mentor)","2016","The main challenge in frontier and near-field exploration is the limited availability of data to interpret the subsurface. In the absence (or prior to drilling) of an appraisal well, possibly only seismic data and prior geology knowledge are at hand. In such data-poor environments, techniques are required that are able to quickly test large amounts of geology scenarios in a high dimensional (many unknown parameters) model space. Markov Chain Monte Carlo optimization techniques have previously shown to be capable of realizing reasonable solutions for large modelling problems, but are less effcient when the problem has very limited prior knowledge available. Optimization techniques as Particle Swarm Optimization and Cuckoo Search may however be able to de-risk the seismic amplitudes with the use of limited prior knowledge, since they may evaluate large sets of potential rock and uid property scenarios in parallel. To validate the applicability of these techniques, they are tested on their capability of solving mineralogy and porosity in a case-study on the Shuaiba Formation, with only limited prior data available. Cuckoo Search in particular has proven to be suitable to achieve reasonable mineralogy and porosity scenarios fitting a seismic section of the Shuaiba Formation. Particle Swarm Optimization has proven to be a robust technique as well, but due to its slightly less exploratory behavior, it has a greater risk of not finding the optimal solution. On the contrary however, if time is limited and effciency is more important than detail, Particle Swarm Optimization may be the preferred tool to use. Both techniques have provided an indication towards a carbonate mineralogy and porosity values only vary in the order of a few percent with respect to prior research. In overall, the model created has shown to be easily applicable by geologists and geophysicists as it requires only limited prior knowledge of the subsurface. The tool may therefore be of great value to the oil and gas business, since it may help decide whether it is worth drilling an appraisal well based on the probabilities of different scenarios, potentially yielding a great economic advantage","","en","master thesis","","","","","","","","","Civil Engineering and Geosciences","Geoscience and Engineering","","Applied Geophysics","",""
"uuid:989a6d56-64dc-4825-be30-863086d342cb","http://resolver.tudelft.nl/uuid:989a6d56-64dc-4825-be30-863086d342cb","Seamless switching between optical ground stations in ground-to-GEO communication","van Roosmalen, M.E.A.","Bliek, L. (mentor)","2016","This thesis discusses the possibility of a seamless switch between optical ground stations (OGSs) in ground-to-GEO communication. First an overview is given of the characteristics of a future optical DVB-S2 feederlink. The equipment at an OGS, the equipment on board of a satellite and the terrestrial network is described along with the effects of the atmosphere on the optical signal. The objective is to synchronise the signals from two different OGSs towards the satellite. Two existing solutions are presented which do not achieve the required performance. Nevertheless the principle of an existing Ka-band site diversity is promising and is adapted to be used in the proposed solution which uses a fixed delay line combined with a variable photonic delay line (VPDL). The solution is based on the principle of equalising the travelling paths via the two OGSs. To determine the required length of the VPDL a downlink sequence is sent from the satellite to both OGSs and then to a control centre, following the same path as the uplink. The time-delay difference measurement is done on the downlink sequence and used to add a time delay to one of the links. The performance of several VPDLs are discussed. At the moment of writing this thesis, no existing continuous tunable VPDL meets the requirements of the system. Therefore a system of concatenated switches and delay lines is proposed and tested in simulations. The satellite movement and the atmospheric turbulences are disturbances on the system that can not be controlled. Predictions of the satellite movement are available, but the turbulences are considered to be purely stochastic. A control strategy is proposed which uses the timing difference measurements on a downlink sequence to control the VPDL, thereby synchronising the OGSs. The measurements are thus done on a parallel feedback system which provides an estimation of the performance metric. A simulation model is built in Simulink containing the signal generation, a switching system, transmission channels for two OGSs and a DVB-S2 receiver. Included in the simulation model is the proposed time delay compensation system. The simulations show the feasibility of the proposed solution. The VPDL will need a precision of a quarter of a symbol time to be seamless. Further tests are necessary to show if the simulation model provides realistic performances.","","en","master thesis","","","","","","","","2021-08-01","Mechanical, Maritime and Materials Engineering","Delft Center for Systems and Control (DCSC)","","","",""
